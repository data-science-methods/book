# Reproducibility, Replicability, and Robustness #

- Reading: @YongReplicationStudiesBad2012, @AschwandenThereNoSuch2017, @HicksCommentSubmittedDaniel2020
- Highly recommended:  @FeestWhyReplicationOverrated2019, @LeonelliRethinkingReproducibilityCriterion2018
- *[https://ai.facebook.com/blog/how-the-ai-community-can-get-serious-about-reproducibility/]

- Some data management disasters
    - @ArrayErrors2011, @VideoKeithBaggerly
    - @HerndonDoesHighPublic2014, but you can just read @BaileyReinhartRogoffErrorHow, @CassidyReinhartRogoffControversy, and/or watch @ReinhartRogoffGrowth2019
    - @LaskowskiWhatWhenYou, @ViglioneAvalancheSpiderpaperRetractions2020, @PennisiProminentSpiderBiologist2020
    - @HowExcelMay2020


## The Replication Crisis ##

- An epistemic-methodological crisis unfolding across multiple scientific fields, especially biomedicine and social psychology
- Usually defined in a phrase:  "studies replicate much less often than you would expect"

- Received the most attention in the wake of a high-profile effort to replicate a collection of high-profile psychology studies [@OpenScienceCollaborationEstimatingReproducibilityPsychological2015]
    - But it's been simmering for a long time [@IoannidisWhyMostPublished2005; @MeehlTheoryTestingPsychologyPhysics1967]

- In psychology
    - Has stimulated a move towards preregistered studies, open science, and lots of papers reflecting on psychological methods
    - Open science movement: @NosekPromotingOpenResearch2015; @MunafoManifestoReproducibleScience2017

```{r, echo = FALSE, out.width = '60%', fig.cap = "As part of the open science movement, some journals have added badges to journal articles indicating open data, open code, etc."}
knitr::include_graphics(file.path('images', 'reproducibility', 'badges.png'))
```

- Beyond academia
    - In my experience most members of the general public aren't aware of it
    - Even scientists in non-experimental fields (e.g., cultural anthropologists) have forgotten
    - But it has been used in policy settings to manufacture doubt about environmental epidemiology [@AschwandenThereNoSuch2017; @HicksCommentSubmittedDaniel2020]


## Terminology ##

- A 2019 National Academy of Sciences (NAS) report noted that people often use the terms "reproduce" and "replicate" interchangeably, and consequently conflate two very different expectations for quantitative experimental research [@NationalAcademiesofSciencesEngineeringandMedicineReproducibilityReplicabilityScience2019]

- @HicksCommentSubmittedDaniel2020 reviews the NAS definitions and also compares them to "robustness"
    - **Replicability**: Run the experimental again, getting new data on a more-or-less similar sample.  Do the same analysis.  Do we get qualitatively similar results?  
    - **Reproducibility**: Run the same analysis script on the same data on a different computer.  Do we get numerically identical results? 
    - **Robustness**: Use a *different* analytical approach to attempt to answer the same research question on the *same* data.  Do we get qualitatively similar results?  [@SteegenIncreasingTransparencyMultiverse2016; @SilberzahnManyAnalystsOne2018]


|                     | same data? | same analysis? | expect to get 
|:--------------------|:----------:|:--------------:|:--------------
| replicability       |            | Y              | similar results
| reproducibility     | Y          | Y              | numerically identical results
| robustness          | Y          |                | similar results

Table: The distinction between replicability, reproducibility, and robustness can be made by considering whether we use the same data, the same analysis, and whether we expect to get numerically identical or merely similar results. 


- "Qualitatively similar" is necessarily vague and requires interpretation
    - Researchers can and do disagree about whether any particular set of results is sufficiently "similar" or "different" [@FeestExperimentersRegressReconsidered2016]
    - Sometimes a quantitative standard is used:  "replication attempt is also statistically significant and effect is in the same direction"
    - But the unavoidable chance of type I and II errors means there's still uncertainty
    - And remember that the threshold of statistical significance is basically arbitrary

- "Numerically identical" is not vague
    - (At least, up to a tolerance threshold, and with the proviso that the computers are working correctly)
    
- Problem-solution mismatch
    - The replication crisis was spurred by high-profile *replication* failures
    - But open science is about promoting *reproducibility* [@HicksCommentSubmittedDaniel2020]
    - Connection between reproducibility and replication is generally quite thin


## Epistemic Value of Reproducibility ##

Reproducibility is a *necessary* condition for computational research, but has *low evidentiary value*. 

- Necessary condition:  If we can't run your code on your data and get the same output, something has gone seriously wrong. 

- Low evidentiary value:  Reproducibility tells us *only* that your code reliably produces the output you reported.  

| |
|:-----| 
| Whether your research question is reasonable |
| Whether your data was collected the way you think it was |
| Whether your data can possibly answer your research question |
| Whether your data was fabricated |
| Whether your data was mismanaged |
| Whether your analyses are appropriate |
| Whether your analysis includes relevant covariates |
| Whether your code runs the analysis that you think it does |
| Whether you p-hacked your analysis [@SimmonsFalsePositivePsychology2011] |
| How to interpret your analysis output |
| How to characterize your intervention and endpoint [@FeestWhyReplicationOverrated2019]

Table: Things reproducibility doesn't tell us


## Open Science â‰  Reproducible ##

- @ObelsAnalysisOpenData2020
    - Reproducibility check of 62 pre-registered studies in a Center for Open Science database
    - "Of the 62 articles that met our inclusion criteria, 41 had data available, and 37 had analysis scripts available"
    - "We could run the scripts for 31 analyses, and we reproduced the main results for 21 articles."

```{r, echo = FALSE, message = FALSE, out.width = '60%', fig.cap = "Infographic for @ObelsAnalysisOpenData2020. While 58% of preregistered studies provided both the data and analysis script, only 50% had runnable scripts and only 34% could actually be reproduced."}
library(tidyverse)

obels_df = tribble(
    ~check, ~n, 
    'included', 62, 
    'data available', 41, 
    'code available', 37, 
    'data & code available', 36,
    'script runnable', 31, 
    'reproduced', 21
) %>% 
    mutate(check = fct_inorder(check), 
           total = 62, 
           frac = n / total)

time2_trans = scales::trans_new(
    name = 'times2', 
    inverse = function(x) {2*x}, 
    transform = function(x) {x/2}
)

ggplot(obels_df, aes(check)) +
    geom_tile(aes(x = fct_rev(check), y = 0L, 
                  width = 1L, height = n, 
                  fill = as.factor(n))) +
    geom_label(aes(label = check,
               x = check, y = 0L), 
               position = position_nudge(x = .15),
               alpha = .75) +
    geom_label(aes(label = scales::percent(frac, accuracy = 2),
                   x = check, y = 0L), 
               position = position_nudge(x = -.15), 
               alpha = .75) +
    scale_fill_viridis_d(direction = -1,
                         guide = FALSE) +
    scale_x_discrete(name = '', breaks = c()) +
    scale_y_continuous(name = '', trans = time2_trans, 
                       breaks = seq(0, 60, by = 20), 
                       minor_breaks = seq(-60, 60, by = 10)) +
    coord_flip() +
    theme_minimal()
```



