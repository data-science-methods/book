[["index.html", "Methods of Data Science I Preface", " Methods of Data Science I Dan Hicks, hicks.daniel.j@gmail.com Wed Feb 23 16:03:34 2022 Preface This document is a set of notes (and perhaps later a textbook) for a course on data science methods for graduate students in social and behavioral sciences, taught by Professor Dan Hicks (they/them), UC Merced. The course site is available here. These notes were written in RStudio using bookdown. The complete source is available on GitHub and is automatically rebuilt using GitHub actions. This version of the book was built with R version 4.1.2 (2021-11-01), pandoc version 2.13, and the following packages: package version source assertthat 0.2.1 CRAN (R 4.1.0) bookdown 0.24 CRAN (R 4.1.0) datasauRus 0.1.4 CRAN (R 4.1.0) desc 1.4.0 CRAN (R 4.1.0) ggbeeswarm 0.6.0 CRAN (R 4.1.0) ggforce 0.3.3 CRAN (R 4.1.0) nycflights13 1.0.2 CRAN (R 4.1.0) printr 0.2 CRAN (R 4.1.0) reticulate 1.24 CRAN (R 4.1.2) sessioninfo 1.2.2 CRAN (R 4.1.0) skimr 2.1.3 CRAN (R 4.1.0) sloop 1.0.1 CRAN (R 4.1.0) tictoc 1.0.1 CRAN (R 4.1.0) tidylog 1.0.2 CRAN (R 4.1.0) tidyverse 1.3.1 CRAN (R 4.1.0) visdat 0.5.3 CRAN (R 4.1.0) vroom 1.5.7 CRAN (R 4.1.0) "],["introduction.html", "Chapter 1 Introduction 1.1 Land acknowledgements 1.2 About the instructor 1.3 What this course isn’t, and is 1.4 Learning outcomes 1.5 Prerequisites 1.6 Requirements and weekly routine", " Chapter 1 Introduction 1.1 Land acknowledgements 1.1.1 Campus land acknowledgment We pause to acknowledge all local indigenous peoples, including the Yokuts and Miwuk, who inhabited this land. We embrace their continued connection to this region and thank them for allowing us to live, work, learn, and collaborate on their traditional homeland. Let us now take a moment of silence to pay respect to their elders and to all Yokuts and Miwuk people, past and present. 1.1.2 Instructor’s land acknowledgment UC Merced and the City of Merced are on the traditional territory of the Yokut people. This land was stolen by Spanish, Mexican, and American settlers through acts of slavery and genocide. In addition, UC Merced is strongly associated with Ahwahne, known as Yosemite Valley. This valley was the traditional home of the Ahwahnechee people, who were the victims of some especially horrific, state-sponsored genocidal acts. For more on the history of Ahwahne, see https://tinyurl.com/y879jw8s. For more information on land acknowledgments, see https://native-land.ca. 1.2 About the instructor Dan Hicks is a philosopher turned data scientist turned philosopher. I use they/them pronouns and identify as nonbinary. I grew up in Placerville, about two hours north of Merced in the Sierra Foothills. One branch of my family came to California during the Gold Rush, so I identify heavily as a Californian and have some complicated feelings about the genocide. I finished my PhD in philosophy of science at Notre Dame in 2012. After that I worked in a series of research positions in academia and the federal government. During 2015-2019 I was using data science methods at least half-time. I joined the faculty at UC Merced in Fall 2019. Email: dhicks4@ucmerced.edu Student hours: By appointment: https://doodle.com/mm/danhicks/office-hours Website: https://dhicks.github.io/ 1.3 What this course isn’t, and is Is Not: a statistics course (in the way you think) a general introduction to software engineering a basic introduction to R carefully planned out from beginning to end Is: an introduction to data science about exploratory data analysis, data management, and reproducibility habituation to some good software engineering practices that are especially valuable for data science work in the alpha stage of development 1.4 Learning outcomes [match to catalog] By the end of the course, students will be able to apply concepts from software engineering and philosophy of science to methodological decisions in data science, including reproducibility vs. robustness vs. replicability value and hazards of open data thick data (Wang 2016) use exploratory data analysis techniques and tools to identify potential data errors and potential phenomena for further analysis clean raw data and produce a reproducible codebook for both downstream analysis and public release of data according to FAIR standards (M. D. Wilkinson et al. 2016) [create and use a cloud database to work with big data] manage data, analysis, and outputs for reproducibility [assembling a project compendium; Gentleman and Lang (2004)] using best practices of data handling, a clear directory structure (Wilson et al. 2017), self-documenting code, version control, build automation, [and continuous integration] apply philosophical and data science concepts to integrated ethical-technical analysis to case studies in algorithmic injustice understand the background on the COMPAS case: Angwin et al. (2016); Corbett-Davies et al. (2016) reproduce several fairness statistics on COMPAS data explain tradeoffs between false negatives and false positives in terms of inductive risk explain Kleinberg, Mullainathan, and Raghavan (2016) impossibility result (not necessarily the proof) and show that it applies to the COMPAS datas critically discuss profiling and thresholds (Pierson et al. 2020) as potential explanation for differences in “base rates” of recidivism critically locate discussions of fairness and recidivism in a broader social context, and identify implications for working data scientists (Selbst et al. 2018; Hoffmann 2019; Hanna et al. 2019) 1.5 Prerequisites This course assumes basic competence with introductory R. “Introductory R”: Lessons 1-5 of the Carpentries “R for Social Scientists” curriculum Installing R and packages Working in the R Studio IDE Common data types Reading and writing CSV files Tidyverse R: mutate(), filter(), select(); plotting with ggplot2 “Basic competence”: Given time and a reference (cheatsheet, Stack Exchange, mentor) you can figure out how to solve a problem 1.6 Requirements and weekly routine This is aspirational. Tuesdays: Mix of discussion and lecture based on assigned readings Thursdays: Live coding leading into work on the week’s lab assignment Labs: 10-ish, done individually or in pairs and submitted via GitHub for automated feedback Running project: Practicing ideas from the course on data sets you find References "],["software-hardware-and-accounts.html", "Chapter 2 Software, hardware, and accounts 2.1 A Note on Accessibility", " Chapter 2 Software, hardware, and accounts R and RStudio: Last year R had a major release (version 4) Addressing some bugs has led to some rapid development in R and RStudio For consistency in class, I encourage you to use the latest version of both R and RStudio Help on switching between different versions of R As of 2021-07-29, the machines in our classroom have R 4.0.2 and RStudio 1.3.1056 OS: All of the tools we’ll use are available on all major OSes (Except Sourcetree, which isn’t essential. List of git GUIs) I have much less experience debugging Windows and Linux, but will do what I can You’ll need a GitHub account and Sourcetree GitHub is owned by Microsoft And there are concerns about how it mined public repositories to develop a code completion tool But it offers some key features that I want to use in this course You can do everything this course requires in private repositories, so long as you give access to the people who need access I strongly recommend against tablets (iPad, Surface) for this course Unless you’re using a cloud service such as RStudio Cloud I recommend that your machine have at least 200 GB free hard drive space 8 GB RAM Intel i5 processor Operating system capable of running the most recent release of R (4.1.0 as of 2021-07-28) Mac: 10.13 (High Sierra) or more recent Windows: See here If you’re running Linux I’m going to assume you can figure this out yourself Using the computer lab machines The machines are just a little behind the most recent versions of R and RStudio But should work fine for our purposes I recommend carrying your work on a flash drive 2.1 A Note on Accessibility I chose the tools and platforms for this course in part because they’re industry-standard. If you pursue a career as a data scientist in industry, you’ll be expected to use GitHub (or something similar) on a daily basis. And RStudio is by far the most commonly used IDE (“integrated development environment”) for R. However, like many other technologies, they were originally developed using ableist assumptions about “normal” computer users. In response to criticism, the developers of these systems and tools have gone back and made their technologies more accessible. But there may still be barriers to accessibility that I have not anticipated. If you encounter a barrier to participating in this course — even a small inconvenience — please let me know. Similarly, if you have ideas for making the course more accessible, please share them with me. "],["data-science-what-and-why.html", "Chapter 3 Data Science: What and why 3.1 Reading 3.2 Discussion question 3.3 Standard definition 3.4 Discussion question", " Chapter 3 Data Science: What and why 3.1 Reading Wilson et al. (2017) McElreath (2020) 3.2 Discussion question Why did you decide to take a class called “data science?” 3.3 Standard definition Figure 3.1: Data science, defined as the intersection of CS, stats, and “business knowledge.” Source: https://www.kdnuggets.com/2020/08/top-10-lists-data-science.html The intersection of computer science/software engineering, statistics, and “business knowledge” But this defines data science in terms of tools and techniques, not epistemic and practical goals. Compare: An ecologist is someone who spends most of their time collecting specimens in the field and processing them in a lab, vs.  An ecologist is someone who studies interactions among organisms and their environment 3.4 Discussion question What are the epistemic and practical goals of your scientific field? How do you think “data science” will be useful for pursuing those goals? References "],["git-and-version-control.html", "Chapter 4 Git and Version Control 4.1 Some motivating examples 4.2 Version control 4.3 Gitting started 4.4 Time travel 4.5 The garden of forking branches 4.6 Generating a GitHub PAT 4.7 Working with GitHub remotes 4.8 Further reading", " Chapter 4 Git and Version Control 4.1 Some motivating examples While working on your analysis code, you accidentally delete the first 35 lines of the script. You only discover this three days later, when you restart R and try to run the script from the top. Because you lost half of your senior thesis in undergrad, you hit Control+S to save every couple of minutes. You’re working on a paper with two coauthors. You prepare the final draft to send for submission: paper final.docx. But one of your coauthors discovers a typo. Now it’s paper final fixed typo.docx. Another realizes six references are missing. paper final fixed typo refs.docx. That’s getting confusing so you change it to paper 2 Aug 2021.docx. Once it comes back from review you need to make revisions. Now you have paper 30 Jan 2022.docx and, after your collaborators make their changes, paper 12 February 2022 DJH.docx and paper 12 February 20222 final.docx. You have a complicated analysis spread over several scripts. You want to explore a variant analysis, but doing so will involve changes in 15 different places across 3 different files. You’re not sure if this variant analysis will work; you may or may not want to keep it. 4.2 Version control Basic idea: Tools for tracking and reversing changes to code over time Useful for identifying and reversing breaking changes Implementations upload to cloud, track who contributes code, control who can suggest vs. actually change code Good for collaboration, publishing code git One of many version control systems Very popular in part thanks to GitHub, which provided free hosting for open-source projects In April 2020, GitHub also made private/closed-source repositories free Resources for students (and teachers): https://education.github.com/ 4.3 Gitting started git is very hard We’re going to use the Sourcetree GUI to get started 4.3.1 Initial commit Install Sourcetree and go through the configuration steps Then you’ll see the (empty) repository browser Create a folder called something like learning-git Then, in Sourcetree: On the Local tab Select New … &gt; Create a local repository (or) Drag and drop the folder on to the browser Figure 3.1: Creating a new local repository in learning-git You can open the repository in Sourcetree, but it’s not interesting yet In learning-git, create a text file, eg, testing.txt Sourcetree’s File Status panel shows the new file, ready to be tracked Tracking changes to a file involves two steps: Adding and committing Add: Click the checkmark This tells git that we want to store these changes to the file in its archive Commit: Type a message in the comment field and click Commit This tells git to go ahead and do the archiving process Figure 4.1: Creating a new local repository in learning-git The commit is now displayed in the History panel Figure 4.2: The History panel after our first commit Make a few more changes to the file. Practice adding and committing them and note how the changes accumulate in the History panel. 4.4 Time travel We can checkout previous commits to work with old versions of our files In the example, suppose I made a commit with a mistake (my code stopped working or whatever) In the History panel, right-click on a previous commit and select Checkout… Figure 4.3: Checking out an old commit to travel through time Sourcetree warns us that we’ll be in an undetached head state To see what this means, try making a change to the file, adding and committing it, then checking out the commit with the main or master tag Figure 4.4: Trying (and failing) to change the past. My current HEAD commit will disappear as soon as I check out main. 4.5 The garden of forking branches To actually change the past, we’ll use a branch Branches allow git to track multiple distinct “timelines” for files For example, most major software projects will have separate “dev” (development) and “release” branches Individual branches will also be created for work on specific areas of the project This allows each area of active work to be isolated from work happening in other areas After checking out the previous commit, click on Branch in the toolbar Name your new branch something like fixing-mistake (no spaces!) Start to work on fixing the mistake in the file, then add and commit as usual Now checkout main. Notice: Your commits on fixing-mistake don’t disappear The state of your file changes to the main version The History panel shows the split between the two branches After we’ve finished fixing the mistake, we want to merge these changes back into main Make sure you’re current on main Right-click on fixing-mistake and select Merge… Figure 4.5: Merging fixing-mistake into main Sourcetree will bring up a message about Merge Conflicts This just means that the files you’re combining have conflicting histories, and git wants you to sort out what to keep and what to throw away Important: It’s not obvious (there isn’t a big red status symbol anywhere), but git is now in a special conflict-resolution state. Until you resolve the conflicts and finish the merge, a lot of standard git functionality either won’t work at all or will cause weird problems. If git starts giving you a bunch of weird errors, check to see if you’re in the middle of a merge and need to resolve conflicts. After starting the merge, Sourcetree’s File status panel will indicate exactly which files have conflicts. Figure 4.6: Sourcetree’s File status panel indicates which files have conflicts Your file will look something like this test another line &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD this line has a mistake ======= no mistake this time &gt;&gt;&gt;&gt;&gt;&gt;&gt; fixing-mistake The &lt;&lt;&lt;&lt;&lt;&lt;&lt; and &gt;&gt;&gt;&gt;&gt;&gt;&gt; surround each area of conflict. The top part (marked HEAD) shows the state of the current branch The bottom part (marked fixing-mistake) shows the state of the branch you’re merging Simply edit these sections so they’re in the state you want, Then save, go back to Sourcetree’s File status panel Sourcetree generates a commit message indicating that you’re resolving conflicts to complete a merge Afterwards the History panel shows the two branches merging back together Figure 4.7: The History panel shows the branches merging back together 4.6 Generating a GitHub PAT To get Sourcetree to work with GitHub, we need to generate a PAT (personal access token) On GitHub Click on your profile image (upper right) to your account settings Scroll down to Developer Settings Then Personal access tokens Put a short description in the Note field For our class, we’ll need a PAT with the repo and workflow permissions Important: After you close/navigate away from the next page, you won’t be able to view your PAT again. Keep a browser window on this page for the next few minutes Don’t save the PAT anywhere (that defeats the purpose) Copy-paste now into Sourcetree’s Password field You may need to enter it again in a few minutes, when you first push up to GitHub 4.7 Working with GitHub remotes A remote is a copy of a repository that lives on a server somewhere else 4.7.1 Working with your own repos On GitHub, click “New” and walk through the steps to create a new repository The name on GitHub doesn’t need to match the local name But why wouldn’t you use the same name? Copy the URL: https://github.com/username/learning-git Back in Sourcetree Go to Settings in the toolbar Go to the Remotes tab Click Add to add a new remote The standard name for a default remote is origin Paste in the GitHub URL Figure 4.8: Adding a new remote After adding the remote, we can send the repository (including its full history) up to GitHub using push Look for Push in the toolbar Sourcetree will ask which branches to push When the push is complete, refresh GitHub in your browser You can edit files directly in GitHub Now we’ll fetch and pull the changes from GitHub back to our local copy Fetch: Compare the local tree to the remote tree, noting any differences. Use this to preview the changes on the remote In Sourcetree, hit Fetch in the toolbar. Note that the GitHub commit shows up in the History panel. Pull: Actually download the remote changes and incorporate them into your local tree. 4.7.2 Lab: Working with someone else’s repos GitHub lets you download someone else’s repo (clone), and modify it locally, but not upload directly. You can suggest a change to someone else’s code by submitting a pull request, which first requires forking the repository. Figure 4.9: Forking copies a repository to your GitHub account. Then you clone the copy to your local machine. You can push to your remote copy as usual. You can suggest changes to the original using a pull request. Source: https://happygitwithr.com/fork-and-clone.html Start with the repo for this week’s lab: https://github.com/data-science-methods/lab-w02-git Fork: Look for the fork button in the upper-right Figure 4.10: The fork button is near the upper-right corner of a GitHub repository page. I wasn’t able to find a keyboard shortcut for this. :-( Clone: After creating the fork, you need to download a copy to your machine. In Sourcetree’s repository browser, select New… &gt; Clone from URL This lesson continues in the lab. Open lab.html and lab.R to continue. 4.8 Further reading Hester (n.d.) References "],["warnings-errors-and-getting-help.html", "Chapter 5 Warnings, Errors, and Getting Help 5.1 Reading 5.2 Dependencies 5.3 Messages, warnings, and errors 5.4 Where to go for help 5.5 Example: Dates are often problems 5.6 Another example: More fun with dates 5.7 Writing a reproducible example: reprex 5.8 Do not do these things in your reprex (or anywhere else) 5.9 Debugging in RStudio", " Chapter 5 Warnings, Errors, and Getting Help 5.1 Reading Bryan (2020) “R Faq - How to Make a Great R Reproducible Example” (n.d.) Wickham (2019) 5.2 Dependencies install.packages(&#39;lubridate&#39;, &#39;assertthat&#39;, &#39;reprex&#39;) 5.3 Messages, warnings, and errors Message: Things are fine, but here’s some information you should know Warning: Uhhhh I’m gonna keep going, but maybe this isn’t what you want Error: Nope. I’m stopping here. You need to fix the thing. message(&#39;Hey, just FYI&#39;) warning(&#39;Uhhhh might want to check this out&#39;) stop(&#39;Noooooo&#39;) 5.4 Where to go for help Rubber duck debugging Isolate the problem Restart your session: Session \\(\\to\\) Restart R Local help: ?fun StackOverflow: https://stackoverflow.com/questions/tagged/r CRAN \\(\\to\\) BugReports (usually GitHub Issues) 5.5 Example: Dates are often problems library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union add_six_months = function(date_str) { parsed_dates = parse_date_time(date_str, orders = &#39;mdY&#39;) parsed_dates %m+% months(6) } some_data = c(&#39;June 2002&#39;, &#39;May 15, 2007&#39;, &#39;August 2007&#39;) add_six_months(some_data) ## Warning: 2 failed to parse. ## [1] NA &quot;2007-11-15 UTC&quot; NA Note that this is a warning, not an error R won’t stop running here (unless we tell it to) Errors might not show up until much later in our code, making it hard to identify the root cause Or they might cause invisible problems, eg, by default lm() silently drops observations with missing values To catch warnings Set options(warn = 2) to turn all warnings into errors Use tryCatch() with the warning argument Example: https://stackoverflow.com/questions/8217901/breaking-loop-when-warnings-appear-in-r/8218794#8218794 Write a unit test My preferred approach: Add an assertion to your primary code ## Using an assertion to prevent warnings from cascading library(assertthat) six_months_later = add_six_months(some_data) assert_that(all(!is.na(six_months_later)), msg = &#39;Missing values in `six_months_later`&#39;) Let’s start by using the RStudio debugger to isolate the problem debugonce(add_six_months) add_six_months(some_data) The problem is in lubridate::parse_date_time(). Spend a few minutes reading the documentation for this function and playing around with the call. What does the argument orders do? ?parse_date_time parse_date_time(some_data, orders = &#39;mdY&#39;) Let’s try SO: https://stackoverflow.com/search?q=%5BR%5D+lubridate+month-year parse_date_time(some_data, orders = c(&#39;mY&#39;, &#39;mdY&#39;)) Make this change in add_six_months() and confirm it no longer trips the assertion. 5.6 Another example: More fun with dates more_data = c(&#39;May 7, 2017&#39;, &#39;May 19, 2017&#39;, &#39;May Fifth, 2017&#39;) mdy(more_data) SO doesn’t seem so helpful: https://stackoverflow.com/search?q=%5BR%5D+lubridate+written+days Let’s check the CRAN page for lubridate: https://cran.r-project.org/web/packages/lubridate/index.html Figure 4.8: Screenshot of lubridate on CRAN, highlighting the BugReports field Trying a couple of searches gives us a promising result: https://github.com/tidyverse/lubridate/issues?q=is%3Aissue+is%3Aopen+mdy Figure 4.9: Screenshot of lubridate issues page, showing a relevant search result This is a known bug; it looks like they’re thinking about doing something about it, but the only workaround is to create an NA: https://github.com/tidyverse/lubridate/issues/685 5.7 Writing a reproducible example: reprex https://reprex.tidyverse.org/ https://reprex.tidyverse.org/articles/articles/learn-reprex.html https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html Practice by writing a reprex for one of our two examples 5.8 Do not do these things in your reprex (or anywhere else) Or Jenny Bryan will come to your office and set your computer on fire. setwd('/users/danhicks/projects/catsaregreat/myscript/') Used to ensure that R is running where your file is Unnecessary if you’re opening different projects in different RStudio sessions Will cause irrelevant errors on any other system Aside: use file.path() or here::here() to build paths rm(list=ls()) Used because people think it clears out the global environment Unnecessary if you’re regularly using Session \\(\\to\\) Restart R Also unnecessary at the top of a Rmd file, which is always knit in a new session Doesn’t actually clear out the global environment eg, doesn’t unload packages or reset options() Not on Bryan’s list, but also don’t do it: require(package) If package is installed, will act just like library() If not, will return FALSE The script will keep going until there’s an error about a missing function 300 lines later Probably not the error you wanted help with Annoying to debug because I have no idea where the function is supposed to come from If library() can’t find the package, it immediately raises an error I can tell right away what package needs to be installed 5.9 Debugging in RStudio This week’s lab introduces you to some of RStudio’s debugging tools. References "],["programming-paradigms.html", "Chapter 6 Programming Paradigms 6.1 Reading 6.2 Dependencies 6.3 Programming paradigms 6.4 Object-oriented programming 6.5 The OOP you’re used to 6.6 S3 is OOP, but weird 6.7 Functional programming 6.8 Common features of functional programming 6.9 Breaking R: Everything that happens is a function 6.10 Actually-useful functional R: Pipes (and the tidyverse) 6.11 Programming paradigms and data science", " Chapter 6 Programming Paradigms 6.1 Reading Introductions to parts II and III of Wickham (2014) Chambers (2014) 6.2 Dependencies install.packages(&#39;sloop&#39;) 6.3 Programming paradigms Procedural or imperative Software is a series of instructions (“procedures”), which the computer carries out in order. Special instructions (if-then, loops) are used to change the order based on inputs or other conditions. - Examples: FORTRAN, BASIC, C, a calculator Object-oriented Software is made up of objects, which have properties (“attributes,” including other objects) and do things (“methods”). - Examples: Python, Java Functional Software is made up of functions, which are run sequentially on the inputs. - Examples: Lisp, Haskell 6.3.1 R is both object-oriented and functional Object-oriented: Everything that exists is an object Functional: Everything that happens is a function call 6.4 Object-oriented programming board game as OOP regression models as OOP 6.5 The OOP you’re used to Classes are defined by their elements and methods Changing/adding elements and methods requires changing the class definition For \\(x\\) to be an \\(F\\), \\(x\\) must be created as an \\(F\\) ## &lt;https://vegibit.com/python-class-examples/&gt; class Vehicle: def __init__(self, brand, model, type): self.brand = brand self.model = model self.type = type self.gas_tank_size = 14 self.fuel_level = 0 def fuel_up(self): self.fuel_level = self.gas_tank_size print(&#39;Gas tank is now full.&#39;) def drive(self): if self.fuel_level &gt; 0: print(f&#39;The {self.model} is now driving.&#39;) self.fuel_level -= 1 else: print(f&#39;The {self.model} is out of gas!&#39;) dhCar = Vehicle(&#39;Honda&#39;, &#39;Fit&#39;, &#39;Hatchback&#39;) dhCar.gas_tank_size = 10 dhCar.fuel_up() dhCar.drive() 6.6 S3 is OOP, but weird S3 classes can be changed on the fly, with no attempt to validate any assumptions. dh_car = list(brand = &#39;Honda&#39;, model = &#39;Fit&#39;, type = &#39;Hatchback&#39;) class(dh_car) class(dh_car) = c(&#39;vehicle&#39;, class(dh_car)) class(dh_car) inherits(dh_car, &#39;vehicle&#39;) model = lm(log(mpg) ~ log(disp), data = mtcars) class(model) print(model) class(model) = &#39;Date&#39; class(model) try(print(model)) Wickham discusses good practices to reduce this chaos in S3 write constructor, validator, and helper functions S4 and R6 provide more conventional OOP structure S3 uses generic functions reg_model = lm(log(mpg) ~ log(disp), data = mtcars) aov_model = aov(log(mpg) ~ log(disp), data = mtcars) class(reg_model) class(aov_model) inherits(aov_model, &#39;lm&#39;) print(reg_model) print(aov_model) residuals(aov_model) residuals(reg_model) Both aov_model and reg_model inherit from lm print() and residuals() are both generics (There can be) different versions of each function for different classes Different output for print() Same output for residuals() print() is a generic; the call just passes us off using UseMethod() print ## function (x, ...) ## UseMethod(&quot;print&quot;) ## &lt;bytecode: 0x7ff7c347d288&gt; ## &lt;environment: namespace:base&gt; sloop package is useful for understanding how S3 figures out which specific function to call library(sloop) s3_dispatch(print(reg_model)) s3_dispatch(print(aov_model)) Note that the class-specific functions are often hidden try(print.lm) s3_get_method(print.lm) # stats:::print.lm s3_get_method(print.aov) # stats:::print.aov Use s3_dispatch() to explain why the two models have the same output for residuals(). sloop::s3_methods_generic() lists all class-specific versions of generics s3_methods_generic(&#39;print&#39;) ## # A tibble: 251 × 4 ## generic class visible source ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 print acf FALSE registered S3method ## 2 print AES FALSE registered S3method ## 3 print anova FALSE registered S3method ## 4 print aov FALSE registered S3method ## 5 print aovlist FALSE registered S3method ## 6 print ar FALSE registered S3method ## 7 print Arima FALSE registered S3method ## 8 print arima0 FALSE registered S3method ## 9 print AsIs TRUE base ## 10 print aspell FALSE registered S3method ## # … with 241 more rows And similarly for all generics defined for a given class s3_methods_class(&#39;lm&#39;) ## # A tibble: 35 × 4 ## generic class visible source ## &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 add1 lm FALSE registered S3method ## 2 alias lm FALSE registered S3method ## 3 anova lm FALSE registered S3method ## 4 case.names lm FALSE registered S3method ## 5 confint lm TRUE stats ## 6 cooks.distance lm FALSE registered S3method ## 7 deviance lm FALSE registered S3method ## 8 dfbeta lm FALSE registered S3method ## 9 dfbetas lm FALSE registered S3method ## 10 drop1 lm FALSE registered S3method ## # … with 25 more rows 6.7 Functional programming board game as a series of functions? regression models as a series of functions 6.8 Common features of functional programming First-class functions Functions can be used like any other data type, including as inputs to and outputs from other functions Determinism Given a list of input values, a function always returns the same output value No side effects A function doesn’t have any effects other than returning its output Immutability Once a variable is assigned a value, that value cannot be changed These features make it much easier to reason about how a functional program works. 6.9 Breaking R: Everything that happens is a function This includes assignment foo = 3 `&lt;-` &lt;- function(x, y) x + y foo &lt;- 5 foo = 7 And brackets `[` &lt;- function(x, y) x * y bar = data.frame(x = rep(1, 15), y = rep(2, 15)) bar[&#39;x&#39;] bar[2] bar[18] 6.10 Actually-useful functional R: Pipes (and the tidyverse) Pipe syntax for function composition (%&gt;% and |&gt;) Tidyverse functions are designed to work with pipes library(dplyr) mtcars %&gt;% filter(cyl &gt; 4) %&gt;% lm(mpg ~ disp, data = .) %&gt;% summary() ## ## Call: ## lm(formula = mpg ~ disp, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4010 -1.5419 -0.5121 1.1408 4.9873 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.623520 1.463181 16.145 1.50e-12 *** ## disp -0.023527 0.004682 -5.025 7.52e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.118 on 19 degrees of freedom ## Multiple R-squared: 0.5706, Adjusted R-squared: 0.548 ## F-statistic: 25.25 on 1 and 19 DF, p-value: 7.521e-05 Using the new native pipes with the new native anonymous functions mtcars |&gt; filter(cyl &gt; 4) |&gt; {\\(d) lm(mpg ~ disp, data = d)}() |&gt; summary() ## ## Call: ## lm(formula = mpg ~ disp, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4010 -1.5419 -0.5121 1.1408 4.9873 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.623520 1.463181 16.145 1.50e-12 *** ## disp -0.023527 0.004682 -5.025 7.52e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.118 on 19 degrees of freedom ## Multiple R-squared: 0.5706, Adjusted R-squared: 0.548 ## F-statistic: 25.25 on 1 and 19 DF, p-value: 7.521e-05 6.11 Programming paradigms and data science OOP is most useful for developers Functional programming rules are really useful for data cleaning and analysis “The analysis pipeline” Reasoning about the state of our code Ensuring reproducibility References "],["eda-models.html", "Chapter 7 Exploratory Data Analysis 7.1 7.2 Exploratory and Confirmatory Research 7.3 Better Models for EDA I: Developing Phenomena 7.4 EDA as phenomena development 7.5 Better Models for EDA II: Epicycle of Analysis 7.6 Peng and Matsui (2016) 7.7 7.8 Discussion 7.9 References", " Chapter 7 Exploratory Data Analysis 7.1 This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation. (John Wilder Tukey 1977) Exploratory Data Analysis (EDA) is “an attitude, AND a flexibility, AND some graph paper (or transparencies, or both)” (John W. Tukey 1980) 7.2 Exploratory and Confirmatory Research Especially in the wake of the replication crisis, one common distinction is between exploratory and confirmatory research (Wagenmakers et al. 2012) Exploratory vs. confirmatory research Confirmatory Exploratory hypothesis testing hypothesis development specified in advance adaptable algorithmic free, creative mechanical objectivity (Daston and Galison 2007) pure subjectivity? avoids inferential errors makes errors? rigorous lacking rigor? real science?? h*cking around with data?? assumes experimental methods relevant to all methods I agree that it’s important to be thoughtful about how much confidence we’re placing in our conclusions interpret findings from one study in light of other studies But the confirmatory/exploratory distinction can overemphasize the confirmatory side Making us too rigid and narrow-minded about what counts as good science 7.3 Better Models for EDA I: Developing Phenomena Brown (2002) The data/phenomena/theory distinction Data Phenomena Theories/ Causal processes Ex: spreadsheet of numbers, downloaded from Qualtrics Ex: Correlation between partisanship and sharing Covid misinformation Ex: Conservative susceptibility to anxiety hypothesis collected abstracted or extracted from data postulated not explained explained by theories explain phenomena highly local to time, place, sample, procedure varying scope universal? “raw,” messy, unwieldy “processed,” clean, stylized “laws of nature?” 7.4 EDA as phenomena development cleaning messy data identifying and mitigating (where possible) errors and idiosyncracies identifying local patterns (“local phenomena”) not yet claiming these will be stable and appear elsewhere not yet worrying (much) about explanations 7.5 Better Models for EDA II: Epicycle of Analysis Figure 3.1: Epicycle of analysis model, Peng and Matsui (2016), 5 7.6 Peng and Matsui (2016) Data analysis is organized into 5 activities Each activity involves the same 3-step “epicycle” process Develop expectations Collect information Compare and revise expectations Not “the scientific method!” (Peng and Matsui 2016, 4) “Highly iterative and non-linear” \"information is learned at each step, which then informs whether (and how) to refine, and redo, the [previous] step …, or whether (and how) to proceed to the next step.\" 7.7 Aligning the goals of EDA with steps in the “epicycle of analysis” (Peng and Matsui 2016) Goals of EDA Epicycle step Determine if there are problems with the data 2. Collecting information Determine whether our question can be answered with these data 3. Comparing and revising expectations Develop sketch of an answer 1. Developing expectations 7.8 Discussion For each of these models, how well do they fit the way you’ve been taught to do science? How do they challenge the way you’ve been taught to do science? 7.9 References References "],["two-eda-checklists.html", "Chapter 8 Two EDA “Checklists” 8.1 Peng and Matsui (2016) 8.2 Huebner, Vach, and le Cessie (2016) 8.3 References", " Chapter 8 Two EDA “Checklists” 8.1 Peng and Matsui (2016) Formulate your question Read in your data Check the packaging Look at the top and the bottom of your data Check your “n”s Validate with at least one external data source Make a plot Try the easy solution first Follow up (Peng and Matsui 2016, 33) 8.2 Huebner, Vach, and le Cessie (2016) Duplicate records need to be eliminated Direction of numerical codes for categorical and ordinal variables Inconsistencies in date and time stamps Presence of bimodal distributions Presence of skewed distributions Presence of ceilings and floors in continuous variables Presence of outliers Distribution of missing data Indications of recording errors in the main variables (after Huebner, Vach, and le Cessie 2016, 26) 8.3 References References "],["inspecting-variables.html", "Chapter 9 Inspecting Variables 9.1 9.2 1. Formulate your question 9.3 Reflexivity 9.4 Set up our workspace 9.5 Packages 9.6 Get the Data 9.7 2. Read in your data 9.8 3. Check the packaging 9.9 For our motivating questions 9.10 Missing values 9.11 A critical question 9.12 4. Look at the top and the bottom of your data 9.13 5. Check your “n”s (and) 6. Validate with at least one external data source 9.14 From dates to years 9.15 7. Make a plot 9.16 8. Try the easy solution first 9.17 Number of stops, by race 9.18 Contraband search results, by race 9.19 9. Follow up 9.20 Discussion questions 9.21 Lab 9.22 References", " Chapter 9 Inspecting Variables 9.1 For this EDA, we’ll work with data on police stops in Oakland, California, that have been pre-cleaned and released by the Stanford Open Policing Project (Pierson et al. 2020). Because this analysis focuses on categorical data and counts of observations, most of the elements in Huebner, Vach, and le Cessie (2016) don’t really fit. So we’ll follow the checklist from Peng and Matsui (2016). We’ll also be learning to use the skimr and visdat packages 9.2 1. Formulate your question The Black Lives Matter protests over the last several years have made us aware of the racial aspects of policing. Here we’re specifically interested in Whether Black people in Oakland might be more likely to be stopped than White people Whether Black people who are stopped might be more likely to have contraband These aren’t very precise, but that’s okay: Part of the goal of EDA is to clarify and refine our research questions 9.3 Reflexivity Whether Black people in Oakland might be more likely to be stopped than White people Whether Black people who are stopped might be more likely to have contraband Once we have a rough idea of what we want to know, we need to take a moment to think about why we want to know it Clarify what “success” means to us Share with others to whom we’re accountable Recognize that we (academic researchers) often lack accountability to people who might be affected by our work especially when we claim to be acting for their benefit We’ll spend 3 minutes writing responses to each of these questions: What do I already know about this subject? Why am I studying this? What do I expect or hope to find/learn, and why? Who is affected by this topic, and how am I connected to them? (Adapted from Tanweer et al. (2021), 14-15, and Liboiron (2021)) 9.4 Set up our workspace Dedicated project folder Clean R session More on project management and organization later in the semester 9.5 Packages library(tidyverse) # for working with the data library(lubridate) # for working with datetime data library(skimr) # generate a text-based overview of the data library(visdat) # generate plots visualizing data types and missingness 9.6 Get the Data We’ll be using data on police stops in Oakland, California, collected and published by the Stanford Open Policing Project. For reproducibility, we’ll write a bit of code that automatically downloads the data To get the download URL: https://openpolicing.stanford.edu/data/ Scroll down to Oakland Right-click on the file symbol to copy the URL README: https://github.com/stanford-policylab/opp/blob/master/data_readme.md. data_dir = &#39;data&#39; target_file = file.path(data_dir, &#39;oakland.zip&#39;) if (!dir.exists(data_dir)) { dir.create(data_dir) } if (!file.exists(target_file)) { download.file(&#39;https://stacks.stanford.edu/file/druid:yg821jf8611/yg821jf8611_ca_oakland_2020_04_01.csv.zip&#39;, target_file) } 9.7 2. Read in your data The dataset is a zipped csv or comma-separated value file. CSVs are structured like Excel spreadsheets, but are stored in plain text rather than Excel’s format. dataf = read_csv(target_file) ## Rows: 133407 Columns: 28 ## ── Column specification ──────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (16): raw_row_number, location, beat, subject_race, subject_sex, officer_assignment, type, ... ## dbl (3): lat, lng, subject_age ## lgl (7): arrest_made, citation_issued, warning_issued, contraband_found, contraband_drugs, con... ## date (1): date ## time (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 9.8 3. Check the packaging Peng and Matsui (2016) use some base R functions to look at dimensions of the dataframe and column (variable) types. skimr is more powerful. ## May take a couple of seconds skim(dataf) ## ── Data Summary ──────────────────────── ## Values ## Name dataf ## Number of rows 133407 ## Number of columns 28 ## _______________________ ## Column type frequency: ## character 16 ## Date 1 ## difftime 1 ## logical 7 ## numeric 3 ## ________________________ ## Group variables None ## ## ── Variable type: character ──────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate min max empty n_unique whitespace ## 1 raw_row_number 0 1 1 71 0 133407 0 ## 2 location 51 1.00 1 78 0 60723 0 ## 3 beat 72424 0.457 3 19 0 129 0 ## 4 subject_race 0 1 5 22 0 5 0 ## 5 subject_sex 90 0.999 4 6 0 2 0 ## 6 officer_assignment 121431 0.0898 5 97 0 20 0 ## 7 type 20066 0.850 9 10 0 2 0 ## 8 outcome 34107 0.744 6 8 0 3 0 ## 9 search_basis 92250 0.309 5 14 0 3 0 ## 10 reason_for_stop 0 1 14 197 0 113 0 ## 11 use_of_force_description 116734 0.125 10 10 0 1 0 ## 12 raw_subject_sdrace 0 1 1 1 0 7 0 ## 13 raw_subject_resultofencounter 0 1 7 213 0 315 0 ## 14 raw_subject_searchconducted 0 1 2 24 0 34 0 ## 15 raw_subject_typeofsearch 52186 0.609 2 112 0 417 0 ## 16 raw_subject_resultofsearch 111633 0.163 5 95 0 298 0 ## ## ── Variable type: Date ───────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate min max median n_unique ## 1 date 2 1.00 2013-04-01 2017-12-31 2015-07-19 1638 ## ## ── Variable type: difftime ───────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate min max median n_unique ## 1 time 2 1.00 0 secs 86340 secs 16:12 1439 ## ## ── Variable type: logical ────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate mean count ## 1 arrest_made 0 1 0.121 FAL: 117217, TRU: 16190 ## 2 citation_issued 0 1 0.394 FAL: 80836, TRU: 52571 ## 3 warning_issued 0 1 0.231 FAL: 102545, TRU: 30862 ## 4 contraband_found 92250 0.309 0.149 FAL: 35005, TRU: 6152 ## 5 contraband_drugs 92250 0.309 0.0844 FAL: 37684, TRU: 3473 ## 6 contraband_weapons 92250 0.309 0.0299 FAL: 39928, TRU: 1229 ## 7 search_conducted 0 1 0.309 FAL: 92250, TRU: 41157 ## ## ── Variable type: numeric ────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ## 1 lat 114 0.999 37.8 0.0284 37.4 37.8 37.8 37.8 38.1 ▁▁▇▁▁ ## 2 lng 114 0.999 -122. 0.0432 -122. -122. -122. -122. -119. ▇▁▁▁▁ ## 3 subject_age 102724 0.230 33.2 13.3 10 23 29 41 97 ▇▆▃▁▁ 133k rows (observations); 28 columns (variables) 16 variables are handled as characters raw_row_number has 1 unique value per row So it’s probably some kind of identifier subject_race and subject_sex have just 5 and 2 unique values These are probably categorical variables represented as characters Similarly with type, outcome, and search_basis Though these have lots of missing values (high n_missing, low complete_rate) 1 variable represents the date, and another is difftime ?difftime tells us that difftime is used to represent intervals or “time differences” 7 logical variables A lot of these look like coded outcomes that we might be interested in, eg, search_conducted and contraband_found search_conducted has no missing values, but contraband_found has a lot of missing values 9.9 For our motivating questions Good: subject_race is 100% complete Also good: search_conducted is also 100% complete Potentially worrisome: contraband_found is only 31% complete 9.10 Missing values Let’s use visdat::vis_miss()to visualize missing values and check what’s up with contraband_found. ## This raises a warning about large data # vis_miss(dataf) ## So we&#39;ll use sample_n() to draw a subset set.seed(2021-09-28) dataf_smol = sample_n(dataf, 1000) vis_miss(dataf_smol) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## Please use `gather()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## Arguments in vis_miss() are useful for picking up patterns in missing values ## cluster = TRUE uses hierarchical clustering to order the rows vis_miss(dataf_smol, cluster = TRUE) Several variables related to search outcomes are missing together contraband_found, contraband_drugs, contraband_weapons, search_basis, use_of_force_description, raw_subject_typeofsearch, and raw_subject_resultofsearch However, search_conducted is complete 9.11 A critical question When a search has been conducted, do we know whether contraband was found? Semi-translated: when search_conducted == TRUE, is !is.na(contraband_found)? dataf %&gt;% filter(search_conducted) %&gt;% mutate(contraband_known = !is.na(contraband_found)) %&gt;% count(search_conducted, contraband_known) ## # A tibble: 1 × 3 ## search_conducted contraband_known n ## &lt;lgl&gt; &lt;lgl&gt; &lt;int&gt; ## 1 TRUE TRUE 41157 9.12 4. Look at the top and the bottom of your data With 28 columns, the dataframe is too wide to print in a readable way. We could use the select() function (from the tidyverse) to extract and print a few columns at a time. Instead we’ll use the base R function View() in an interactive session. This shows us an Excel-like spreadsheet presentation of a dataframe. View() can cause significant problems if you use it with a large dataframe on a slower machine. So we’ll use a pipe: first extract the head() or tail() of the dataset, and then View() it. We’ll also go ahead and view dataf_smol, the subset we created for visdat above. dataf %&gt;% head() %&gt;% View() dataf %&gt;% tail() %&gt;% View() View(dataf_smol) Some of my observations: The ID variable raw_row_number can’t be turned into a numeric value location is a mix of addresses and intersections (“Bond St @ 48TH AVE”) If we were going to generate a map using this column, geocoding might be tricky Fortunately we also get latitude and longitude columns use_of_force_description doesn’t seem to be a descriptive text field; instead it seems to be mostly missing or “handcuffed” We can also use skimr to check data quality by looking at the minimum and maximum values. Do these ranges make sense for what we expect the variable to be? skim(dataf) ## ── Data Summary ──────────────────────── ## Values ## Name dataf ## Number of rows 133407 ## Number of columns 28 ## _______________________ ## Column type frequency: ## character 16 ## Date 1 ## difftime 1 ## logical 7 ## numeric 3 ## ________________________ ## Group variables None ## ## ── Variable type: character ──────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate min max empty n_unique whitespace ## 1 raw_row_number 0 1 1 71 0 133407 0 ## 2 location 51 1.00 1 78 0 60723 0 ## 3 beat 72424 0.457 3 19 0 129 0 ## 4 subject_race 0 1 5 22 0 5 0 ## 5 subject_sex 90 0.999 4 6 0 2 0 ## 6 officer_assignment 121431 0.0898 5 97 0 20 0 ## 7 type 20066 0.850 9 10 0 2 0 ## 8 outcome 34107 0.744 6 8 0 3 0 ## 9 search_basis 92250 0.309 5 14 0 3 0 ## 10 reason_for_stop 0 1 14 197 0 113 0 ## 11 use_of_force_description 116734 0.125 10 10 0 1 0 ## 12 raw_subject_sdrace 0 1 1 1 0 7 0 ## 13 raw_subject_resultofencounter 0 1 7 213 0 315 0 ## 14 raw_subject_searchconducted 0 1 2 24 0 34 0 ## 15 raw_subject_typeofsearch 52186 0.609 2 112 0 417 0 ## 16 raw_subject_resultofsearch 111633 0.163 5 95 0 298 0 ## ## ── Variable type: Date ───────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate min max median n_unique ## 1 date 2 1.00 2013-04-01 2017-12-31 2015-07-19 1638 ## ## ── Variable type: difftime ───────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate min max median n_unique ## 1 time 2 1.00 0 secs 86340 secs 16:12 1439 ## ## ── Variable type: logical ────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate mean count ## 1 arrest_made 0 1 0.121 FAL: 117217, TRU: 16190 ## 2 citation_issued 0 1 0.394 FAL: 80836, TRU: 52571 ## 3 warning_issued 0 1 0.231 FAL: 102545, TRU: 30862 ## 4 contraband_found 92250 0.309 0.149 FAL: 35005, TRU: 6152 ## 5 contraband_drugs 92250 0.309 0.0844 FAL: 37684, TRU: 3473 ## 6 contraband_weapons 92250 0.309 0.0299 FAL: 39928, TRU: 1229 ## 7 search_conducted 0 1 0.309 FAL: 92250, TRU: 41157 ## ## ── Variable type: numeric ────────────────────────────────────────────────────────────────────────── ## skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ## 1 lat 114 0.999 37.8 0.0284 37.4 37.8 37.8 37.8 38.1 ▁▁▇▁▁ ## 2 lng 114 0.999 -122. 0.0432 -122. -122. -122. -122. -119. ▇▁▁▁▁ ## 3 subject_age 102724 0.230 33.2 13.3 10 23 29 41 97 ▇▆▃▁▁ Date range is April 1, 2013 to December 31, 2017 If we break things down by year, we should expect 2013 to have fewer cases For some purposes, we might need to exclude 2013 data: filter(dataf, date &gt;= '2014-01-01') Age range is from 10 years old (!) to 97 (!) Median (p50) is 29; 50% of values are between 23 and 41 For some purposes, we might need to restrict the analysis to working-age adults: filter(dataf, subject_age &gt;= 18, subject_age &lt; 65) 9.13 5. Check your “n”s (and) 6. Validate with at least one external data source Peng and Matsui (2016) use an air quality example with a regular sampling rate, so they can calculate exactly how many observations they should have. We can’t do that here So we’ll combine steps 5 and 6 together A web search leads us to this City of Oakland page on police stop data: https://www.oaklandca.gov/resources/stop-data The page mentions a Stanford study that was released in June 2016 Recall we got our data from the Stanford Open Policing Project Our data run through December 2017 So there’s a good chance we’re using a superset of the “Stanford study” data The page links to this report: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf Page 8 has two summary tables that we can compare to our data Figure 4.9: Screenshot of the two summary tables from the Oakland report. Source: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf, page 8 9.14 From dates to years Our data has the particular date of each stop We need to extract the year of each stop lubridate::year() does exactly this Filter to the years in our data that overlap with the tables And then aggregate by year (and gender) using count dataf %&gt;% mutate(year = year(date)) %&gt;% filter(year %in% c(2016, 2017)) %&gt;% count(year) ## # A tibble: 2 × 2 ## year n ## &lt;dbl&gt; &lt;int&gt; ## 1 2016 30268 ## 2 2017 30715 dataf %&gt;% mutate(year = year(date)) %&gt;% filter(year %in% c(2016, 2017)) %&gt;% count(year, subject_sex) ## # A tibble: 6 × 3 ## year subject_sex n ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 2016 female 7677 ## 2 2016 male 22563 ## 3 2016 &lt;NA&gt; 28 ## 4 2017 female 7879 ## 5 2017 male 22818 ## 6 2017 &lt;NA&gt; 18 For both years, we have fewer observations than the report table indicates Could our data have been pre-filtered? Let’s check the documentation for our data: https://github.com/stanford-policylab/opp/blob/master/data_readme.md#oakland-ca “Data is deduplicated on raw columns contactdate, contacttime, streetname, subject_sdrace, subject_sex, and subject_age, reducing the number of records by ~5.2%” The difference with the report is on this order of magnitude, But varies within groups by several percentage points So deduplication might explain the difference But in a more serious analysis we might want to check, eg, with the Stanford Open Policing folks ## Men in 2016 in the report vs. our data: 8.2% (24576 - 22563) / 24576 ## [1] 0.08190918 ## Women in 2016 in the report vs. our data: 3.6% (7965 - 7677) / 7965 ## [1] 0.03615819 ## All of 2016 in the report vs. our data: 7.1% (32569 - 30268) / 32569 ## [1] 0.07065 9.15 7. Make a plot Plotting is a whole additional discussion. We’ll talk about it next week. 9.16 8. Try the easy solution first We are interested in two rough questions: Whether Black people in Oakland might be more likely to be stopped than White people Whether Black people who are stopped might be more likely to have contraband The easy solution is to count rows and then shares within groups 9.17 Number of stops, by race dataf %&gt;% count(subject_race) %&gt;% mutate(share = n / sum(n)) %&gt;% arrange(desc(share)) ## # A tibble: 5 × 3 ## subject_race n share ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 black 78925 0.592 ## 2 hispanic 26257 0.197 ## 3 white 15628 0.117 ## 4 asian/pacific islander 8099 0.0607 ## 5 other 4498 0.0337 Police stop demographics 60% of subjects stopped by police are Black 19% are Hispanic 12% are White 6% are API Oakland demographics: https://en.wikipedia.org/wiki/Oakland,_California#Race_and_ethnicity 23% of residents are Black 27% are Hispanic 29% are non-Hispanic White 15% are Asian Comparing the two Blacks are severely overrepresented in police stops Hispanics and API folks are slightly underrepresented Whites are significantly underrepresented 9.18 Contraband search results, by race What fraction of stops had a search? Are there disparities by race there? ## What fraction of stops had a search? dataf %&gt;% count(search_conducted) %&gt;% mutate(share = n / sum(n)) ## # A tibble: 2 × 3 ## search_conducted n share ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 92250 0.691 ## 2 TRUE 41157 0.309 Across all subjects, 31% of stops involved a search. For the other question, we need to calculate something like share within racial groups. That is, among all stops of Black folks, what fraction involved a search? This is a just for group_by() Remember to ungroup() ## What fraction of stops had a search, by race? ## Note that, for each racial group, `share` adds up to 100%. ## ## For all groups, most stops didn&#39;t involve a search; ## For Black subjects, 38% of stops involved a search; ## For White subjects, 15% of stops involved a search. dataf %&gt;% group_by(subject_race) %&gt;% count(search_conducted) %&gt;% mutate(share = n / sum(n)) %&gt;% ungroup() %&gt;% filter(search_conducted) ## # A tibble: 5 × 4 ## subject_race search_conducted n share ## &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 asian/pacific islander TRUE 1304 0.161 ## 2 black TRUE 30025 0.380 ## 3 hispanic TRUE 6722 0.256 ## 4 other TRUE 706 0.157 ## 5 white TRUE 2400 0.154 For all groups, most stops didn’t involve a search For Black subjects, 38% of stops involved a search For White subjects, 15% of stops involved a search So police were much more likely to search stopped Black subjects than White subjects Finally, let’s consider our second main question: contraband results from searches. Note that we want to restrict ourselves to only stops where search_conducted is true. ## Contraband results from searches, by race dataf %&gt;% filter(search_conducted) %&gt;% count(subject_race, contraband_found) %&gt;% group_by(subject_race) %&gt;% mutate(share = n / sum(n)) %&gt;% ungroup() %&gt;% filter(contraband_found) ## # A tibble: 5 × 4 ## subject_race contraband_found n share ## &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 asian/pacific islander TRUE 176 0.135 ## 2 black TRUE 4364 0.145 ## 3 hispanic TRUE 1116 0.166 ## 4 other TRUE 85 0.120 ## 5 white TRUE 411 0.171 For Black subjects who were searched, contraband was found 15% of the time For White subjects, 17% of the time This preliminary analysis indicates that Black subjects were more likely to be searched than White subjects; but, when they were searched, White subjects were more likely to have contraband. 9.19 9. Follow up There are several directions we could take this analysis: Investigate outstanding questions about quality and reliability of the data eg, follow up with Stanford Open Policing Project about the difference in row counts Fits with epicycle of analysis: checking expectations Break down our question into more fine-grained analyses eg, the Oakland web site and report talk about policy changes; do we see changes by year in the data? Fits with epicycle of analysis: refine and specify research questions Apply more sophisticated statistical analysis eg, a regression model to control for age, gender, and other covariates Fits with phenomena development: reducing data, eliminating noise, in order to identity local phenomena 9.20 Discussion questions Suppose you’ve conducted this EDA because you’re working with an activist organization that promotes defunding the police and prison abolition. Should you share the preliminary findings above with your organization contacts? What influence should the following factors make to your answer? Funding: Whether you’re being paid as a consultant vs. volunteering your expertise Values: Your own views about policing and prisons Relationships: Whether you are friends with members of the activist organization and/or police Communications: The degree to which you can control whether and how the organization will publish your preliminary findings Timeliness: Whether these findings are relevant to a pending law or policy change What other factors should be taken into account as you decide whether to share your findings? Or not taken into account? How has this “raw data” been shaped by the journey of the data to get to us? 9.21 Lab The lab related to this material is available at https://github.com/data-science-methods/lab-w06-eda. 9.22 References References "],["visual-eda.html", "Chapter 10 Visual EDA 10.1 Reading 10.2 The Role of Visuals in EDA 10.3 In terms of our models of EDA 10.4 Plot Your Data 10.5 Bar plots, and better than bar plots 10.6 A bar plot of mean + 95% CI 10.7 Replace the bar with the data 10.8 Even fancier: Violin plot 10.9 References", " Chapter 10 Visual EDA 10.1 Reading Reading: Weissgerber et al. (2015) Further reading: Healy (2018), ch. 1; Matejka and Fitzmaurice (n.d.) 10.2 The Role of Visuals in EDA John Wilder Tukey (1977) emphasized visual methods for EDA Including not just graphs but also structured tables, such as stem-and-leaf displays Figure 3.1: A stem-and-leaf display, promoted by Tukey for use in EDA 10.3 In terms of our models of EDA 10.3.1 Epicycle of analysis Check expectations about the distribution of variables Outliers Degeneracies (eg, perfect correlation) Skew or bimodal distributions Non-linear relationships Develop expectations about relationships between variables 10.3.2 Phenomena development Quickly identify potential patterns Contrast potential patterns with noise/uncertainty/imprecision 10.4 Plot Your Data Summary statistics almost always focus only on central tendency (mean, median) and dispersion (standard deviation, IQR) This is all you would need if the world were made of normal distributions (Or at least unimodal symmetric ones) The world is not made of normal distributions (Lyon 2014) (Or even unimodal symmetric ones) We’ll illustrate this using The Datasaurus Dozen (Matejka and Fitzmaurice n.d.) library(tidyverse) library(datasauRus) library(ggforce) ## We&#39;ll need this, but don&#39;t want to load it ## install.packages(&#39;Hmisc&#39;) theme_set(theme_minimal()) ds_df = datasaurus_dozen The dataset combines 13 different datasets with \\(n=142\\) for each ds_df ## # A tibble: 1,846 × 3 ## dataset x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dino 55.4 97.2 ## 2 dino 51.5 96.0 ## 3 dino 46.2 94.5 ## 4 dino 42.8 91.4 ## 5 dino 40.8 88.3 ## 6 dino 38.7 84.9 ## 7 dino 35.6 79.9 ## 8 dino 33.1 77.6 ## 9 dino 29.0 74.5 ## 10 dino 26.2 71.4 ## # … with 1,836 more rows count(ds_df, dataset) ## # A tibble: 13 × 2 ## dataset n ## &lt;chr&gt; &lt;int&gt; ## 1 away 142 ## 2 bullseye 142 ## 3 circle 142 ## 4 dino 142 ## 5 dots 142 ## 6 h_lines 142 ## 7 high_lines 142 ## 8 slant_down 142 ## 9 slant_up 142 ## 10 star 142 ## 11 v_lines 142 ## 12 wide_lines 142 ## 13 x_shape 142 The datasets have the same means, standard deviations, and (Pearson) correlation coefficient p-value of the correlation coefficient is not statistically significant ## This is a more complex summarize() call than we&#39;ve seen before ## 1. Number of rows ## 2. &quot;Summarize across the columns x and y, using the functions mean and sd&quot;; automatically generates names ## 3. Correlation between x and y ## 4. p-value from a t-test of the null that the correlation = 0 ds_df %&gt;% group_by(dataset) %&gt;% summarize(n = n(), across(.cols = c(x, y), .fns = lst(mean, sd)), cor_xy = cor(x, y), p_value = cor.test(x, y)$p.value) %&gt;% ungroup() ## # A tibble: 13 × 8 ## dataset n x_mean x_sd y_mean y_sd cor_xy p_value ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 away 142 54.3 16.8 47.8 26.9 -0.0641 0.448 ## 2 bullseye 142 54.3 16.8 47.8 26.9 -0.0686 0.417 ## 3 circle 142 54.3 16.8 47.8 26.9 -0.0683 0.419 ## 4 dino 142 54.3 16.8 47.8 26.9 -0.0645 0.446 ## 5 dots 142 54.3 16.8 47.8 26.9 -0.0603 0.476 ## 6 h_lines 142 54.3 16.8 47.8 26.9 -0.0617 0.466 ## 7 high_lines 142 54.3 16.8 47.8 26.9 -0.0685 0.418 ## 8 slant_down 142 54.3 16.8 47.8 26.9 -0.0690 0.415 ## 9 slant_up 142 54.3 16.8 47.8 26.9 -0.0686 0.417 ## 10 star 142 54.3 16.8 47.8 26.9 -0.0630 0.457 ## 11 v_lines 142 54.3 16.8 47.8 26.9 -0.0694 0.412 ## 12 wide_lines 142 54.3 16.8 47.8 26.9 -0.0666 0.431 ## 13 x_shape 142 54.3 16.8 47.8 26.9 -0.0656 0.438 But, when plotted, they’re obviously very different ggplot(ds_df, aes(x, y)) + geom_point() + facet_wrap(vars(dataset)) 10.5 Bar plots, and better than bar plots Weissgerber et al. (2015) argue for the importance of plotting data, not just summaries, even in publications Let’s pull a few datasets out of datasaurus, as though they were groups in a study cdw_df = ds_df |&gt; filter(dataset %in% c(&#39;circle&#39;, &#39;dino&#39;, &#39;wide_lines&#39;)) 10.6 A bar plot of mean + 95% CI ggplot(cdw_df, aes(x = dataset, y = x)) + stat_summary(geom = &#39;errorbar&#39;, width = 0.2, fun.data = mean_cl_boot, fun.args = list(conf.int = .95)) + stat_summary(geom = &#39;bar&#39;, fun = mean, aes(fill = dataset)) + scale_fill_viridis_d(guide = &#39;none&#39;) ## Warning: Computation failed in `stat_summary()`: 10.7 Replace the bar with the data ggplot(cdw_df, aes(x = dataset, y = x)) + # geom_point(aes(color = dataset)) + geom_sina(aes(fill = dataset), shape = 21L) + stat_summary(geom = &#39;errorbar&#39;, width = 0.2, fun.data = mean_cl_boot) + scale_color_viridis_d(guide = &#39;none&#39;, aesthetics = c(&#39;color&#39;, &#39;fill&#39;)) ## Warning: Computation failed in `stat_summary()`: 10.8 Even fancier: Violin plot ggplot(cdw_df, aes(x = dataset, y = x, fill = dataset)) + geom_violin(draw_quantiles = c(.5)) + geom_sina(shape = 21, fill = &#39;white&#39;) + scale_fill_viridis_d(guide = &#39;none&#39;) 10.9 References References "],["the-grammar-of-graphics.html", "Chapter 11 The Grammar of Graphics 11.1 Reading 11.2 Artist’s palette model 11.3 Grammar of graphics 11.4 Graphs are maps 11.5 Graphs are models 11.6 References", " Chapter 11 The Grammar of Graphics 11.1 Reading Wickham, Navarro, and Pedersen (n.d.), ch. 13, “Mastering the Grammar” Further reading: L. Wilkinson (1999); Wickham (2010) 11.2 Artist’s palette model Many programming languages use the artist’s palette model of computer graphics Define some low-level tools for drawing on a canvas: points(), lines(), curve() One-liners that bundle these tools into standard plots: hist(), barplot() This is used by base graphics in R, Python packages including matplotlib and seaborn, MATLAB, etc. In R: base graphics are side effects, not first-class objects Figure 3.1: matplotlib, a popular Python package for graphing, uses the artist’s palette model. Source: https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplot.html 11.3 Grammar of graphics The grammar of graphics is a fundamentally different approach Implemented by ggplot2 in the tidyverse Figure 4.1: The Grammar of Graphics regards graphs as mappings from variables in the data to features of geometric objects. This means that every graph is a model. 11.4 Graphs are maps A graph is a mapping from variables in the data to features of geometric objects An individual mapping is called an aesthetic So a graph is a collection of aesthetics applied to a dataset oakland_df = read_csv(file.path(&#39;data&#39;, &#39;oakland.zip&#39;), show_col_types = FALSE) ggplot(data = oakland_df, aes(x = subject_race, fill = search_conducted)) + geom_bar(position = &#39;fill&#39;) + scale_fill_brewer(palette = &#39;Set1&#39;) 11.5 Graphs are models Simplifications of the data That provide us with cognitive affordances Fit with our cognitive capabilities Support pattern detection, inference Direct our attention to certain things and away from others Involve researcher degrees of freedom Consider the bar plots vs. dots + CI plots vs. violin plots from the last chapter How do these simplify the data? In doing so, how do they provide us with cognitive affordances? How do these different in terms of where they direct our attention? What researcher degrees of freedom does each involve? 11.6 References References "],["covid-eda.html", "Chapter 12 Covid EDA 12.1 12.2 The standard narrative 12.3 Reflexivity 12.4 Setup 12.5 Data 12.6 Get the data 12.7 Some quick data checking 12.8 Variables 12.9 Too many timeseries plots 12.10 Number of observations per county 12.11 No more than 1 observation per county per day 12.12 Cumulative vs. daily cases 12.13 Add an assertion 12.14 Distribution of cases by county 12.15 Revisions 12.16 9 plots of the same plot 12.17 Why so many dropped values? 12.18 Cases vs. population 12.19 Absolute counts to rates 12.20 Rates vs. population 12.21 Counties by maximum rate of new cases 12.22 Cases vs. deaths when we control for population", " Chapter 12 Covid EDA 12.1 This EDA will start in class and continue on to the lab We’re interested in the role of social distancing in the July 2020 Covid wave in California 12.2 The standard narrative The standard narrative about the July 2020 wave goes like this: California had some of the first confirmed cases of Covid-19 in the US California was also the first state to institute a stay-at-home order and encourage social distancing These social distancing policies are why California did not experience the large first wave in March 2020 (contrast NYC) Starting in May 2020, these policies were relaxed and “lockdown fatigue” meant that people were ignoring them anyways This is why California did experience a more significant wave in July 2020 Our research question: Is reduced social distancing (measured using cellphone tracking data) correlated with increased Covid-19 case counts 4 weeks later? In class, we’ll just be looking at the case counts. 12.3 Reflexivity (For time, you’ll do these on your own as part of the lab.) 12.4 Setup library(tidyverse) library(tidylog) library(skimr) library(visdat) library(assertthat) theme_set(theme_bw()) 12.5 Data To save some time in class, I’ve prepared a CSV that combines three datasets: Covid-19 case and death counts, at the county level, collected by the New York Times Estimated county populations in 2018, from the US Census’ American Community Survey “Mobility data,” based on cellphone tracking, from Google The dataset has also been filtered down to California. 12.6 Get the data This week’s lab on GitHub https://github.com/data-science-methods/lab_w06_covid data -&gt; covid.csv -&gt; “Download” (right-click and copy) covid_file = file.path(&#39;data&#39;, &#39;covid.csv&#39;) covid_url = &#39;https://github.com/data-science-methods/lab_w06_covid/raw/main/data/covid.csv&#39; if (!file.exists(covid_file)) { download.file(covid_url, covid_file) } ## Original version # covid_df = read_csv(covid_file, show_col_types = FALSE) ## With daily change covid_df = read_csv(covid_file, show_col_types = FALSE) |&gt; group_by(county) |&gt; mutate(across(.cols = c(cases, deaths), .fns = list(new = daily_new), date)) |&gt; ungroup() |&gt; mutate(across(.cols = c(matches(&#39;cases&#39;), matches(&#39;deaths&#39;)), .fns = list(rate = ~ .x / population * 100000))) ## group_by: one grouping variable (county) ## mutate (grouped): new variable &#39;cases_new&#39; (double) with 1,737 unique values and &lt;1% NA ## new variable &#39;deaths_new&#39; (double) with 168 unique values and &lt;1% NA ## ungroup: no grouping variables ## mutate: new variable &#39;cases_rate&#39; (double) with 24,351 unique values and 0% NA ## new variable &#39;cases_new_rate&#39; (double) with 9,753 unique values and &lt;1% NA ## new variable &#39;deaths_rate&#39; (double) with 8,485 unique values and 0% NA ## new variable &#39;deaths_new_rate&#39; (double) with 1,056 unique values and &lt;1% NA 12.7 Some quick data checking skim(covid_df) Table 12.1: Data summary Name covid_df Number of rows 33322 Number of columns 19 _______________________ Column type frequency: character 3 Date 1 numeric 15 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace county 0 1 4 15 0 58 0 state 0 1 10 10 0 1 0 fips 0 1 5 5 0 58 0 Variable type: Date skim_variable n_missing complete_rate min max median n_unique date 0 1 2020-01-25 2021-10-12 2020-12-29 627 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist cases 0 1.00 38007.85 128692.55 1.00 479.25 4458.00 23744.50 1473123.00 ▇▁▁▁▁ deaths 0 1.00 590.84 2290.48 0.00 5.00 55.00 299.00 26346.00 ▇▁▁▁▁ population 0 1.00 708598.07 1508235.90 1146.00 53932.00 186661.00 765935.00 10098052.00 ▇▁▁▁▁ retail 15286 0.54 -25.50 16.78 -100.00 -35.00 -25.00 -16.00 63.00 ▁▃▇▁▁ grocery 16225 0.51 -5.84 13.46 -74.00 -14.00 -7.00 1.00 86.00 ▁▆▇▁▁ parks 18301 0.45 2.16 40.35 -83.00 -22.00 -3.00 18.00 323.00 ▇▆▁▁▁ transit 18639 0.44 -29.67 27.29 -88.00 -49.00 -32.00 -15.00 97.00 ▃▇▂▁▁ workplaces 14112 0.58 -30.74 15.01 -87.00 -39.00 -31.00 -20.00 11.00 ▁▂▇▇▁ residential 17022 0.49 10.82 6.33 -7.00 6.00 10.00 14.00 37.00 ▁▇▆▂▁ cases_new 58 1.00 144.62 680.67 -1157.00 0.00 13.00 77.00 29174.00 ▇▁▁▁▁ deaths_new 58 1.00 2.11 12.92 -423.00 0.00 0.00 1.00 930.00 ▁▇▁▁▁ cases_rate 0 1.00 4714.32 4521.80 0.01 541.96 3567.44 7764.35 23466.41 ▇▃▂▁▁ cases_new_rate 58 1.00 20.11 45.88 -211.66 0.00 6.53 21.91 2055.69 ▇▁▁▁▁ deaths_rate 0 1.00 58.83 65.99 0.00 5.53 38.53 88.80 422.27 ▇▂▁▁▁ deaths_new_rate 58 1.00 0.24 1.04 -31.56 0.00 0.00 0.05 48.63 ▁▇▁▁▁ 12.8 Variables county, state, and fips are all geographic identifiers, and complete 58 unique values for both county and fips How many counties does California have? date gives the date, is also complete cases and deaths come from the NYT covid data population comes from the Census retail, grocery, parks, transit, workplaces, and residential come from the Google Mobility data Much lower completion rate 12.9 Too many timeseries plots ggplot(covid_df, aes(date, cases)) + geom_line() + facet_wrap(vars(county), scales = &#39;free_y&#39;) 12.10 Number of observations per county ggplot(covid_df, aes(fct_rev(fct_infreq(county)))) + # geom_bar() + geom_point(stat = &#39;count&#39;) + geom_segment(stat = &#39;count&#39;, aes(xend = county), yend = 0) + coord_flip() + labs(x = &#39;county&#39;) Why do different counties have different numbers of observations? 12.11 No more than 1 observation per county per day covid_df |&gt; count(county, date) |&gt; filter(n &gt; 1) |&gt; nrow() |&gt; identical(0L) |&gt; assert_that(msg = &#39;More than 1 observation per county per day&#39;) ## count: now 33,322 rows and 3 columns, ungrouped ## filter: removed all rows (100%) ## [1] TRUE 12.12 Cumulative vs. daily cases Both cases and deaths are cumulative, not the daily new value covid_df |&gt; filter(county == &#39;Merced&#39;) |&gt; ggplot(aes(date, cases)) + geom_line() ## filter: removed 32,754 rows (98%), 568 rows remaining We’ll write a little function to calculate the differences Incorporate it into the pipe that loads the data Then restart R and rerun daily_new = function(x, order_var) { diff = x - dplyr::lag(x, order_by = order_var) return(diff) } Test it on Orange and LA Counties covid_df |&gt; filter(county %in% c(&#39;Orange&#39;, &#39;Los Angeles&#39;)) |&gt; group_by(county) |&gt; mutate(across(.cols = c(cases, deaths), .fns = list(new = daily_new), date)) |&gt; select(date, county, matches(&#39;cases&#39;), matches(&#39;deaths&#39;)) |&gt; View() Now we have daily values covid_df |&gt; filter(county == &#39;Merced&#39;) |&gt; ggplot(aes(date, cases_new)) + geom_line() ## filter: removed 32,754 rows (98%), 568 rows remaining ## Warning: Removed 1 row(s) containing missing values (geom_path). 12.13 Add an assertion For each county, the first daily diff should be NA But none of the others covid_df |&gt; group_by(county) |&gt; slice(-1) |&gt; pull(cases_new) |&gt; is.na() |&gt; any() |&gt; magrittr::not() |&gt; assert_that(msg = &#39;missing values in cases_new&#39;) ## group_by: one grouping variable (county) ## slice (grouped): removed 58 rows (&lt;1%), 33,264 rows remaining ## [1] TRUE 12.14 Distribution of cases by county First pass is hard to read ggplot(covid_df, aes(county, cases_new)) + geom_boxplot() ## Warning: Removed 58 rows containing non-finite values (stat_boxplot). 12.15 Revisions coef = 1000 y-axis on a log scale flip the coordinates reorder the counties by median number of cases filter out cases_new == 0 meaningful axis labels covid_df |&gt; filter(cases_new &gt; 0) |&gt; ggplot(aes(fct_reorder(county, cases_new, .fun = median, na.rm = TRUE), cases_new)) + geom_boxplot(coef = 1000) + scale_y_log10() + coord_flip() + labs(x = &#39;county&#39;, y = &#39;daily new cases&#39;) ## filter: removed 9,133 rows (27%), 24,189 rows remaining 12.16 9 plots of the same plot ggplot(covid_df, aes(x = cases_new, y = deaths_new)) + # geom_point() + # geom_point(alpha = .2) + # geom_count(alpha = .5) + # geom_bin2d() + # geom_hex() + # geom_hex(aes(color = after_stat(count))) + # geom_density2d(size = 1) + # stat_density2d(contour = FALSE, geom = &#39;raster&#39;, # aes(fill = after_stat(density)), # show.legend = FALSE) + stat_density2d(contour = TRUE, geom = &#39;polygon&#39;, aes(fill = after_stat(level)), show.legend = FALSE) + scale_x_log10() + scale_y_log10() + scale_color_viridis_c(aesthetics = c(&#39;color&#39;, &#39;fill&#39;)) ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous x-axis ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 25048 rows containing non-finite values (stat_density2d). 12.17 Why so many dropped values? The binned plots drop 25,048 rows, or 75% of the data Why? covid_df |&gt; mutate(log_cases_new = log(cases_new)) |&gt; select(cases_new, log_cases_new) |&gt; filter(!is.finite(log_cases_new)) |&gt; count(cases_new) |&gt; arrange(desc(n)) ## Warning in log(cases_new): NaNs produced ## mutate: new variable &#39;log_cases_new&#39; (double) with 1,682 unique values and 1% NA ## select: dropped 18 variables (date, county, state, fips, cases, …) ## filter: removed 24,189 rows (73%), 9,133 rows remaining ## count: now 58 rows and 2 columns, ungrouped ## # A tibble: 58 × 2 ## cases_new n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 8829 ## 2 -1 113 ## 3 NA 58 ## 4 -2 27 ## 5 -3 11 ## 6 -6 9 ## 7 -8 8 ## 8 -17 4 ## 9 -12 4 ## 10 -5 4 ## # … with 48 more rows 12.18 Cases vs. population ggplot(covid_df, aes(population, cases_new)) + geom_point() ## Warning: Removed 58 rows containing missing values (geom_point). 12.19 Absolute counts to rates cases and deaths are absolute counts But county populations differ over orders of magnitude Construct a lollipop plot to illustrate this covid_df |&gt; select(county, population) |&gt; distinct() |&gt; ggplot(aes(x = fct_reorder(county, population), y = population)) + geom_point() + geom_segment(aes(xend = county), yend = 0) + scale_y_log10() + coord_flip() ## select: dropped 17 variables (date, state, fips, cases, deaths, …) ## distinct: removed 33,264 rows (&gt;99%), 58 rows remaining Let’s calculate rates for new and cumulative cases and deaths test_df = covid_df |&gt; mutate(across(.cols = c(matches(&#39;cases&#39;), matches(&#39;deaths&#39;)), .fns = list(rate = ~ .x / population * 100000))) ## mutate: new variable &#39;cases_rate_rate&#39; (double) with 24,355 unique values and 0% NA ## new variable &#39;cases_new_rate_rate&#39; (double) with 9,755 unique values and &lt;1% NA ## new variable &#39;deaths_rate_rate&#39; (double) with 8,485 unique values and 0% NA ## new variable &#39;deaths_new_rate_rate&#39; (double) with 1,056 unique values and &lt;1% NA test_df |&gt; filter(county %in% c(&#39;Merced&#39;, &#39;Fresno&#39;, &#39;Los Angeles&#39;)) |&gt; ggplot(aes(date, cases_new_rate, group = county, color = county)) + geom_line() ## filter: removed 31,543 rows (95%), 1,779 rows remaining ## Warning: Removed 3 row(s) containing missing values (geom_path). Incorporate this into the pipe when we load covid_df 12.20 Rates vs. population ggplot(covid_df, aes(population, cases_new_rate)) + geom_point() ## Warning: Removed 58 rows containing missing values (geom_point). 12.21 Counties by maximum rate of new cases covid_df |&gt; group_by(county) |&gt; summarize(population = max(population), cases = max(cases_new_rate, na.rm = TRUE)) |&gt; arrange(desc(cases)) ## group_by: one grouping variable (county) ## summarize: now 58 rows and 3 columns, ungrouped ## # A tibble: 58 × 3 ## county population cases ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Tulare 460477 2056. ## 2 Alpine 1146 1832. ## 3 Imperial 180216 1640. ## 4 Lassen 31185 1260. ## 5 Shasta 179085 1032. ## 6 Tehama 63373 1023. ## 7 Fresno 978130 950. ## 8 Glenn 27897 921. ## 9 Butte 227075 875. ## 10 Kings 150075 782. ## # … with 48 more rows 12.22 Cases vs. deaths when we control for population ggplot(covid_df, aes(x = cases_new_rate, y = deaths_new_rate)) + # geom_point() + geom_point(alpha = .2) + # geom_count(alpha = .5) + # geom_bin2d() + # geom_hex() + # geom_hex(aes(color = after_stat(count))) + # geom_density2d(size = 1) + # stat_density2d(contour = FALSE, geom = &#39;raster&#39;, # aes(fill = after_stat(density)), # show.legend = FALSE) + # stat_density2d(contour = TRUE, geom = &#39;polygon&#39;, # aes(fill = after_stat(level)), # show.legend = FALSE) + scale_x_log10() + scale_y_log10() + scale_color_viridis_c(aesthetics = c(&#39;color&#39;, &#39;fill&#39;)) ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous x-axis ## Warning in self$trans$transform(x): NaNs produced ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 556 rows containing missing values (geom_point). "],["readable-code-is-reliable-code.html", "Chapter 13 Readable code is reliable code 13.1 Reading 13.2 Writing code is writing 13.3 Code style 13.4 Highlights from the Tidyverse style guide 13.5 Spaces 13.6 Code blocks 13.7 Boolean variables vs. control flow 13.8 References", " Chapter 13 Readable code is reliable code 13.1 Reading (BryanCodeSmellsFeels2018?) Postolovski (2020) Buckens (2019) 13.2 Writing code is writing Multiple target audiences Collaborators (Some) reviewers and readers of the paper Peers who want to analyze and extend your methods Yourself in two months/one year/five years Your code is readable to the extent that people can use it to easily and reliably predict, diagnose, and extend your code 13.3 Code style iNéwritteNélanguagEáconventionSéfoRîpunctuatioNøcapitalizatioNîaiDécomprehensioNébYéindicatinGéstructurE this is what it’s like to read poorly-styled code conventions only work if they’re shared conventions Style guides provide shared conventions for readable code In-line spacing makes it easier to pick out distinguish functions, operators, and variables in a line Returns distinguish arguments in a function call Indentation corresponds to structure of complex expressions Common conventions for naming, assignment reduce cognitive load Tidyverse style guide: https://style.tidyverse.org/ 13.4 Highlights from the Tidyverse style guide Place all package() calls at the top of the script Limit your code to 80 characters per line Use at least 4 spaces for indenting multiline expression Control-I in RStudio will do automagic indenting In multiline function calls, 1 argument = 1 line Use comments to explain “why” rather than “what” or “how” DH’s comment convention: Single # is for code that isn’t currently used but might need to be brought back (eg, for debugging) ## is for substantial comments 13.5 Spaces Always put spaces after commas, and never before (like English) But not between a function name and the parentheses (like math) And surrounding infix operators (==, +, -, &lt;-, =) Pipes %&gt;% |&gt; should have a space before and be at the end of the line 13.6 Code blocks When you put a block of code in curly braces {}: { should be the last character on a line } should be the first character on the line if (y == 0) { if (x &gt; 0) { log(x) } else { message(&quot;x is negative or zero&quot;) } } else { y^x } 13.7 Boolean variables vs. control flow Functions that return vectors: &amp;, |, ==, ifelse(), dplyr::if_else() Functions that return a single value: &amp;&amp;, ||, identical if (x) a else b only looks at the first (hopefully single) value of x 13.8 References References "],["a-code-review-checklist.html", "Chapter 14 A code review checklist 14.1 Reading 14.2 1. Readability / understandability 14.3 2. Maintainability 14.4 3. Security 14.5 4. Speed and Performance 14.6 5. Documentation 14.7 6. Reinventing the Wheel 14.8 7. Reliability 14.9 Skipping a few 14.10 13. Notice what’s missing 14.11 14. Zoom out 14.12 References", " Chapter 14 A code review checklist 14.1 Reading Postolovski (2020) Readability Maintainability Security Speed and Performance Documentation Reinventing the Wheel Reliability Scalability Reusability Patterns [style and readability] Test coverage and quality Fit for purpose Notice what’s missing Zoom out 14.2 1. Readability / understandability File naming What order should the scripts be run in? Follows a style guide Or at least uses consistent style Whitespace, whitespace, whitespace Meaningful names / self-documenting code (Buckens 2019) For data science: Functionality promotes understandability 14.3 2. Maintainability DRY Avoid hard-coded configuration Check strong assumptions about structure and content of inputs Avoid deprecated or removed features 14.4 3. Security How are passwords for websites, APIs, and sensitive data handled? 14.5 4. Speed and Performance 14.5.1 Performance of the script Avoid (repeating) large downloads Cache the results of slow calculations With an easy way to clear the cache and re-run the calculations Or break them up in to separate steps of the pipeline Use tictoc for simple wall time checks 14.5.2 Resource use Does this really need to be run on a compute cluster? How much will this cost if it’s accidentally run 1,000 times? Cache raw returns from web scraping and API queries 14.6 5. Documentation Scripts: Section markers, signposting what’s happening and why Functions: Assumptions about inputs, explanation of output Consider using roxygen2 + R’s package infrastructure https://r-pkgs.org/man.html#man-functions Data: Provenance, date and time retrieved, codebook Project: README, NEWS 14.7 6. Reinventing the Wheel Is there a (well-maintained, checked) package for this? DRY 14.8 7. Reliability What happens if Packages aren’t installed? Reminder: require() doesn’t stop if the package is missing Data can’t be found? Data has missing values or errors? Data has been altered or corrupted? A function returns a missing or zero-length value? One script in the middle of the pipeline is altered? 14.9 Skipping a few Because I have less to say, not because they’re less important Scalability Reusability Patterns [style and readability] Test coverage and quality Fit for purpose 14.10 13. Notice what’s missing Using packages that haven’t been loaded Data validation Edge cases, unexpected inputs Missing values Error handling 14.11 14. Zoom out Does the code fit the analytic approach described in the paper? Does the analytic approach fit the the research question? 14.12 References References "],["the-life-changing-magic-of-tidying-your-projects.html", "Chapter 15 The life-changing magic of tidying your projects 15.1 Reading 15.2 Dumpster organization 15.3 Project organization 15.4 Model 1: Noble (2009) 15.5 Model 2: Some of DH’s projects 15.6 Just the directories 15.7 A larger text-mining project 15.8 Just the directories 15.9 DH’s Project Template 15.10 A reminder on paths 15.11 References", " Chapter 15 The life-changing magic of tidying your projects 15.1 Reading Noble (2009) 15.1.1 Some data management disasters The Economist (2011), “Video: Keith Baggerly, \"When Is Reproducibility an Ethical Issue? Genomics, Personalized Medicine, and Human Error\"” (n.d.) Herndon, Ash, and Pollin (2014), but you can just read Bailey and Borwein (Jon) (n.d.), Cassidy (n.d.), and/or watch Reinhart &amp; Rogoff - Growth in a Time of Debt - EXERCISE! (2019) Laskowski (n.d.), Viglione (2020), Pennisi (2020) “How Excel May Have Caused Loss of 16,000 Covid Tests in England” (2020) 15.2 Dumpster organization Figure 3.1: 😱 Source: https://pbs.twimg.com/media/DFca5SRXsAAx1NA Dump all of your files into one place Use search tools to find what you want Just assume that things aren’t getting corrupted The way many Gen Z students think about their files? (Chin 2021) 15.3 Project organization Keep your project self-contained Locate files quickly Play nicely with version control Self-document key relationships between project files Figure 4.1: Folder organization is your friend. I have top-level folders for teaching, coding, and writing projects. Then each project or collection of projects lives in its own folder. An ‘Archives’ folder is good for tucking old projects out of the way. 15.4 Model 1: Noble (2009) Figure 4.2: Noble’s (2009) sample folder structure is designed for experimental biologists. Includes code for running experiments cleaning and analyzing data typesetting a LaTeX file into a PDF Other features notebook file, updated regularly chronological folders for experimental runs bin for compiled code and source for corresponding source files 15.5 Model 2: Some of DH’s projects GitHub repo: https://github.com/dhicks/p_curve 14 directories, 121 files . ├── DESCRIPTION ├── Makefile ├── README.md ├── _deploy.sh ├── out │   ├── estimates_meta.png │   ├── estimates_study.png │   ├── fig_1_samples_young.png │   ├── fig_2_young_composite.png │   ├── fig_3_evidence_severity.png │   ├── fig_4_evidence_likelihood_zero.png │   ├── fig_5_evidence_likelihood_mix.png │   ├── likelihood.tex │   ├── linearity.png │   ├── linearity.tex │   ├── samples_schsp.png │   ├── samples_simonsohn.png │   ├── severity.tex │   ├── slopes.png │   ├── slopes.tex │   ├── slopes_scatter.png │   ├── test.png │   └── test_out.png ├── p.curve │   ├── DESCRIPTION │   ├── NAMESPACE │   ├── R │   │   └── p-curve.R │   ├── man │   │   ├── draw_samples.Rd │   │   ├── draw_studies.Rd │   │   ├── flatten_to_chr.Rd │   │   ├── likelihood_ratio.Rd │   │   ├── many_metas.Rd │   │   ├── p_gap.Rd │   │   ├── p_value.Rd │   │   ├── qq_linear.Rd │   │   ├── qq_plot.Rd │   │   ├── qq_slope.Rd │   │   ├── schsp_curve.Rd │   │   ├── schsp_slope.Rd │   │   ├── simonsohn_curve.Rd │   │   ├── t_test.Rd │   │   ├── young_composite.Rd │   │   ├── young_curve.Rd │   │   └── young_slope.Rd │   └── p.curve.Rproj ├── paper │   ├── *enviro\\ epi │   │   └── EE\\ Submission\\ Confirmation\\ for\\ Young&#39;s\\ p-value\\ plot\\ does\\ not\\ provide\\ evidence\\ against\\ air\\ pollution\\ hazards.eml │   ├── Young\\ papers.gsheet │   ├── Young.bib │   ├── cover\\ letter.pdf │   ├── diff.pdf │   ├── ehp │   │   ├── A\\ manuscript\\ number\\ has\\ been\\ assigned\\ to\\ Young&#39;s\\ p-value\\ plot\\ does\\ not\\ provide\\ evidence\\ against\\ air\\ pollution\\ hazards\\ -\\ [EMID_0ef854c3bb0b5cae].eml │   │   ├── Decision\\ on\\ EHP\\ Submission\\ EHP8013\\ -\\ [EMID_932e44ac2192c44f].eml │   │   ├── EHP-CFI-form.pdf │   │   ├── cover\\ letter.txt │   │   ├── paper_2020-07-31.docx │   │   └── title\\ page.md │   ├── example-refs.bib │   ├── fig_1_samples_young.png │   ├── fig_2_young_composite.png │   ├── fig_3_evidence_severity.png │   ├── fig_4_evidence_likelihood_zero.png │   ├── fig_5_evidence_likelihood_mix.png │   ├── header.yaml │   ├── paper.md │   ├── paper.pdf │   ├── paper.synctex.gz │   ├── paper.tex │   ├── paper.zip │   ├── paper_20201211.md │   ├── peerj │   │   ├── comments.md │   │   └── peerj.pdf │   ├── phil\\ med │   │   ├── [philmed]\\ Editor\\ Decision.eml │   │   └── [philmed]\\ Submission\\ Acknowledgement.eml │   ├── render.R │   ├── summary.md │   ├── summary.pdf │   ├── summary.tex │   ├── supplement.md │   ├── supplement.pdf │   ├── title.md │   ├── title.pdf │   ├── vancouver-superscript.csl │   └── wlpeerj.cls ├── scripts │   ├── Makefile │   ├── run_metas.R │   ├── run_metas.html │   ├── run_metas_cache │   │   └── html │   │   ├── __packages │   │   ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.RData │   │   ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.rdb │   │   ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.rdx │   │   ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.RData │   │   ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.rdb │   │   ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.rdx │   │   ├── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.RData │   │   ├── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.rdb │   │   └── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.rdx │   ├── run_metas_files │   │   └── figure-html │   │   ├── QQ\\ linearity\\ tests-1.png │   │   ├── gaps-1.png │   │   ├── gaps-2.png │   │   ├── likelihood\\ analysis-1.png │   │   ├── likelihood\\ analysis-3.png │   │   ├── model\\ validation-1.png │   │   ├── model\\ validation-2.png │   │   ├── power_sim-1.png │   │   ├── power_sim-2.png │   │   ├── sample\\ plots-1.png │   │   ├── sample\\ plots-2.png │   │   ├── sample\\ plots-3.png │   │   ├── sample\\ plots-4.png │   │   ├── severity\\ analysis-1.png │   │   ├── severity\\ analysis-2.png │   │   ├── slopes-1.png │   │   ├── slopes-2.png │   │   ├── slopes-3.png │   │   ├── slopes-4.png │   │   ├── slopes-5.png │   │   ├── slopes-6.png │   │   ├── slopes-7.png │   │   ├── unnamed-chunk-2-1.png │   │   ├── unnamed-chunk-4-1.png │   │   └── unnamed-chunk-6-1.png │   └── scripts.Rproj └── tree.md 15.6 Just the directories . ├── out ├── p.curve │   ├── R │   └── man ├── paper │   ├── *enviro\\ epi │   ├── ehp │   ├── peerj │   └── phil\\ med └── scripts ├── run_metas_cache │   └── html └── run_metas_files └── figure-html scripts, paper, and out p.curve, a little package containing the simulation code simulation and analysis automatically reproduced: https://dhicks.github.io/p_curve/ 15.7 A larger text-mining project Published paper: https://doi.org/10.1162/qss_a_00150 GitHub repo: https://github.com/dhicks/orus 23 directories, 274 files (plus 160k data files) . ├── Makefile ├── ORU\\ faculty │   ├── ORU\\ Faculty.docx │   ├── ORU\\ Faculty.html │   ├── ORU\\ Publications.docx │   ├── ORU\\ Publications.fld │   │   ├── colorschememapping.xml │   │   ├── filelist.xml │   │   ├── header.html │   │   ├── image001.png │   │   ├── item0001.xml │   │   ├── props002.xml │   │   └── themedata.thmx │   ├── ORU\\ Publications.html │   └── auids.csv ├── ORU\\ founding\\ dates.gsheet ├── QSS\\ forms │   ├── QSS-Checklist-AcceptedManuscripts.docx │   ├── QSS_pub_agreement.pdf │   └── Quantitative\\ Science\\ Studies\\ -\\ Decision\\ on\\ Manuscript\\ ID\\ QSS-2021-0014.R2.eml ├── R │   ├── api_keys.R │   └── hellinger.R ├── auid\\ flow.txt ├── data │   ├── *ORUs\\ -\\ DSL\\ -\\ Google\\ Drive.webloc │   ├── 00_UCD_2016.csv │   ├── 00_UCD_2017.csv │   ├── 00_UCD_2018.csv │   ├── 00_faculty_list.html │   ├── 00_manual_matches.csv │   ├── 00_publications_list.html │   ├── 01_departments.csv │   ├── 01_departments_canonical.csv │   ├── 01_faculty.Rds │   ├── 02_pubs.Rds │   ├── 03_codepartmentals.Rds │   ├── 03_dropout.Rds │   ├── 03_matched.Rds │   ├── 03_unmatched.Rds │   ├── 04_author_meta.Rds │   ├── 04_dropouts.Rds │   ├── 04_genderize │   ├── 04_namsor.Rds │   ├── 05_author_meta.Rds │   ├── 05_dept_dummies.Rds │   ├── 05_dropouts.Rds │   ├── 05_layout.Rds │   ├── 05_matched.Rds │   ├── 06_author_histories.Rds │   ├── 07_coauth_count.Rds │   ├── 07_parsed_histories.Rds │   ├── 08_phrases.Rds │   ├── 09_H.Rds │   ├── 09_atm.csv │   ├── 09_vocab.tex │   ├── 10_atm.csv │   ├── 10_atm_pc.Rds │   ├── 10_aytm.csv │   ├── 10_aytm_comp.csv │   ├── 10_aytm_did.csv │   ├── 10_model_stats.Rds │   ├── 10_models.Rds │   ├── 11_au_dept_xwalk.Rds │   ├── 11_departments.csv │   ├── 11_departments_canonical.csv │   ├── 11_dept_dummies.Rds │   ├── 11_dept_gamma.Rds │   ├── 11_dept_term_matrix.Rds │   ├── 11_oru_gamma.Rds │   ├── 11_oru_term_matrix.Rds │   ├── 11_test_train.Rds │   ├── 12_layout.Rds │   ├── author_histories [7665 entries exceeds filelimit, not opening dir] │   ├── authors_meta [6020 entries exceeds filelimit, not opening dir] │   ├── docs [145144 entries exceeds filelimit, not opening dir] │   ├── ldatuning_results │   │   ├── tuningResult_comp.Rds │   │   ├── tuningResult_comp.docx │   │   ├── tuningResult_comp.pdf │   │   ├── tuningResult_did.Rds │   │   └── tuningResult_did.pdf │   ├── ldatuning_results-20190415T164055Z-001.zip │   ├── parsed_blocks [430 entries exceeds filelimit, not opening dir] │   ├── pubs [282 entries exceeds filelimit, not opening dir] │   └── temp ├── interdisciplinarity\\ project\\ notes.gdoc ├── notes.txt ├── paper │   ├── QSS_a_00150-Hicks_Proof1.pdf │   ├── apa-6th-edition.csl │   ├── cover\\ letter.txt │   ├── diff.pdf │   ├── header.yaml │   ├── img │   │   ├── ORU_DAG.png │   │   ├── cites_regression.png │   │   ├── coauths_regression.png │   │   ├── conceptual_model.png │   │   ├── dept_dist_fixed_reg.png │   │   ├── dept_dist_reg.png │   │   ├── dept_gamma.png │   │   ├── dept_hell_net.png │   │   ├── dept_hell_net_50.png │   │   ├── entropies.png │   │   ├── entropies_selected.png │   │   ├── entropy_regression.png │   │   ├── gender.png │   │   ├── mds.png │   │   ├── mds_dept.png │   │   ├── network.png │   │   ├── oru_dept_entropy.png │   │   ├── oru_dept_min_dist.png │   │   ├── oru_dept_min_dist_ridges.png │   │   ├── oru_dept_network.png │   │   ├── oru_dept_org_dist.png │   │   ├── oru_dept_org_dist_ridges.png │   │   ├── oru_gamma.png │   │   ├── pub_regression.png │   │   └── sample.png │   ├── lit\\ review\\ notes.txt │   ├── oru_paper.aux │   ├── oru_paper.log │   ├── oru_paper.md │   ├── oru_paper.out │   ├── oru_paper.pdf │   ├── oru_paper.synctex.gz │   ├── oru_paper.tex │   ├── oru_paper.zip │   ├── oru_paper_20200616.pdf │   ├── oru_paper_20210805.pdf │   ├── oru_project.bib │   ├── oru_project.yaml │   ├── response1.gdoc │   ├── response1.pdf │   ├── response2.gdoc │   ├── response2.pdf │   ├── scraps │   │   ├── Hellinger.md │   │   ├── Holbrook.md │   │   ├── table.md │   │   └── table.pdf │   ├── supplement.md │   └── supplement.pdf ├── plots │   ├── 12_beta.tex │   ├── 12_cites_regression.png │   ├── 12_coauths_regression.png │   ├── 12_dept_dist_fixed_reg.png │   ├── 12_dept_dist_reg.png │   ├── 12_dept_gamma.png │   ├── 12_dept_hell_net.png │   ├── 12_dept_hell_net_50.png │   ├── 12_dept_topics.png │   ├── 12_entropies.png │   ├── 12_entropies_selected.png │   ├── 12_entropy_regression.png │   ├── 12_gender.png │   ├── 12_mds.png │   ├── 12_mds_dept.png │   ├── 12_mds_wide.png │   ├── 12_network.png │   ├── 12_oru_dept_entropy.png │   ├── 12_oru_dept_mean_dist.png │   ├── 12_oru_dept_mean_dist_ridges.png │   ├── 12_oru_dept_min_dist.png │   ├── 12_oru_dept_min_dist_ridges.png │   ├── 12_oru_dept_network.png │   ├── 12_oru_dept_org_dist.png │   ├── 12_oru_dept_org_dist_ridges.png │   ├── 12_oru_entropy.png │   ├── 12_oru_gamma.png │   ├── 12_pub_regression.png │   ├── 12_sample.png │   └── ORU_DAG.png ├── presentations │   └── 2019-06-07\\ for\\ Paul\\ Dodd.gslides ├── questions\\ for\\ jane.md ├── scripts │   ├── 01_parse_faculty_list.R │   ├── 02_Scopus_search_results.R │   ├── 03_match.R │   ├── 03_matched.csv │   ├── 04_author_meta.R │   ├── 05_filtering.R │   ├── 06_author_histories.R │   ├── 07_complete_histories.R │   ├── 08_text_annotation.R │   ├── 09_build_vocab.R │   ├── 10_topic_modeling.R │   ├── 11_depts.R │   ├── 11_depts.html │   ├── 12_analysis\\ copy.html │   ├── 12_analysis-matched.html │   ├── 12_analysis.R │   ├── 12_analysis.html │   ├── 12_analysis_cache │   │   └── html │   │   ├── __packages │   │   ├── mds_viz_efd9009c794d667852b2549df2bccf96.RData │   │   ├── mds_viz_efd9009c794d667852b2549df2bccf96.rdb │   │   ├── mds_viz_efd9009c794d667852b2549df2bccf96.rdx │   │   ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.RData │   │   ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.rdb │   │   ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.rdx │   │   ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.RData │   │   ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.rdb │   │   ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.rdx │   │   ├── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.RData │   │   ├── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdb │   │   └── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdx │   ├── 12_analysis_files │   │   └── figure-html │   │   ├── author-dept\\ distance-1.png │   │   ├── author-dept\\ distance-2.png │   │   ├── author-dept\\ distance-3.png │   │   ├── author-dept\\ distance-4.png │   │   ├── author-dept\\ distance-5.png │   │   ├── desc_plots_tabs-1.png │   │   ├── desc_plots_tabs-2.png │   │   ├── desc_plots_tabs-3.png │   │   ├── desc_plots_tabs-4.png │   │   ├── h3-1.png │   │   ├── h3-2.png │   │   ├── h3-3.png │   │   ├── h3-4.png │   │   ├── h3-5.png │   │   ├── h3-6.png │   │   ├── mds_viz-1.png │   │   ├── mds_viz-10.png │   │   ├── mds_viz-11.png │   │   ├── mds_viz-12.png │   │   ├── mds_viz-13.png │   │   ├── mds_viz-14.png │   │   ├── mds_viz-2.png │   │   ├── mds_viz-3.png │   │   ├── mds_viz-4.png │   │   ├── mds_viz-5.png │   │   ├── mds_viz-6.png │   │   ├── mds_viz-7.png │   │   ├── mds_viz-8.png │   │   ├── mds_viz-9.png │   │   ├── network-1.png │   │   ├── network-2.png │   │   ├── productivity-1.png │   │   ├── productivity-2.png │   │   ├── productivity-3.png │   │   ├── productivity-4.png │   │   ├── productivity-5.png │   │   ├── productivity-6.png │   │   ├── productivity-7.png │   │   ├── productivity-8.png │   │   ├── productivity-9.png │   │   ├── silhouette-1.png │   │   ├── topic_models-1.png │   │   ├── topic_models-10.png │   │   ├── topic_models-11.png │   │   ├── topic_models-12.png │   │   ├── topic_models-13.png │   │   ├── topic_models-2.png │   │   ├── topic_models-3.png │   │   ├── topic_models-4.png │   │   ├── topic_models-5.png │   │   ├── topic_models-6.png │   │   ├── topic_models-7.png │   │   ├── topic_models-8.png │   │   ├── topic_models-9.png │   │   ├── topic_viz-1.png │   │   └── topic_viz-2.png │   ├── api_key.R │   └── scraps │   ├── 02_parse_pubs_list.R │   ├── 03_coe_pubs.R │   ├── 03_match_auids.R │   ├── 07.R │   ├── 12_regressions.R │   ├── BML-CMSI\\ deep\\ dive.R │   ├── Hellinger_low_memory.R │   ├── dept_hell_net.R │   ├── divergence\\ against\\ lagged\\ distributions.R │   ├── exploring\\ topics.R │   ├── fractional_authorship.R │   ├── hellinger.R │   ├── model_scratch.R │   ├── multicore.R │   ├── net_viz.R │   ├── prcomp.R │   ├── propensity.R │   ├── rs_diversity.R │   ├── spacyr.R │   ├── topic\\ counts\\ rather\\ than\\ entropies.R │   ├── topic_cosine_sim.R │   ├── unit-level.R │   ├── weighted\\ regression.R │   ├── word-topic_distance.R │   ├── xx_construct_samples.R │   └── xx_oru_complete_histories.R └── tree.md 15.8 Just the directories . ├── ORU\\ faculty │   └── ORU\\ Publications.fld ├── QSS\\ forms ├── R ├── data │   ├── author_histories │   ├── authors_meta │   ├── docs │   ├── ldatuning_results │   ├── parsed_blocks │   ├── pubs │   └── temp ├── paper │   ├── img │   └── scraps ├── plots ├── presentations └── scripts ├── 12_analysis_cache │   └── html ├── 12_analysis_files │   └── figure-html └── scraps 15.9 DH’s Project Template https://github.com/dhicks/project_template Configured as a GitHub “template,” making it easy to create new repositories for new projects Designated folders for data, plots/outputs, and utility functions 15.10 A reminder on paths Windows and Unix-based systems write paths differently Use file.path() or the here package to construct paths .. in a path means “go up to the parent folder” so ../data/00_raw_data.csv goes up one level (eg, from the scripts folder), then down to the data folder, then the file 00_raw_data.csv 15.11 References References "],["managing-and-publishing-data.html", "Chapter 16 Managing and publishing data 16.1 Reading 16.2 The first rule of data management 16.3 Documentation 16.4 Questions a codebook should answer 16.5 Major codebook elements 16.6 Data management plans 16.7 Data management plan: Common elements 16.8 FAIR principles for published data 16.9 DOIs for data 16.10 References", " Chapter 16 Managing and publishing data 16.1 Reading Hudon (2018) M. D. Wilkinson et al. (2016) 16.2 The first rule of data management Do not edit your data. 16.3 Documentation Many social science fields have a tradition of writing codebooks for their data Stanford Open Policing codebook “Codebook-like summary” of the covdata package, automatically generated using skimr Caitlin Hudon’s approach (Hudon 2018) Table and field name, both verbatim Field example value Notes for both table and field Figure 3.1: Example of Caitlin Hudon’s approach to building a data dictionary. Source: https://caitlinhudon.com/2018/10/30/data-dictionaries/ 16.4 Questions a codebook should answer What does this field mean? How should I use it? What is the data [journey]? Where does this data come from? How exactly is it collected? How often is it updated? Where does it go next? What does the data in this field actually look like? Are there any caveats to keep in mind when using this data? Where can I go for more information? (Hudon 2018) 16.5 Major codebook elements (https://afit-r.github.io/codebook) Original source of the data Sampling information Where and how the data were generated Variable-level metadata and summaries Structure of the data 16.6 Data management plans Much like a research plan, data management plans provide an overview of the steps you’ll take to gather, publish, and maintain your data Since 2011, NSF has required a 2-page data management plan for most types of proposals Examples and resources UCM Library UCSD NSF examples SBE example 1 SBE example 2 NSF policy summary SBE-specific guidance 16.7 Data management plan: Common elements Who is responsible for data management Who else will have access to which data How data will be collected Data formatting standards Whether and how data will be archived and made available for reuse 16.8 FAIR principles for published data Findable F1. (meta)data are assigned a globally unique and persistent identifier F2. data are described with rich metadata (defined by R1 below) F3. metadata clearly and explicitly include the identifier of the data it describes F4. (meta)data are registered or indexed in a searchable resource Accessible A1. (meta)data are retrievable by their identifier using a standardized communications protocol A1.1 the protocol is open, free, and universally implementable A1.2 the protocol allows for an authentication and authorization procedure, where necessary A2. metadata are accessible, even when the data are no longer available Interoperable I1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation. I2. (meta)data use vocabularies that follow FAIR principles I3. (meta)data include qualified references to other (meta)data Reusable R1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standards 16.9 DOIs for data Instructions for OSF Notes for Zenodo Zenodo also plays nicely with GitHub for minting DOIs for code Citation models at Harvard Dataverse 16.10 References References "],["make.html", "Chapter 17 Make", " Chapter 17 Make "],["tracking-package-versions-with-renv.html", "Chapter 18 Tracking package versions with renv 18.1 renv 18.2 The problem renv tries to solve 18.3 How R locates packages 18.4 renv workflow 18.5 Example: with the learning-make project", " Chapter 18 Tracking package versions with renv 18.1 renv renv homepage Two date-based alternatives checkpoint groundhog 18.2 The problem renv tries to solve Figure 3.1: dplyr 0.5.0 introduced a breaking change to distinct() in June 2016. Source: https://datacolada.org/95 18.3 How R locates packages .libPaths() ## [1] &quot;/Users/runner/Library/R/x86_64/4.1/library&quot; ## [2] &quot;/Library/Frameworks/R.framework/Versions/4.1/Resources/library&quot; 18.4 renv workflow Initialize renv for a project with renv::init() Track renv.lock in version control renv::snapshot() to update the lockfile renv::restore() to install local copies of the packages to match the lockfile 18.5 Example: with the learning-make project Check .libPaths() renv::init() (and snapshot()?) What did this do? .libPaths() git status Delete renv/library and renv::restore() "],["references-9.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
