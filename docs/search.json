[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods of Data Science",
    "section": "",
    "text": "This document is a set of notes (and perhaps later a textbook) for a course on data science methods for graduate students in social and behavioral sciences, taught by Professor Dan Hicks (they/them), UC Merced.\nThe course site is available here.\n\nThese notes were written in RStudio using [quarto]. The complete source is available on GitHub.\nThis version of the book was built with R version 4.1.2 (2021-11-01), pandoc version 2.16.2, and the following packages:"
  },
  {
    "objectID": "content/01-01-intro.html",
    "href": "content/01-01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "We pause to acknowledge all local indigenous peoples, including the Yokuts and Miwuk, who inhabited this land. We embrace their continued connection to this region and thank them for allowing us to live, work, learn, and collaborate on their traditional homeland. Let us now take a moment of silence to pay respect to their elders and to all Yokuts and Miwuk people, past and present.\n\n\n\nUC Merced and the City of Merced are on the traditional territory of the Yokut people. This land was stolen by Spanish, Mexican, and American settlers through acts of slavery and genocide. In addition, UC Merced is strongly associated with Ahwahne, known as Yosemite Valley. This valley was the traditional home of the Ahwahnechee people, who were the victims of some especially horrific, state-sponsored genocidal acts. For more on the history of Ahwahne, see https://tinyurl.com/y879jw8s. For more information on land acknowledgments, see https://native-land.ca."
  },
  {
    "objectID": "content/01-01-intro.html#about-the-instructor",
    "href": "content/01-01-intro.html#about-the-instructor",
    "title": "1  Introduction",
    "section": "1.2 About the instructor",
    "text": "1.2 About the instructor\nDan Hicks is a philosopher turned data scientist turned philosopher.\nI use they/them pronouns and identify as nonbinary. I grew up in Placerville, about two hours north of Merced in the Sierra Foothills. One branch of my family came to California during the Gold Rush, so I identify heavily as a Californian and have some complicated feelings about the genocide. I finished my PhD in philosophy of science at Notre Dame in 2012. After that I worked in a series of research positions in academia and the federal government. During 2015-2019 I was using data science methods at least half-time. I joined the faculty at UC Merced in Fall 2019.\n\nEmail: dhicks4@ucmerced.edu\nStudent hours: By appointment: https://doodle.com/mm/danhicks/office-hours\nWebsite: https://dhicks.github.io/"
  },
  {
    "objectID": "content/01-01-intro.html#what-this-course-isnt-and-is",
    "href": "content/01-01-intro.html#what-this-course-isnt-and-is",
    "title": "1  Introduction",
    "section": "1.3 What this course isn’t, and is",
    "text": "1.3 What this course isn’t, and is\nIs Not:\n\na statistics course (in the way you think)\na general introduction to software engineering\na basic introduction to R\n\nIs:\n\nan introduction to data science\nabout exploratory data analysis, data management, and reproducibility\nhabituation to some good software engineering practices that are especially valuable for data science work"
  },
  {
    "objectID": "content/01-01-intro.html#learning-outcomes",
    "href": "content/01-01-intro.html#learning-outcomes",
    "title": "1  Introduction",
    "section": "1.4 Learning outcomes",
    "text": "1.4 Learning outcomes\n[match to catalog]\nBy the end of the course, students will be able to\n\napply concepts from software engineering and philosophy of science to methodological decisions in data science, such as\n\nobject-oriented and functional programming paradigms\nexploratory data analysis and the data-phenomenon-theory distinction\nreproducibility vs. replicability\n\nuse exploratory data analysis techniques and tools to identify potential data errors and potential phenomena for further analysis\nclean raw data and produce a reproducible codebook for both downstream analysis and public release of data according to FAIR standards (Wilkinson et al. 2016)\nmanage data, analysis, and outputs for reproducibility, using practices such as\n\nreproducible data cleaning\nclear directory structure (Wilson et al. 2017)\nself-documenting code\nversion control\nbuild automation\n\napply philosophical and data science concepts to integrated ethical-technical analysis of case studies"
  },
  {
    "objectID": "content/01-01-intro.html#prerequisites",
    "href": "content/01-01-intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "1.5 Prerequisites",
    "text": "1.5 Prerequisites\nThis course assumes basic competence with introductory R.\n\n“Introductory R”\n\nLessons 1-5 of the Carpentries “R for Social Scientists” curriculum - Installing R and packages - Working in the R Studio IDE - Common data types - Reading and writing CSV files - Tidyverse R: mutate(), filter(), select(); plotting with ggplot2\n\n“Basic competence”\n\nGiven time and a reference (cheatsheet, Stack Exchange, mentor) you can figure out how to solve a problem"
  },
  {
    "objectID": "content/01-01-intro.html#requirements-and-weekly-routine",
    "href": "content/01-01-intro.html#requirements-and-weekly-routine",
    "title": "1  Introduction",
    "section": "1.6 Requirements and weekly routine",
    "text": "1.6 Requirements and weekly routine\nThis is aspirational.\n\nTuesdays: Mix of discussion and lecture based on assigned readings\nThursdays: Live coding leading into work on the week’s lab assignment\nLabs: [10-ish], done individually or in pairs and submitted via GitHub for automated feedback\nRunning project: Practicing ideas from the course on data sets you find"
  },
  {
    "objectID": "content/01-02-setup.html",
    "href": "content/01-02-setup.html",
    "title": "2  Software, hardware, and accounts",
    "section": "",
    "text": "R 4.1 or higher\n\nRStudio\n\ngit [package manager]\n\nGitHub account\n\nSourcetree"
  },
  {
    "objectID": "content/01-02-setup.html#a-note-on-accessibility",
    "href": "content/01-02-setup.html#a-note-on-accessibility",
    "title": "2  Software, hardware, and accounts",
    "section": "2.1 A Note on Accessibility",
    "text": "2.1 A Note on Accessibility\nI chose the tools and platforms for this course in part because they’re industry-standard. If you pursue a career as a data scientist in industry, you’ll be expected to use GitHub (or something similar) on a daily basis. And RStudio is by far the most commonly used IDE (“integrated development environment”) for R.\nHowever, like many other technologies, they were originally developed using ableist assumptions about “normal” computer users. In response to criticism, the developers of these systems and tools have gone back and made their technologies more accessible. But there may still be barriers to accessibility that I have not anticipated.\nIf you encounter a barrier to participating in this course — even a small inconvenience — please let me know. Similarly, if you have ideas for making the course more accessible, please share them with me."
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html",
    "href": "content/01-03-data-sci-wtf.html",
    "title": "3  Data Science: What and why",
    "section": "",
    "text": "Wilson et al. (2017)\nMcElreath (2020)"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#discussion-question",
    "href": "content/01-03-data-sci-wtf.html#discussion-question",
    "title": "3  Data Science: What and why",
    "section": "3.2 Discussion question",
    "text": "3.2 Discussion question\nWhy did you decide to take a class called “data science”?"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#standard-definition",
    "href": "content/01-03-data-sci-wtf.html#standard-definition",
    "title": "3  Data Science: What and why",
    "section": "3.3 Standard definition",
    "text": "3.3 Standard definition\n\n\n\nData science, defined as the intersection of CS, stats, and “business knowledge.” Source: https://www.kdnuggets.com/2020/08/top-10-lists-data-science.html\n\n\n\nThe intersection of computer science/software engineering, statistics, and “business knowledge”\nBut this defines data science in terms of tools and techniques, not epistemic and practical goals. Compare:\n\nAn ecologist is someone who spends most of their time collecting specimens in the field and processing them in a lab, vs. \nAn ecologist is someone who studies interactions among organisms and their environment"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#discussion-question-1",
    "href": "content/01-03-data-sci-wtf.html#discussion-question-1",
    "title": "3  Data Science: What and why",
    "section": "3.4 Discussion question",
    "text": "3.4 Discussion question\nWhat are the epistemic and practical goals of your scientific field? How do you think “data science” will be useful for pursuing those goals?"
  },
  {
    "objectID": "content/02-02-git.html",
    "href": "content/02-02-git.html",
    "title": "4  Git for version control",
    "section": "",
    "text": "While working on your analysis code, you accidentally delete the first 35 lines of the script. You only discover this three days later, when you restart R and try to run the script from the top. Because you lost half of your senior thesis in undergrad, you hit Control+S to save every couple of minutes.\n\nYou’re working on a paper with two coauthors. You prepare the final draft to send for submission: paper final.docx. But one of your coauthors discovers a typo. Now it’s paper final fixed typo.docx. Another realizes six references are missing. paper final fixed typo refs.docx. That’s getting confusing so you change it to paper 2 Aug 2021.docx. Once it comes back from review you need to make revisions. Now you have paper 30 Jan 2022.docx and, after your collaborators make their changes, paper 12 February 2022 DJH.docx and paper 12 February 20222 final.docx.\nYou have a complicated analysis spread over several scripts. You want to explore a variant analysis, but doing so will involve changes in 15 different places across 3 different files. You’re not sure if this variant analysis will work; you may or may not want to keep it."
  },
  {
    "objectID": "content/02-02-git.html#version-control",
    "href": "content/02-02-git.html#version-control",
    "title": "4  Git for version control",
    "section": "4.2 Version control",
    "text": "4.2 Version control\n\nBasic idea: Tools for tracking and reversing changes to code over time\nUseful for identifying and reversing breaking changes\nImplementations upload to cloud, track who contributes code, control who can suggest vs. actually change code\nGood for collaboration, publishing code\ngit\n\nOne of many version control systems\nVery popular in part thanks to GitHub, which provided free hosting for open-source projects\n\nIn April 2020, GitHub also made private/closed-source repositories free\nResources for students (and teachers): https://education.github.com/"
  },
  {
    "objectID": "content/02-02-git.html#gitting-started",
    "href": "content/02-02-git.html#gitting-started",
    "title": "4  Git for version control",
    "section": "4.3 Gitting started",
    "text": "4.3 Gitting started\n\ngit is very hard\nWe’re going to use the Sourcetree GUI to get started"
  },
  {
    "objectID": "content/02-02-git.html#time-travel",
    "href": "content/02-02-git.html#time-travel",
    "title": "4  Git for version control",
    "section": "4.6 Time travel",
    "text": "4.6 Time travel\n\nWe can checkout previous commits to work with old versions of our files\nIn the example, suppose I made a commit with a mistake (my code stopped working or whatever)\nIn the History panel, right-click on a previous commit and select Checkout…\n\n\n\n\nChecking out an old commit to travel through time\n\n\n\n\nSourcetree warns us that we’ll be in an undetached head state\nTo see what this means, try making a change to the file, adding and committing it, then checking out the commit with the main or master tag\n\n\n\n\nTrying (and failing) to change the past. My current HEAD commit will disappear as soon as I check out main."
  },
  {
    "objectID": "content/02-02-git.html#the-garden-of-forking-branches",
    "href": "content/02-02-git.html#the-garden-of-forking-branches",
    "title": "4  Git for version control",
    "section": "4.7 The garden of forking branches",
    "text": "4.7 The garden of forking branches\n\nTo actually change the past, we’ll use a branch\n\nBranches allow git to track multiple distinct “timelines” for files\nFor example, most major software projects will have separate “dev” (development) and “release” branches\nIndividual branches will also be created for work on specific areas of the project\nThis allows each area of active work to be isolated from work happening in other areas\n\n\n\n\nAfter checking out the previous commit, click on Branch in the toolbar\n\nName your new branch something like fixing-mistake (no spaces!)\n\nStart to work on fixing the mistake in the file, then add and commit as usual\nNow checkout main. Notice:\n\nYour commits on fixing-mistake don’t disappear\nThe state of your file changes to the main version\nThe History panel shows the split between the two branches\n\nAfter we’ve finished fixing the mistake, we want to merge these changes back into main\n\nMake sure you’re current on main\nRight-click on fixing-mistake and select Merge…\n\n\n\n\n\nMerging fixing-mistake into main\n\n\n\n\nSourcetree will bring up a message about Merge Conflicts\n\nThis just means that the files you’re combining have conflicting histories, and git wants you to sort out what to keep and what to throw away\n\n\nImportant: It’s not obvious (there isn’t a big red status symbol anywhere), but git is now in a special conflict-resolution state. Until you resolve the conflicts and finish the merge, a lot of standard git functionality either won’t work at all or will cause weird problems. If git starts giving you a bunch of weird errors, check to see if you’re in the middle of a merge and need to resolve conflicts.\n\n\nAfter starting the merge, Sourcetree’s File status panel will indicate exactly which files have conflicts.\n\n\n\n\nSourcetree’s File status panel indicates which files have conflicts\n\n\n\n\nYour file will look something like this\n\ntest\nanother line\n<<<<<<< HEAD\nthis line has a mistake\n=======\nno mistake this time\n>>>>>>> fixing-mistake\n\nThe <<<<<<< and >>>>>>> surround each area of conflict.\n\nThe top part (marked HEAD) shows the state of the current branch\nThe bottom part (marked fixing-mistake) shows the state of the branch you’re merging\n\nSimply edit these sections so they’re in the state you want,\n\nThen save, go back to Sourcetree’s File status panel\nSourcetree generates a commit message indicating that you’re resolving conflicts to complete a merge\n\n\n\n\nAfterwards the History panel shows the two branches merging back together\n\n\n\n\nThe History panel shows the branches merging back together"
  },
  {
    "objectID": "content/02-02-git.html#generating-a-github-pat",
    "href": "content/02-02-git.html#generating-a-github-pat",
    "title": "4  Git for version control",
    "section": "4.8 Generating a GitHub PAT",
    "text": "4.8 Generating a GitHub PAT\n\nTo get Sourcetree to work with GitHub, we need to generate a PAT (personal access token)\nOn GitHub\n\nClick on your profile image (upper right) to your account settings\n\n\nScroll down to Developer Settings\nThen Personal access tokens\nPut a short description in the Note field\nFor our class, we’ll need a PAT with the repo and workflow permissions\nImportant: After you close/navigate away from the next page, you won’t be able to view your PAT again.\n\nKeep a browser window on this page for the next few minutes\nDon’t save the PAT anywhere (that defeats the purpose)\n\nCopy-paste now into Sourcetree’s Password field\n\nYou may need to enter it again in a few minutes, when you first push up to GitHub"
  },
  {
    "objectID": "content/02-02-git.html#working-with-github-remotes",
    "href": "content/02-02-git.html#working-with-github-remotes",
    "title": "4  Git for version control",
    "section": "4.9 Working with GitHub remotes",
    "text": "4.9 Working with GitHub remotes\n\nA remote is a copy of a repository that lives on a server somewhere else"
  },
  {
    "objectID": "content/02-02-git.html#further-reading",
    "href": "content/02-02-git.html#further-reading",
    "title": "4  Git for version control",
    "section": "4.10 Further reading",
    "text": "4.10 Further reading\n\nHester (n.d.)\n\n\n\n\n\nHester, Jim, the STAT 545 TAs. n.d. Happy Git and GitHub for the useR. Accessed March 1, 2020. https://happygitwithr.com/."
  },
  {
    "objectID": "content/02-03-getting-help.html",
    "href": "content/02-03-getting-help.html",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "",
    "text": "Bryan (2020)\n“R Faq - How to Make a Great R Reproducible Example” (n.d.)\nWickham (2019)"
  },
  {
    "objectID": "content/02-03-getting-help.html#dependencies",
    "href": "content/02-03-getting-help.html#dependencies",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.2 Dependencies",
    "text": "5.2 Dependencies\n\ninstall.packages('lubridate', 'assertthat', 'reprex')"
  },
  {
    "objectID": "content/02-03-getting-help.html#messages-warnings-and-errors",
    "href": "content/02-03-getting-help.html#messages-warnings-and-errors",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.3 Messages, warnings, and errors",
    "text": "5.3 Messages, warnings, and errors\n\nMessage: Things are fine, but here’s some information you should know\nWarning: Uhhhh I’m gonna keep going, but maybe this isn’t what you want\nError: Nope. I’m stopping here. You need to fix the thing.\n\n\nmessage('Hey, just FYI')\nwarning('Uhhhh might want to check this out')\nstop('Noooooo')"
  },
  {
    "objectID": "content/02-03-getting-help.html#where-to-go-for-help",
    "href": "content/02-03-getting-help.html#where-to-go-for-help",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.4 Where to go for help",
    "text": "5.4 Where to go for help\n\nRubber duck debugging\nIsolate the problem\nRestart your session: Session \\(\\to\\) Restart R\nLocal help: ?fun\nStackOverflow: https://stackoverflow.com/questions/tagged/r\nCRAN \\(\\to\\) BugReports (usually GitHub Issues)"
  },
  {
    "objectID": "content/02-03-getting-help.html#example-dates-are-often-problems",
    "href": "content/02-03-getting-help.html#example-dates-are-often-problems",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.5 Example: Dates are often problems",
    "text": "5.5 Example: Dates are often problems\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nadd_six_months = function(date_str) {\n    parsed_dates = parse_date_time(date_str, orders = 'mdY')\n    parsed_dates %m+% months(6) \n}\n\nsome_data = c('June 2002', 'May 15, 2007', 'August 2007')\nadd_six_months(some_data)\n\nWarning: 2 failed to parse.\n\n\n[1] NA               \"2007-11-15 UTC\" NA              \n\n\n\nNote that this is a warning, not an error\n\nR won’t stop running here (unless we tell it to)\nErrors might not show up until much later in our code, making it hard to identify the root cause\nOr they might cause invisible problems, eg, by default lm() silently drops observations with missing values\n\nTo catch warnings\n\nSet options(warn = 2) to turn all warnings into errors\nUse tryCatch() with the warning argument\n\nExample: https://stackoverflow.com/questions/8217901/breaking-loop-when-warnings-appear-in-r/8218794#8218794\n\nWrite a unit test\nMy preferred approach: Add an assertion to your primary code\n\n\n\n## Using an assertion to prevent warnings from cascading\nlibrary(assertthat)\n\nsix_months_later = add_six_months(some_data)\nassert_that(all(!is.na(six_months_later)), \n            msg = 'Missing values in `six_months_later`')\n\n\nLet’s start by using the RStudio debugger to isolate the problem\n\n\ndebugonce(add_six_months)\nadd_six_months(some_data)\n\n\nThe problem is in lubridate::parse_date_time().\n\nSpend a few minutes reading the documentation for this function and playing around with the call.\n\nWhat does the argument orders do?\n\n\n\n?parse_date_time\nparse_date_time(some_data, orders = 'mdY')\n\n\nLet’s try SO: https://stackoverflow.com/search?q=%5BR%5D+lubridate+month-year\n\n\nparse_date_time(some_data, orders = c('mY', 'mdY'))\n\n\nMake this change in add_six_months() and confirm it no longer trips the assertion."
  },
  {
    "objectID": "content/02-03-getting-help.html#another-example-more-fun-with-dates",
    "href": "content/02-03-getting-help.html#another-example-more-fun-with-dates",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.6 Another example: More fun with dates",
    "text": "5.6 Another example: More fun with dates\n\nmore_data = c('May 7, 2017', 'May 19, 2017', 'May Fifth, 2017')\nmdy(more_data)\n\n\nSO doesn’t seem so helpful: https://stackoverflow.com/search?q=%5BR%5D+lubridate+written+days\nLet’s check the CRAN page for lubridate: https://cran.r-project.org/web/packages/lubridate/index.html\n\n\n\n\n\n\nScreenshot of lubridate on CRAN, highlighting the BugReports field\n\n\n\n\n\nTrying a couple of searches gives us a promising result: https://github.com/tidyverse/lubridate/issues?q=is%3Aissue+is%3Aopen+mdy\n\n\n\n\n\n\nScreenshot of lubridate issues page, showing a relevant search result\n\n\n\n\nThis is a known bug; it looks like they’re thinking about doing something about it, but the only workaround is to create an NA: https://github.com/tidyverse/lubridate/issues/685"
  },
  {
    "objectID": "content/02-03-getting-help.html#writing-a-reproducible-example-reprex",
    "href": "content/02-03-getting-help.html#writing-a-reproducible-example-reprex",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.7 Writing a reproducible example: reprex",
    "text": "5.7 Writing a reproducible example: reprex\n\nhttps://reprex.tidyverse.org/\nhttps://reprex.tidyverse.org/articles/articles/learn-reprex.html\nhttps://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\nPractice by writing a reprex for one of our two examples"
  },
  {
    "objectID": "content/02-03-getting-help.html#do-not-do-these-things-in-your-reprex-or-anywhere-else",
    "href": "content/02-03-getting-help.html#do-not-do-these-things-in-your-reprex-or-anywhere-else",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.8 Do not do these things in your reprex (or anywhere else)",
    "text": "5.8 Do not do these things in your reprex (or anywhere else)\nOr Jenny Bryan will come to your office and set your computer on fire.\n\nsetwd('/users/danhicks/projects/catsaregreat/myscript/')\n\nUsed to ensure that R is running where your file is\nUnnecessary if you’re opening different projects in different RStudio sessions\nWill cause irrelevant errors on any other system\n\nAside: use file.path() or here::here() to build paths\nrm(list=ls())\n\nUsed because people think it clears out the global environment\nUnnecessary if you’re regularly using Session \\(\\to\\) Restart R\nAlso unnecessary at the top of a Rmd file, which is always knit in a new session\nDoesn’t actually clear out the global environment\n\neg, doesn’t unload packages or reset options()\n\n\n\nNot on Bryan’s list, but also don’t do it:\n\nrequire(package)\n\nIf package is installed, will act just like library()\nIf not, will return FALSE\n\nThe script will keep going until there’s an error about a missing function 300 lines later\nProbably not the error you wanted help with\nAnnoying to debug because I have no idea where the function is supposed to come from\n\nIf library() can’t find the package, it immediately raises an error\n\nI can tell right away what package needs to be installed"
  },
  {
    "objectID": "content/02-03-getting-help.html#debugging-in-rstudio",
    "href": "content/02-03-getting-help.html#debugging-in-rstudio",
    "title": "5  Warnings, Errors, and Getting Help",
    "section": "5.9 Debugging in RStudio",
    "text": "5.9 Debugging in RStudio\nThis week’s lab introduces you to some of RStudio’s debugging tools.\n\n\n\n\nBryan, Jenny. 2020. “Object of Type ‘Closure’ Is Not Subsettable.” Presented at the RSTUDIO::CONF 2020, January 31. https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/.\n\n\n“R Faq - How to Make a Great R Reproducible Example.” n.d. Stack Overflow. Accessed August 31, 2018. https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example.\n\n\nWickham, Hadley. 2019. “Debugging.” In Advanced R, Second Edition, ch. 22. CRC Press. https://adv-r.hadley.nz/debugging.html."
  },
  {
    "objectID": "content/02-04-functional-programming.html",
    "href": "content/02-04-functional-programming.html",
    "title": "6  Programming Paradigms",
    "section": "",
    "text": "Introductions to parts II and III of Wickham (2014)\nChambers (2014)"
  },
  {
    "objectID": "content/02-04-functional-programming.html#dependencies",
    "href": "content/02-04-functional-programming.html#dependencies",
    "title": "6  Programming Paradigms",
    "section": "6.2 Dependencies",
    "text": "6.2 Dependencies\n\ninstall.packages('sloop')"
  },
  {
    "objectID": "content/02-04-functional-programming.html#programming-paradigms",
    "href": "content/02-04-functional-programming.html#programming-paradigms",
    "title": "6  Programming Paradigms",
    "section": "6.3 Programming paradigms",
    "text": "6.3 Programming paradigms\n\nProcedural or imperative\n\nSoftware is a series of instructions (“procedures”), which the computer carries out in order. Special instructions (if-then, loops) are used to change the order based on inputs or other conditions.\n- Examples: FORTRAN, BASIC, C, a calculator\n\nObject-oriented\n\nSoftware is made up of objects, which have properties (“attributes,” including other objects) and do things (“methods”).\n- Examples: Python, Java\n\nFunctional\n\nSoftware is made up of functions, which are run sequentially on the inputs.\n- Examples: Lisp, Haskell\n\n\n\n6.3.1 R is both object-oriented and functional\n\nObject-oriented: Everything that exists is an object\nFunctional: Everything that happens is a function call"
  },
  {
    "objectID": "content/02-04-functional-programming.html#object-oriented-programming",
    "href": "content/02-04-functional-programming.html#object-oriented-programming",
    "title": "6  Programming Paradigms",
    "section": "6.4 Object-oriented programming",
    "text": "6.4 Object-oriented programming\n\nboard game as OOP\nregression models as OOP"
  },
  {
    "objectID": "content/02-04-functional-programming.html#the-oop-youre-used-to",
    "href": "content/02-04-functional-programming.html#the-oop-youre-used-to",
    "title": "6  Programming Paradigms",
    "section": "6.5 The OOP you’re used to",
    "text": "6.5 The OOP you’re used to\n\nClasses are defined by their elements and methods\nChanging/adding elements and methods requires changing the class definition\nFor \\(x\\) to be an \\(F\\), \\(x\\) must be created as an \\(F\\)\n\n\n## <https://vegibit.com/python-class-examples/>\nclass Vehicle:\n    def __init__(self, brand, model, type):\n        self.brand = brand\n        self.model = model\n        self.type = type\n        self.gas_tank_size = 14\n        self.fuel_level = 0\n\n    def fuel_up(self):\n        self.fuel_level = self.gas_tank_size\n        print('Gas tank is now full.')\n\n    def drive(self):\n        if self.fuel_level > 0:\n            print(f'The {self.model} is now driving.')\n            self.fuel_level -= 1\n        else:\n            print(f'The {self.model} is out of gas!')\n\ndhCar = Vehicle('Honda', 'Fit', 'Hatchback')\ndhCar.gas_tank_size = 10\ndhCar.fuel_up()\ndhCar.drive()"
  },
  {
    "objectID": "content/02-04-functional-programming.html#s3-is-oop-but-weird",
    "href": "content/02-04-functional-programming.html#s3-is-oop-but-weird",
    "title": "6  Programming Paradigms",
    "section": "6.6 S3 is OOP, but weird",
    "text": "6.6 S3 is OOP, but weird\n\nS3 classes can be changed on the fly, with no attempt to validate any assumptions.\n\n\ndh_car = list(brand = 'Honda', model = 'Fit', type = 'Hatchback')\nclass(dh_car)\nclass(dh_car) = c('vehicle', class(dh_car))\nclass(dh_car)\ninherits(dh_car, 'vehicle')\n\n\nmodel = lm(log(mpg) ~ log(disp), data = mtcars)\nclass(model)\nprint(model)\nclass(model) = 'Date'\nclass(model)\ntry(print(model))\n\n\nWickham discusses good practices to reduce this chaos in S3\n\nwrite constructor, validator, and helper functions\n\nS4 and R6 provide more conventional OOP structure\n\n\n\nS3 uses generic functions\n\n\nreg_model = lm(log(mpg) ~ log(disp), data = mtcars)\naov_model = aov(log(mpg) ~ log(disp), data = mtcars)\n\nclass(reg_model)\nclass(aov_model)\ninherits(aov_model, 'lm')\n\nprint(reg_model)\nprint(aov_model)\n\nresiduals(aov_model)\nresiduals(reg_model)\n\n\nBoth aov_model and reg_model inherit from lm\nprint() and residuals() are both generics\n\n(There can be) different versions of each function for different classes\nDifferent output for print()\nSame output for residuals()\n\n\n\n\nprint() is a generic; the call just passes us off using UseMethod()\n\n\nprint\n\nfunction (x, ...) \nUseMethod(\"print\")\n<bytecode: 0x7fea0043c040>\n<environment: namespace:base>\n\n\n\nsloop package is useful for understanding how S3 figures out which specific function to call\n\n\nlibrary(sloop)\n\n\ns3_dispatch(print(reg_model))\ns3_dispatch(print(aov_model))\n\n\nNote that the class-specific functions are often hidden ::: {.cell}\n\ntry(print.lm)\ns3_get_method(print.lm)\n# stats:::print.lm\ns3_get_method(print.aov)\n# stats:::print.aov\n:::\n\nUse s3_dispatch() to explain why the two models have the same output for residuals().\n\n\n\nsloop::s3_methods_generic() lists all class-specific versions of generics\n\n\ns3_methods_generic('print')\n\n# A tibble: 253 × 4\n   generic class   visible source             \n   <chr>   <chr>   <lgl>   <chr>              \n 1 print   acf     FALSE   registered S3method\n 2 print   AES     FALSE   registered S3method\n 3 print   anova   FALSE   registered S3method\n 4 print   aov     FALSE   registered S3method\n 5 print   aovlist FALSE   registered S3method\n 6 print   ar      FALSE   registered S3method\n 7 print   Arima   FALSE   registered S3method\n 8 print   arima0  FALSE   registered S3method\n 9 print   AsIs    TRUE    base               \n10 print   aspell  FALSE   registered S3method\n# … with 243 more rows\n\n\n\nAnd similarly for all generics defined for a given class\n\n\ns3_methods_class('lm')\n\n# A tibble: 35 × 4\n   generic        class visible source             \n   <chr>          <chr> <lgl>   <chr>              \n 1 add1           lm    FALSE   registered S3method\n 2 alias          lm    FALSE   registered S3method\n 3 anova          lm    FALSE   registered S3method\n 4 case.names     lm    FALSE   registered S3method\n 5 confint        lm    TRUE    stats              \n 6 cooks.distance lm    FALSE   registered S3method\n 7 deviance       lm    FALSE   registered S3method\n 8 dfbeta         lm    FALSE   registered S3method\n 9 dfbetas        lm    FALSE   registered S3method\n10 drop1          lm    FALSE   registered S3method\n# … with 25 more rows"
  },
  {
    "objectID": "content/02-04-functional-programming.html#functional-programming",
    "href": "content/02-04-functional-programming.html#functional-programming",
    "title": "6  Programming Paradigms",
    "section": "6.7 Functional programming",
    "text": "6.7 Functional programming\n\nboard game as a series of functions?\nregression models as a series of functions"
  },
  {
    "objectID": "content/02-04-functional-programming.html#common-features-of-functional-programming",
    "href": "content/02-04-functional-programming.html#common-features-of-functional-programming",
    "title": "6  Programming Paradigms",
    "section": "6.8 Common features of functional programming",
    "text": "6.8 Common features of functional programming\n\nFirst-class functions\n\nFunctions can be used like any other data type, including as inputs to and outputs from other functions\n\nDeterminism\n\nGiven a list of input values, a function always returns the same output value\n\nNo side effects\n\nA function doesn’t have any effects other than returning its output\n\nImmutability\n\nOnce a variable is assigned a value, that value cannot be changed\n\n\nThese features make it much easier to reason about how a functional program works."
  },
  {
    "objectID": "content/02-04-functional-programming.html#breaking-r-everything-that-happens-is-a-function",
    "href": "content/02-04-functional-programming.html#breaking-r-everything-that-happens-is-a-function",
    "title": "6  Programming Paradigms",
    "section": "6.9 Breaking R: Everything that happens is a function",
    "text": "6.9 Breaking R: Everything that happens is a function\n\nThis includes assignment\n\nfoo = 3\n`<-` <- function(x, y) x + y\nfoo <- 5\nfoo = 7\n\nAnd brackets\n\n`[` <- function(x, y) x * y\nbar = data.frame(x = rep(1, 15),\n                 y = rep(2, 15))\nbar['x']\nbar[2]\nbar[18]"
  },
  {
    "objectID": "content/02-04-functional-programming.html#actually-useful-functional-r-pipes-and-the-tidyverse",
    "href": "content/02-04-functional-programming.html#actually-useful-functional-r-pipes-and-the-tidyverse",
    "title": "6  Programming Paradigms",
    "section": "6.10 Actually-useful functional R: Pipes (and the tidyverse)",
    "text": "6.10 Actually-useful functional R: Pipes (and the tidyverse)\n\nPipe syntax for function composition (%>% and |>)\nTidyverse functions are designed to work with pipes\n\n\nlibrary(dplyr)\n\n\nmtcars %>%\n    filter(cyl > 4) %>% \n    lm(mpg ~ disp, data = .) %>% \n    summary()\n\n\nCall:\nlm(formula = mpg ~ disp, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4010 -1.5419 -0.5121  1.1408  4.9873 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.623520   1.463181  16.145 1.50e-12 ***\ndisp        -0.023527   0.004682  -5.025 7.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.118 on 19 degrees of freedom\nMultiple R-squared:  0.5706,    Adjusted R-squared:  0.548 \nF-statistic: 25.25 on 1 and 19 DF,  p-value: 7.521e-05\n\n\n\nUsing the new native pipes with the new native anonymous functions\n\n\nmtcars |>\n    filter(cyl > 4) |>\n    {\\(d) lm(mpg ~ disp, data = d)}() |>\n    summary()\n\n\nCall:\nlm(formula = mpg ~ disp, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4010 -1.5419 -0.5121  1.1408  4.9873 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.623520   1.463181  16.145 1.50e-12 ***\ndisp        -0.023527   0.004682  -5.025 7.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.118 on 19 degrees of freedom\nMultiple R-squared:  0.5706,    Adjusted R-squared:  0.548 \nF-statistic: 25.25 on 1 and 19 DF,  p-value: 7.521e-05"
  },
  {
    "objectID": "content/02-04-functional-programming.html#programming-paradigms-and-data-science",
    "href": "content/02-04-functional-programming.html#programming-paradigms-and-data-science",
    "title": "6  Programming Paradigms",
    "section": "6.11 Programming paradigms and data science",
    "text": "6.11 Programming paradigms and data science\n\nOOP is most useful for developers\nFunctional programming rules are really useful for data cleaning and analysis\n\n“The analysis pipeline”\nReasoning about the state of our code\nEnsuring reproducibility\n\n\n\n\n\n\n\nChambers, John M. 2014. “Object-Oriented Programming, Functional Programming and R.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452.\n\n\nWickham, Hadley. 2014. Advanced R. CRC Press. https://adv-r.hadley.nz."
  },
  {
    "objectID": "content/03-02-models-of-eda.html",
    "href": "content/03-02-models-of-eda.html",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation. (John Wilder Tukey 1977)\n\n\nExploratory Data Analysis (EDA) is “an attitude, AND a flexibility, AND some graph paper (or transparencies, or both)” (John W. Tukey 1980)"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#exploratory-and-confirmatory-research",
    "href": "content/03-02-models-of-eda.html#exploratory-and-confirmatory-research",
    "title": "7  Exploratory Data Analysis",
    "section": "7.2 Exploratory and Confirmatory Research",
    "text": "7.2 Exploratory and Confirmatory Research\n\nEspecially in the wake of the replication crisis, one common distinction is between exploratory and confirmatory research (Wagenmakers et al. 2012)\n\n\nExploratory vs. confirmatory research\n\n\n\n\n\n\nConfirmatory\nExploratory\n\n\n\n\nhypothesis testing\nhypothesis development\n\n\nspecified in advance\nadaptable\n\n\nalgorithmic\nfree, creative\n\n\nmechanical objectivity (Daston and Galison 2007)\npure subjectivity?\n\n\navoids inferential errors\nmakes errors?\n\n\nrigorous\nlacking rigor?\n\n\nreal science??\nh*cking around with data??\n\n\nassumes experimental methods\nrelevant to all methods\n\n\n\n\nI agree that it’s important to\n\nbe thoughtful about how much confidence we’re placing in our conclusions\ninterpret findings from one study in light of other studies\n\nBut the confirmatory/exploratory distinction can overemphasize the confirmatory side\n\nMaking us too rigid and narrow-minded about what counts as good science"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#better-models-for-eda-i-developing-phenomena",
    "href": "content/03-02-models-of-eda.html#better-models-for-eda-i-developing-phenomena",
    "title": "7  Exploratory Data Analysis",
    "section": "7.3 Better Models for EDA I: Developing Phenomena",
    "text": "7.3 Better Models for EDA I: Developing Phenomena\n\nBrown (2002)\n\n\nThe data/phenomena/theory distinction\n\n\n\n\n\n\n\nData\nPhenomena\nTheories/ Causal processes\n\n\n\n\nEx: spreadsheet of numbers, downloaded from Qualtrics\nEx: Correlation between partisanship and sharing Covid misinformation\nEx: Conservative susceptibility to anxiety hypothesis\n\n\n\n\n\n\n\ncollected\nabstracted or extracted from data\npostulated\n\n\nnot explained\nexplained by theories\nexplain phenomena\n\n\nhighly local to time, place, sample, procedure\nvarying scope\nuniversal?\n\n\n“raw,” messy, unwieldy\n“processed,” clean, stylized\n“laws of nature”?"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#eda-as-phenomena-development",
    "href": "content/03-02-models-of-eda.html#eda-as-phenomena-development",
    "title": "7  Exploratory Data Analysis",
    "section": "7.4 EDA as phenomena development",
    "text": "7.4 EDA as phenomena development\n\ncleaning messy data\n\nidentifying and mitigating (where possible) errors and idiosyncracies\n\nidentifying local patterns (“local phenomena”)\nnot yet claiming these will be stable and appear elsewhere\nnot yet worrying (much) about explanations"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#better-models-for-eda-ii-epicycle-of-analysis",
    "href": "content/03-02-models-of-eda.html#better-models-for-eda-ii-epicycle-of-analysis",
    "title": "7  Exploratory Data Analysis",
    "section": "7.5 Better Models for EDA II: Epicycle of Analysis",
    "text": "7.5 Better Models for EDA II: Epicycle of Analysis\n\n\n\n\n\nEpicycle of analysis model, Peng and Matsui (2016), 5"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#pengartdatascience2016",
    "href": "content/03-02-models-of-eda.html#pengartdatascience2016",
    "title": "7  Exploratory Data Analysis",
    "section": "7.6 Peng and Matsui (2016)",
    "text": "7.6 Peng and Matsui (2016)\n\nData analysis is organized into 5 activities\nEach activity involves the same 3-step “epicycle” process\n\nDevelop expectations\nCollect information\nCompare and revise expectations\n\nNot “the scientific method”! (Peng and Matsui 2016, 4)\n\n“Highly iterative and non-linear”\n“information is learned at each step, which then informs\n\nwhether (and how) to refine, and redo, the [previous] step …, or\nwhether (and how) to proceed to the next step.”"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#section-1",
    "href": "content/03-02-models-of-eda.html#section-1",
    "title": "7  Exploratory Data Analysis",
    "section": "7.7 ",
    "text": "7.7 \n\nAligning the goals of EDA with steps in the “epicycle of analysis” (Peng and Matsui 2016)\n\n\n\n\n\n\nGoals of EDA\nEpicycle step\n\n\n\n\nDetermine if there are problems with the data\n2. Collecting information\n\n\nDetermine whether our question can be answered with these data\n3. Comparing and revising expectations\n\n\nDevelop sketch of an answer\n1. Developing expectations"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#discussion",
    "href": "content/03-02-models-of-eda.html#discussion",
    "title": "7  Exploratory Data Analysis",
    "section": "7.8 Discussion",
    "text": "7.8 Discussion\n\nFor each of these models, how well do they fit the way you’ve been taught to do science?\nHow do they challenge the way you’ve been taught to do science?"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#references",
    "href": "content/03-02-models-of-eda.html#references",
    "title": "7  Exploratory Data Analysis",
    "section": "7.9 References",
    "text": "7.9 References\n\n\n\n\nBrown, James Robert. 2002. Smoke and Mirrors: How Science Reflects Reality. Routledge.\n\n\nDaston, Lorraine, and Peter Galison. 2007. Objectivity. New York : Cambridge, Mass: Zone Books ; Distributed by the MIT Press.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience.\n\n\nTukey, John W. 1980. “We Need Both Exploratory and Confirmatory.” The American Statistician 34 (1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley Series in Behavioral Science. Reading, Mass: Addison-Wesley Pub. Co. https://archive.org/details/exploratorydataa00tuke_0.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der Maas, and Rogier A. Kievit. 2012. “An Agenda for Purely Confirmatory Research.” Perspectives on Psychological Science 7 (6): 632–38. https://doi.org/10.1177/1745691612463078."
  },
  {
    "objectID": "content/03-03-checklists.html",
    "href": "content/03-03-checklists.html",
    "title": "8  Two EDA “Checklists”",
    "section": "",
    "text": "Formulate your question\nRead in your data\nCheck the packaging\nLook at the top and the bottom of your data\nCheck your “n”s\nValidate with at least one external data source\nMake a plot\nTry the easy solution first\nFollow up (Peng and Matsui 2016, 33)"
  },
  {
    "objectID": "content/03-03-checklists.html#huebnersystematicapproachinitial2016",
    "href": "content/03-03-checklists.html#huebnersystematicapproachinitial2016",
    "title": "8  Two EDA “Checklists”",
    "section": "8.2 Huebner, Vach, and le Cessie (2016)",
    "text": "8.2 Huebner, Vach, and le Cessie (2016)\n\nDuplicate records need to be eliminated\nDirection of numerical codes for categorical and ordinal variables\nInconsistencies in date and time stamps\nPresence of bimodal distributions\nPresence of skewed distributions\nPresence of ceilings and floors in continuous variables\nPresence of outliers\nDistribution of missing data\nIndications of recording errors in the main variables (after Huebner, Vach, and le Cessie 2016, 26)"
  },
  {
    "objectID": "content/03-03-checklists.html#references",
    "href": "content/03-03-checklists.html#references",
    "title": "8  Two EDA “Checklists”",
    "section": "8.3 References",
    "text": "8.3 References\n\n\n\n\nHuebner, Marianne, Werner Vach, and Saskia le Cessie. 2016. “A Systematic Approach to Initial Data Analysis Is Good Research Practice.” The Journal of Thoracic and Cardiovascular Surgery 151 (1): 25–27. https://doi.org/10.1016/j.jtcvs.2015.09.085.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience."
  },
  {
    "objectID": "content/03-04-inspecting-vars.html",
    "href": "content/03-04-inspecting-vars.html",
    "title": "9  Inspecting Variables",
    "section": "",
    "text": "For this EDA, we’ll work with data on police stops in Oakland, California, that have been pre-cleaned and released by the Stanford Open Policing Project (Pierson et al. 2020).\nBecause this analysis focuses on categorical data and counts of observations, most of the elements in Huebner, Vach, and le Cessie (2016) don’t really fit.\n\nSo we’ll follow the checklist from Peng and Matsui (2016).\n\nWe’ll also be learning to use the skimr and visdat packages"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#formulate-your-question",
    "href": "content/03-04-inspecting-vars.html#formulate-your-question",
    "title": "9  Inspecting Variables",
    "section": "9.2 1. Formulate your question",
    "text": "9.2 1. Formulate your question\n\nThe Black Lives Matter protests over the last several years have made us aware of the racial aspects of policing.\nHere we’re specifically interested in\n\nWhether Black people in Oakland might be more likely to be stopped than White people\nWhether Black people who are stopped might be more likely to have contraband\n\nThese aren’t very precise, but that’s okay: Part of the goal of EDA is to clarify and refine our research questions"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#reflexivity",
    "href": "content/03-04-inspecting-vars.html#reflexivity",
    "title": "9  Inspecting Variables",
    "section": "9.3 Reflexivity",
    "text": "9.3 Reflexivity\n\nWhether Black people in Oakland might be more likely to be stopped than White people\nWhether Black people who are stopped might be more likely to have contraband\n\n\nOnce we have a rough idea of what we want to know, we need to take a moment to think about why we want to know it\n\nClarify what “success” means to us\nShare with others to whom we’re accountable\nRecognize that we (academic researchers) often lack accountability to people who might be affected by our work\nespecially when we claim to be acting for their benefit\n\nWe’ll spend 3 minutes writing responses to each of these questions:\n\n\nWhat do I already know about this subject?\nWhy am I studying this?\nWhat do I expect or hope to find/learn, and why?\nWho is affected by this topic, and how am I connected to them?\n\n(Adapted from Tanweer et al. (2021), 14-15, and Liboiron (2021))"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#set-up-our-workspace",
    "href": "content/03-04-inspecting-vars.html#set-up-our-workspace",
    "title": "9  Inspecting Variables",
    "section": "9.4 Set up our workspace",
    "text": "9.4 Set up our workspace\n\nDedicated project folder\nClean R session\nMore on project management and organization later in the semester"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#packages",
    "href": "content/03-04-inspecting-vars.html#packages",
    "title": "9  Inspecting Variables",
    "section": "9.5 Packages",
    "text": "9.5 Packages\n\nlibrary(tidyverse)   # for working with the data\nlibrary(lubridate)   # for working with datetime data\n\nlibrary(skimr)       # generate a text-based overview of the data\nlibrary(visdat)      # generate plots visualizing data types and missingness"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#get-the-data",
    "href": "content/03-04-inspecting-vars.html#get-the-data",
    "title": "9  Inspecting Variables",
    "section": "9.6 Get the Data",
    "text": "9.6 Get the Data\n\nWe’ll be using data on police stops in Oakland, California, collected and published by the Stanford Open Policing Project.\nFor reproducibility, we’ll write a bit of code that automatically downloads the data\nTo get the download URL:\n\nhttps://openpolicing.stanford.edu/data/\nScroll down to Oakland\nRight-click on the file symbol to copy the URL\n\nREADME: https://github.com/stanford-policylab/opp/blob/master/data_readme.md.\n\n\ndata_dir = 'data'\ntarget_file = file.path(data_dir, 'oakland.zip')\n\nif (!dir.exists(data_dir)) {\n    dir.create(data_dir)\n}\nif (!file.exists(target_file)) {\n    download.file('https://stacks.stanford.edu/file/druid:yg821jf8611/yg821jf8611_ca_oakland_2020_04_01.csv.zip', \n                  target_file)\n}"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#read-in-your-data",
    "href": "content/03-04-inspecting-vars.html#read-in-your-data",
    "title": "9  Inspecting Variables",
    "section": "9.7 2. Read in your data",
    "text": "9.7 2. Read in your data\nThe dataset is a zipped csv or comma-separated value file. CSVs are structured like Excel spreadsheets, but are stored in plain text rather than Excel’s format.\n\ndataf = read_csv(target_file)\n\nRows: 133407 Columns: 28\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): raw_row_number, location, beat, subject_race, subject_sex, officer_assignment, type, ...\ndbl   (3): lat, lng, subject_age\nlgl   (7): arrest_made, citation_issued, warning_issued, contraband_found, contraband_drugs, con...\ndate  (1): date\ntime  (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#check-the-packaging",
    "href": "content/03-04-inspecting-vars.html#check-the-packaging",
    "title": "9  Inspecting Variables",
    "section": "9.8 3. Check the packaging",
    "text": "9.8 3. Check the packaging\nPeng and Matsui (2016) use some base R functions to look at dimensions of the dataframe and column (variable) types. skimr is more powerful.\n\n## May take a couple of seconds\nskim(dataf)\n\n── Data Summary ────────────────────────\n                           Values\nName                       dataf \nNumber of rows             133407\nNumber of columns          28    \n_______________________          \nColumn type frequency:           \n  character                16    \n  Date                     1     \n  difftime                 1     \n  logical                  7     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────────────────────────\n   skim_variable                 n_missing complete_rate min max empty n_unique whitespace\n 1 raw_row_number                        0        1        1  71     0   133407          0\n 2 location                             51        1.00     1  78     0    60723          0\n 3 beat                              72424        0.457    3  19     0      129          0\n 4 subject_race                          0        1        5  22     0        5          0\n 5 subject_sex                          90        0.999    4   6     0        2          0\n 6 officer_assignment               121431        0.0898   5  97     0       20          0\n 7 type                              20066        0.850    9  10     0        2          0\n 8 outcome                           34107        0.744    6   8     0        3          0\n 9 search_basis                      92250        0.309    5  14     0        3          0\n10 reason_for_stop                       0        1       14 197     0      113          0\n11 use_of_force_description         116734        0.125   10  10     0        1          0\n12 raw_subject_sdrace                    0        1        1   1     0        7          0\n13 raw_subject_resultofencounter         0        1        7 213     0      315          0\n14 raw_subject_searchconducted           0        1        2  24     0       34          0\n15 raw_subject_typeofsearch          52186        0.609    2 112     0      417          0\n16 raw_subject_resultofsearch       111633        0.163    5  95     0      298          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min        max        median     n_unique\n1 date                  2          1.00 2013-04-01 2017-12-31 2015-07-19     1638\n\n── Variable type: difftime ─────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min    max        median n_unique\n1 time                  2          1.00 0 secs 86340 secs 16:12      1439\n\n── Variable type: logical ──────────────────────────────────────────────────────────────────────────\n  skim_variable      n_missing complete_rate   mean count                  \n1 arrest_made                0         1     0.121  FAL: 117217, TRU: 16190\n2 citation_issued            0         1     0.394  FAL: 80836, TRU: 52571 \n3 warning_issued             0         1     0.231  FAL: 102545, TRU: 30862\n4 contraband_found       92250         0.309 0.149  FAL: 35005, TRU: 6152  \n5 contraband_drugs       92250         0.309 0.0844 FAL: 37684, TRU: 3473  \n6 contraband_weapons     92250         0.309 0.0299 FAL: 39928, TRU: 1229  \n7 search_conducted           0         1     0.309  FAL: 92250, TRU: 41157 \n\n── Variable type: numeric ──────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   mean      sd     p0    p25    p50    p75   p100 hist \n1 lat                 114         0.999   37.8  0.0284   37.4   37.8   37.8   37.8   38.1 ▁▁▇▁▁\n2 lng                 114         0.999 -122.   0.0432 -122.  -122.  -122.  -122.  -119.  ▇▁▁▁▁\n3 subject_age      102724         0.230   33.2 13.3      10     23     29     41     97   ▇▆▃▁▁\n\n\n\n\n133k rows (observations); 28 columns (variables)\n16 variables are handled as characters\n\nraw_row_number has 1 unique value per row\n\nSo it’s probably some kind of identifier\n\nsubject_race and subject_sex have just 5 and 2 unique values\n\nThese are probably categorical variables represented as characters\n\nSimilarly with type, outcome, and search_basis\n\nThough these have lots of missing values (high n_missing, low complete_rate)\n\n\n1 variable represents the date, and another is difftime\n\n?difftime tells us that difftime is used to represent intervals or “time differences”\n\n7 logical variables\n\nA lot of these look like coded outcomes that we might be interested in, eg, search_conducted and contraband_found\nsearch_conducted has no missing values, but contraband_found has a lot of missing values"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#for-our-motivating-questions",
    "href": "content/03-04-inspecting-vars.html#for-our-motivating-questions",
    "title": "9  Inspecting Variables",
    "section": "9.9 For our motivating questions",
    "text": "9.9 For our motivating questions\n\nGood: subject_race is 100% complete\nAlso good: search_conducted is also 100% complete\nPotentially worrisome: contraband_found is only 31% complete"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#missing-values",
    "href": "content/03-04-inspecting-vars.html#missing-values",
    "title": "9  Inspecting Variables",
    "section": "9.10 Missing values",
    "text": "9.10 Missing values\n\nLet’s use visdat::vis_miss()to\n\nvisualize missing values and\ncheck what’s up with contraband_found.\n\n\n\n## This raises a warning about large data\n# vis_miss(dataf)\n## So we'll use sample_n() to draw a subset\nset.seed(2021-09-28)\ndataf_smol = sample_n(dataf, 1000)\n\nvis_miss(dataf_smol)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nPlease use `gather()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n## Arguments in vis_miss() are useful for picking up patterns in missing values\n## cluster = TRUE uses hierarchical clustering to order the rows\nvis_miss(dataf_smol, cluster = TRUE)\n\n\n\n\n\nSeveral variables related to search outcomes are missing together\n\ncontraband_found, contraband_drugs, contraband_weapons, search_basis, use_of_force_description, raw_subject_typeofsearch, and raw_subject_resultofsearch\nHowever, search_conducted is complete"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#a-critical-question",
    "href": "content/03-04-inspecting-vars.html#a-critical-question",
    "title": "9  Inspecting Variables",
    "section": "9.11 A critical question",
    "text": "9.11 A critical question\nWhen a search has been conducted, do we know whether contraband was found?\n\nSemi-translated: when search_conducted == TRUE, is !is.na(contraband_found)?\n\n\ndataf %>% \n    filter(search_conducted) %>% \n    mutate(contraband_known = !is.na(contraband_found)) %>% \n    count(search_conducted, contraband_known)\n\n# A tibble: 1 × 3\n  search_conducted contraband_known     n\n  <lgl>            <lgl>            <int>\n1 TRUE             TRUE             41157"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#look-at-the-top-and-the-bottom-of-your-data",
    "href": "content/03-04-inspecting-vars.html#look-at-the-top-and-the-bottom-of-your-data",
    "title": "9  Inspecting Variables",
    "section": "9.12 4. Look at the top and the bottom of your data",
    "text": "9.12 4. Look at the top and the bottom of your data\nWith 28 columns, the dataframe is too wide to print in a readable way. We could use the select() function (from the tidyverse) to extract and print a few columns at a time.\nInstead we’ll use the base R function View() in an interactive session. This shows us an Excel-like spreadsheet presentation of a dataframe.\nView() can cause significant problems if you use it with a large dataframe on a slower machine. So we’ll use a pipe: first extract the head() or tail() of the dataset, and then View() it. We’ll also go ahead and view dataf_smol, the subset we created for visdat above.\n\ndataf %>% \n    head() %>% \n    View()\n\ndataf %>% \n    tail() %>% \n    View()\n\nView(dataf_smol)\n\nSome of my observations:\n\nThe ID variable raw_row_number can’t be turned into a numeric value\nlocation is a mix of addresses and intersections (“Bond St @ 48TH AVE”)\n\nIf we were going to generate a map using this column, geocoding might be tricky\nFortunately we also get latitude and longitude columns\n\nuse_of_force_description doesn’t seem to be a descriptive text field; instead it seems to be mostly missing or “handcuffed”\n\nWe can also use skimr to check data quality by looking at the minimum and maximum values. Do these ranges make sense for what we expect the variable to be?\n\nskim(dataf)\n\n── Data Summary ────────────────────────\n                           Values\nName                       dataf \nNumber of rows             133407\nNumber of columns          28    \n_______________________          \nColumn type frequency:           \n  character                16    \n  Date                     1     \n  difftime                 1     \n  logical                  7     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────────────────────────\n   skim_variable                 n_missing complete_rate min max empty n_unique whitespace\n 1 raw_row_number                        0        1        1  71     0   133407          0\n 2 location                             51        1.00     1  78     0    60723          0\n 3 beat                              72424        0.457    3  19     0      129          0\n 4 subject_race                          0        1        5  22     0        5          0\n 5 subject_sex                          90        0.999    4   6     0        2          0\n 6 officer_assignment               121431        0.0898   5  97     0       20          0\n 7 type                              20066        0.850    9  10     0        2          0\n 8 outcome                           34107        0.744    6   8     0        3          0\n 9 search_basis                      92250        0.309    5  14     0        3          0\n10 reason_for_stop                       0        1       14 197     0      113          0\n11 use_of_force_description         116734        0.125   10  10     0        1          0\n12 raw_subject_sdrace                    0        1        1   1     0        7          0\n13 raw_subject_resultofencounter         0        1        7 213     0      315          0\n14 raw_subject_searchconducted           0        1        2  24     0       34          0\n15 raw_subject_typeofsearch          52186        0.609    2 112     0      417          0\n16 raw_subject_resultofsearch       111633        0.163    5  95     0      298          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min        max        median     n_unique\n1 date                  2          1.00 2013-04-01 2017-12-31 2015-07-19     1638\n\n── Variable type: difftime ─────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min    max        median n_unique\n1 time                  2          1.00 0 secs 86340 secs 16:12      1439\n\n── Variable type: logical ──────────────────────────────────────────────────────────────────────────\n  skim_variable      n_missing complete_rate   mean count                  \n1 arrest_made                0         1     0.121  FAL: 117217, TRU: 16190\n2 citation_issued            0         1     0.394  FAL: 80836, TRU: 52571 \n3 warning_issued             0         1     0.231  FAL: 102545, TRU: 30862\n4 contraband_found       92250         0.309 0.149  FAL: 35005, TRU: 6152  \n5 contraband_drugs       92250         0.309 0.0844 FAL: 37684, TRU: 3473  \n6 contraband_weapons     92250         0.309 0.0299 FAL: 39928, TRU: 1229  \n7 search_conducted           0         1     0.309  FAL: 92250, TRU: 41157 \n\n── Variable type: numeric ──────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   mean      sd     p0    p25    p50    p75   p100 hist \n1 lat                 114         0.999   37.8  0.0284   37.4   37.8   37.8   37.8   38.1 ▁▁▇▁▁\n2 lng                 114         0.999 -122.   0.0432 -122.  -122.  -122.  -122.  -119.  ▇▁▁▁▁\n3 subject_age      102724         0.230   33.2 13.3      10     23     29     41     97   ▇▆▃▁▁\n\n\n\nDate range is April 1, 2013 to December 31, 2017\n\nIf we break things down by year, we should expect 2013 to have fewer cases\nFor some purposes, we might need to exclude 2013 data: filter(dataf, date >= '2014-01-01')\n\nAge range is from 10 years old (!) to 97 (!)\n\nMedian (p50) is 29; 50% of values are between 23 and 41\nFor some purposes, we might need to restrict the analysis to working-age adults: filter(dataf, subject_age >= 18, subject_age < 65)"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#check-your-ns-and-6.-validate-with-at-least-one-external-data-source",
    "href": "content/03-04-inspecting-vars.html#check-your-ns-and-6.-validate-with-at-least-one-external-data-source",
    "title": "9  Inspecting Variables",
    "section": "9.13 5. Check your “n”s (and) 6. Validate with at least one external data source",
    "text": "9.13 5. Check your “n”s (and) 6. Validate with at least one external data source\n\nPeng and Matsui (2016) use an air quality example with a regular sampling rate,\n\nso they can calculate exactly how many observations they should have.\n\nWe can’t do that here\n\nSo we’ll combine steps 5 and 6 together\n\nA web search leads us to this City of Oakland page on police stop data: https://www.oaklandca.gov/resources/stop-data\n\nThe page mentions a Stanford study that was released in June 2016\nRecall we got our data from the Stanford Open Policing Project\nOur data run through December 2017\nSo there’s a good chance we’re using a superset of the “Stanford study” data\nThe page links to this report: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf\nPage 8 has two summary tables that we can compare to our data\n\n\n\n\n\n\n\nScreenshot of the two summary tables from the Oakland report. Source: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf, page 8"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#from-dates-to-years",
    "href": "content/03-04-inspecting-vars.html#from-dates-to-years",
    "title": "9  Inspecting Variables",
    "section": "9.14 From dates to years",
    "text": "9.14 From dates to years\n\nOur data has the particular date of each stop\n\nWe need to extract the year of each stop\n\nlubridate::year() does exactly this\n\nFilter to the years in our data that overlap with the tables\nAnd then aggregate by year (and gender) using count\n\n\n\ndataf %>% \n    mutate(year = year(date)) %>% \n    filter(year %in% c(2016, 2017)) %>% \n    count(year)\n\n# A tibble: 2 × 2\n   year     n\n  <dbl> <int>\n1  2016 30268\n2  2017 30715\n\ndataf %>% \n    mutate(year = year(date)) %>% \n    filter(year %in% c(2016, 2017)) %>% \n    count(year, subject_sex)\n\n# A tibble: 6 × 3\n   year subject_sex     n\n  <dbl> <chr>       <int>\n1  2016 female       7677\n2  2016 male        22563\n3  2016 <NA>           28\n4  2017 female       7879\n5  2017 male        22818\n6  2017 <NA>           18\n\n\n\nFor both years, we have fewer observations than the report table indicates\n\nCould our data have been pre-filtered?\nLet’s check the documentation for our data: https://github.com/stanford-policylab/opp/blob/master/data_readme.md#oakland-ca\n“Data is deduplicated on raw columns contactdate, contacttime, streetname, subject_sdrace, subject_sex, and subject_age, reducing the number of records by ~5.2%”\n\nThe difference with the report is on this order of magnitude,\nBut varies within groups by several percentage points\nSo deduplication might explain the difference\nBut in a more serious analysis we might want to check, eg, with the Stanford Open Policing folks\n\n\n\n\n## Men in 2016 in the report vs. our data: 8.2%\n(24576 - 22563) / 24576\n\n[1] 0.08190918\n\n## Women in 2016 in the report vs. our data: 3.6%\n(7965 - 7677) / 7965\n\n[1] 0.03615819\n\n## All of 2016 in the report vs. our data: 7.1%\n(32569 - 30268) / 32569\n\n[1] 0.07065"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#make-a-plot",
    "href": "content/03-04-inspecting-vars.html#make-a-plot",
    "title": "9  Inspecting Variables",
    "section": "9.15 7. Make a plot",
    "text": "9.15 7. Make a plot\nPlotting is a whole additional discussion. We’ll talk about it next week."
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#try-the-easy-solution-first",
    "href": "content/03-04-inspecting-vars.html#try-the-easy-solution-first",
    "title": "9  Inspecting Variables",
    "section": "9.16 8. Try the easy solution first",
    "text": "9.16 8. Try the easy solution first\nWe are interested in two rough questions:\n\nWhether Black people in Oakland might be more likely to be stopped than White people\nWhether Black people who are stopped might be more likely to have contraband\n\n\nThe easy solution is to count rows and then shares within groups"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#number-of-stops-by-race",
    "href": "content/03-04-inspecting-vars.html#number-of-stops-by-race",
    "title": "9  Inspecting Variables",
    "section": "9.17 Number of stops, by race",
    "text": "9.17 Number of stops, by race\n\ndataf %>% \n    count(subject_race) %>% \n    mutate(share = n / sum(n)) %>% \n    arrange(desc(share))\n\n# A tibble: 5 × 3\n  subject_race               n  share\n  <chr>                  <int>  <dbl>\n1 black                  78925 0.592 \n2 hispanic               26257 0.197 \n3 white                  15628 0.117 \n4 asian/pacific islander  8099 0.0607\n5 other                   4498 0.0337\n\n\n\nPolice stop demographics\n\n60% of subjects stopped by police are Black\n19% are Hispanic\n12% are White\n6% are API\n\nOakland demographics: https://en.wikipedia.org/wiki/Oakland,_California#Race_and_ethnicity\n\n23% of residents are Black\n27% are Hispanic\n29% are non-Hispanic White\n15% are Asian\n\nComparing the two\n\nBlacks are severely overrepresented in police stops\nHispanics and API folks are slightly underrepresented\nWhites are significantly underrepresented"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#contraband-search-results-by-race",
    "href": "content/03-04-inspecting-vars.html#contraband-search-results-by-race",
    "title": "9  Inspecting Variables",
    "section": "9.18 Contraband search results, by race",
    "text": "9.18 Contraband search results, by race\n\nWhat fraction of stops had a search?\n\nAre there disparities by race there?\n\n\n## What fraction of stops had a search? \ndataf %>% \n    count(search_conducted) %>% \n    mutate(share = n / sum(n))\n\n# A tibble: 2 × 3\n  search_conducted     n share\n  <lgl>            <int> <dbl>\n1 FALSE            92250 0.691\n2 TRUE             41157 0.309\n\n\nAcross all subjects, 31% of stops involved a search.\n\n\nFor the other question, we need to calculate something like share within racial groups.\n\nThat is, among all stops of Black folks, what fraction involved a search?\n\nThis is a just for group_by()\n\nRemember to ungroup()\n\n\n## What fraction of stops had a search, by race? \n## Note that, for each racial group, `share` adds up to 100%. \n## \n## For all groups, most stops didn't involve a search; \n## For Black subjects, 38% of stops involved a search; \n## For White subjects, 15% of stops involved a search. \ndataf %>% \n    group_by(subject_race) %>% \n    count(search_conducted) %>% \n    mutate(share = n / sum(n)) %>% \n    ungroup() %>% \n    filter(search_conducted)\n\n# A tibble: 5 × 4\n  subject_race           search_conducted     n share\n  <chr>                  <lgl>            <int> <dbl>\n1 asian/pacific islander TRUE              1304 0.161\n2 black                  TRUE             30025 0.380\n3 hispanic               TRUE              6722 0.256\n4 other                  TRUE               706 0.157\n5 white                  TRUE              2400 0.154\n\n\n\nFor all groups, most stops didn’t involve a search\nFor Black subjects, 38% of stops involved a search\nFor White subjects, 15% of stops involved a search\nSo police were much more likely to search stopped Black subjects than White subjects\n\n\nFinally, let’s consider our second main question: contraband results from searches.\nNote that we want to restrict ourselves to only stops where search_conducted is true.\n\n## Contraband results from searches, by race\ndataf %>% \n    filter(search_conducted) %>% \n    count(subject_race, contraband_found) %>% \n    group_by(subject_race) %>% \n    mutate(share = n / sum(n)) %>% \n    ungroup() %>% \n    filter(contraband_found)\n\n# A tibble: 5 × 4\n  subject_race           contraband_found     n share\n  <chr>                  <lgl>            <int> <dbl>\n1 asian/pacific islander TRUE               176 0.135\n2 black                  TRUE              4364 0.145\n3 hispanic               TRUE              1116 0.166\n4 other                  TRUE                85 0.120\n5 white                  TRUE               411 0.171\n\n\n\nFor Black subjects who were searched, contraband was found 15% of the time\nFor White subjects, 17% of the time\n\nThis preliminary analysis indicates that Black subjects were more likely to be searched than White subjects; but, when they were searched, White subjects were more likely to have contraband."
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#follow-up",
    "href": "content/03-04-inspecting-vars.html#follow-up",
    "title": "9  Inspecting Variables",
    "section": "9.19 9. Follow up",
    "text": "9.19 9. Follow up\n\nThere are several directions we could take this analysis:\n\nInvestigate outstanding questions about quality and reliability of the data\n\neg, follow up with Stanford Open Policing Project about the difference in row counts\nFits with epicycle of analysis: checking expectations\n\nBreak down our question into more fine-grained analyses\n\neg, the Oakland web site and report talk about policy changes; do we see changes by year in the data?\nFits with epicycle of analysis: refine and specify research questions\n\nApply more sophisticated statistical analysis\n\neg, a regression model to control for age, gender, and other covariates\nFits with phenomena development: reducing data, eliminating noise, in order to identity local phenomena"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#discussion-questions",
    "href": "content/03-04-inspecting-vars.html#discussion-questions",
    "title": "9  Inspecting Variables",
    "section": "9.20 Discussion questions",
    "text": "9.20 Discussion questions\n\nSuppose you’ve conducted this EDA because you’re working with an activist organization that promotes defunding the police and prison abolition. Should you share the preliminary findings above with your organization contacts?\nWhat influence should the following factors make to your answer?\n\nFunding: Whether you’re being paid as a consultant vs. volunteering your expertise\nValues: Your own views about policing and prisons\nRelationships: Whether you are friends with members of the activist organization and/or police\nCommunications: The degree to which you can control whether and how the organization will publish your preliminary findings\nTimeliness: Whether these findings are relevant to a pending law or policy change\n\nWhat other factors should be taken into account as you decide whether to share your findings? Or not taken into account?\nHow has this “raw data” been shaped by the journey of the data to get to us?"
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#lab",
    "href": "content/03-04-inspecting-vars.html#lab",
    "title": "9  Inspecting Variables",
    "section": "9.21 Lab",
    "text": "9.21 Lab\nThe lab related to this material is available at https://github.com/data-science-methods/lab-w06-eda."
  },
  {
    "objectID": "content/03-04-inspecting-vars.html#references",
    "href": "content/03-04-inspecting-vars.html#references",
    "title": "9  Inspecting Variables",
    "section": "9.22 References",
    "text": "9.22 References\n\n\n\n\nHuebner, Marianne, Werner Vach, and Saskia le Cessie. 2016. “A Systematic Approach to Initial Data Analysis Is Good Research Practice.” The Journal of Thoracic and Cardiovascular Surgery 151 (1): 25–27. https://doi.org/10.1016/j.jtcvs.2015.09.085.\n\n\nLiboiron, Max. 2021. Pollution Is Colonialism. Duke University Press. https://books.google.com?id=NL4lEAAAQBAJ.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience.\n\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, May, 1–10. https://doi.org/10.1038/s41562-020-0858-1.\n\n\nTanweer, Anissa, Emily Kalah Gade, P. M. Krafft, and Sarah K. Dreier. 2021. “Why the Data Revolution Needs Qualitative Thinking.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.eee0b0da."
  },
  {
    "objectID": "content/03-05-graphs.html",
    "href": "content/03-05-graphs.html",
    "title": "10  Visual EDA",
    "section": "",
    "text": "Reading: Weissgerber et al. (2015)\nFurther reading: Healy (2018), ch. 1; Matejka and Fitzmaurice (n.d.)"
  },
  {
    "objectID": "content/03-05-graphs.html#the-role-of-visuals-in-eda",
    "href": "content/03-05-graphs.html#the-role-of-visuals-in-eda",
    "title": "10  Visual EDA",
    "section": "10.2 The Role of Visuals in EDA",
    "text": "10.2 The Role of Visuals in EDA\n\nTukey (1977) emphasized visual methods for EDA\nIncluding not just graphs but also structured tables, such as stem-and-leaf displays\n\n\n\n\n\n\nA stem-and-leaf display, promoted by Tukey for use in EDA"
  },
  {
    "objectID": "content/03-05-graphs.html#in-terms-of-our-models-of-eda",
    "href": "content/03-05-graphs.html#in-terms-of-our-models-of-eda",
    "title": "10  Visual EDA",
    "section": "10.3 In terms of our models of EDA",
    "text": "10.3 In terms of our models of EDA\n\n10.3.1 Epicycle of analysis\n\nCheck expectations about the distribution of variables\n\nOutliers\nDegeneracies (eg, perfect correlation)\nSkew or bimodal distributions\nNon-linear relationships\n\nDevelop expectations about relationships between variables\n\n\n\n10.3.2 Phenomena development\n\nQuickly identify potential patterns\nContrast potential patterns with noise/uncertainty/imprecision"
  },
  {
    "objectID": "content/03-05-graphs.html#plot-your-data",
    "href": "content/03-05-graphs.html#plot-your-data",
    "title": "10  Visual EDA",
    "section": "10.4 Plot Your Data",
    "text": "10.4 Plot Your Data\n\nSummary statistics almost always focus only on central tendency (mean, median) and dispersion (standard deviation, IQR)\n\nThis is all you would need if the world were made of normal distributions\n\n(Or at least unimodal symmetric ones)\n\nThe world is not made of normal distributions (Lyon 2014)\n\n(Or even unimodal symmetric ones)\n\n\nWe’ll illustrate this using The Datasaurus Dozen (Matejka and Fitzmaurice n.d.)\n\n\n\nlibrary(tidyverse)\nlibrary(datasauRus)\nlibrary(ggforce)\n\n## We'll need this, but don't want to load it\n## install.packages('Hmisc')\n\ntheme_set(theme_minimal())\n\nds_df = datasaurus_dozen\n\n\n\nThe dataset combines 13 different datasets with \\(n=142\\) for each\n\n\nds_df\n\n# A tibble: 1,846 × 3\n   dataset     x     y\n   <chr>   <dbl> <dbl>\n 1 dino     55.4  97.2\n 2 dino     51.5  96.0\n 3 dino     46.2  94.5\n 4 dino     42.8  91.4\n 5 dino     40.8  88.3\n 6 dino     38.7  84.9\n 7 dino     35.6  79.9\n 8 dino     33.1  77.6\n 9 dino     29.0  74.5\n10 dino     26.2  71.4\n# … with 1,836 more rows\n\ncount(ds_df, dataset)\n\n# A tibble: 13 × 2\n   dataset        n\n   <chr>      <int>\n 1 away         142\n 2 bullseye     142\n 3 circle       142\n 4 dino         142\n 5 dots         142\n 6 h_lines      142\n 7 high_lines   142\n 8 slant_down   142\n 9 slant_up     142\n10 star         142\n11 v_lines      142\n12 wide_lines   142\n13 x_shape      142\n\n\n\n\nThe datasets have the same means, standard deviations, and (Pearson) correlation coefficient\n\np-value of the correlation coefficient is not statistically significant\n\n\n\n## This is a more complex summarize() call than we've seen before\n## 1. Number of rows\n## 2. \"Summarize across the columns x and y, using the functions mean and sd\"; automatically generates names\n## 3. Correlation between x and y\n## 4. p-value from a t-test of the null that the correlation = 0\nds_df %>% \n    group_by(dataset) %>% \n    summarize(n = n(), \n              across(.cols = c(x, y), \n                     .fns = lst(mean, sd)), \n              cor_xy = cor(x, y), \n              p_value = cor.test(x, y)$p.value) %>% \n    ungroup()\n\n# A tibble: 13 × 8\n   dataset        n x_mean  x_sd y_mean  y_sd  cor_xy p_value\n   <chr>      <int>  <dbl> <dbl>  <dbl> <dbl>   <dbl>   <dbl>\n 1 away         142   54.3  16.8   47.8  26.9 -0.0641   0.448\n 2 bullseye     142   54.3  16.8   47.8  26.9 -0.0686   0.417\n 3 circle       142   54.3  16.8   47.8  26.9 -0.0683   0.419\n 4 dino         142   54.3  16.8   47.8  26.9 -0.0645   0.446\n 5 dots         142   54.3  16.8   47.8  26.9 -0.0603   0.476\n 6 h_lines      142   54.3  16.8   47.8  26.9 -0.0617   0.466\n 7 high_lines   142   54.3  16.8   47.8  26.9 -0.0685   0.418\n 8 slant_down   142   54.3  16.8   47.8  26.9 -0.0690   0.415\n 9 slant_up     142   54.3  16.8   47.8  26.9 -0.0686   0.417\n10 star         142   54.3  16.8   47.8  26.9 -0.0630   0.457\n11 v_lines      142   54.3  16.8   47.8  26.9 -0.0694   0.412\n12 wide_lines   142   54.3  16.8   47.8  26.9 -0.0666   0.431\n13 x_shape      142   54.3  16.8   47.8  26.9 -0.0656   0.438\n\n\n\n\nBut, when plotted, they’re obviously very different\n\n\nggplot(ds_df, aes(x, y)) +\n    geom_point() +\n    facet_wrap(vars(dataset))"
  },
  {
    "objectID": "content/03-05-graphs.html#bar-plots-and-better-than-bar-plots",
    "href": "content/03-05-graphs.html#bar-plots-and-better-than-bar-plots",
    "title": "10  Visual EDA",
    "section": "10.5 Bar plots, and better than bar plots",
    "text": "10.5 Bar plots, and better than bar plots\n\nWeissgerber et al. (2015) argue for the importance of plotting data, not just summaries, even in publications\nLet’s pull a few datasets out of datasaurus, as though they were groups in a study\n\n\ncdw_df = ds_df |> \n    filter(dataset %in% c('circle', 'dino', 'wide_lines'))"
  },
  {
    "objectID": "content/03-05-graphs.html#a-bar-plot-of-mean-95-ci",
    "href": "content/03-05-graphs.html#a-bar-plot-of-mean-95-ci",
    "title": "10  Visual EDA",
    "section": "10.6 A bar plot of mean + 95% CI",
    "text": "10.6 A bar plot of mean + 95% CI\n\nggplot(cdw_df, aes(x = dataset, \n                   y = x)) +\n    stat_summary(geom = 'errorbar', width = 0.2,\n                 fun.data = mean_cl_boot, \n                 fun.args = list(conf.int = .95)) +\n    stat_summary(geom = 'bar', fun = mean, \n                 aes(fill = dataset)) +\n    scale_fill_viridis_d(guide = 'none')\n\nWarning: Computation failed in `stat_summary()`:"
  },
  {
    "objectID": "content/03-05-graphs.html#replace-the-bar-with-the-data",
    "href": "content/03-05-graphs.html#replace-the-bar-with-the-data",
    "title": "10  Visual EDA",
    "section": "10.7 Replace the bar with the data",
    "text": "10.7 Replace the bar with the data\n\nggplot(cdw_df, aes(x = dataset, \n                   y = x)) +\n    # geom_point(aes(color = dataset)) +\n    geom_sina(aes(fill = dataset), \n              shape = 21L) +\n    stat_summary(geom = 'errorbar', width = 0.2,\n                 fun.data = mean_cl_boot) +\n    scale_color_viridis_d(guide = 'none', \n                          aesthetics = c('color', 'fill'))\n\nWarning: Computation failed in `stat_summary()`:"
  },
  {
    "objectID": "content/03-05-graphs.html#even-fancier-violin-plot",
    "href": "content/03-05-graphs.html#even-fancier-violin-plot",
    "title": "10  Visual EDA",
    "section": "10.8 Even fancier: Violin plot",
    "text": "10.8 Even fancier: Violin plot\n\nggplot(cdw_df, aes(x = dataset, y = x, fill = dataset)) +\n    geom_violin(draw_quantiles = c(.5)) +\n    geom_sina(shape = 21, fill = 'white') +\n    scale_fill_viridis_d(guide = 'none')"
  },
  {
    "objectID": "content/03-05-graphs.html#references",
    "href": "content/03-05-graphs.html#references",
    "title": "10  Visual EDA",
    "section": "10.9 References",
    "text": "10.9 References\n\n\n\n\nHealy, Kieran. 2018. Data Visualization: A Practical Introduction. Princeton University Press. https://books.google.com?id=3XOYDwAAQBAJ.\n\n\nLyon, A. 2014. “Why Are Normal Distributions Normal?” The British Journal for the Philosophy of Science 65 (3): 621–49. https://doi.org/10.1093/bjps/axs046.\n\n\nMatejka, Justin, and George Fitzmaurice. n.d. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing (The Datasaurus Dozen) | Autodesk Research.” Accessed May 21, 2017. https://www.autodeskresearch.com/publications/samestats.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley Series in Behavioral Science. Reading, Mass: Addison-Wesley Pub. Co. https://archive.org/details/exploratorydataa00tuke_0.\n\n\nWeissgerber, Tracey L., Natasa M. Milic, Stacey J. Winham, and Vesna D. Garovic. 2015. “Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm.” PLoS Biol 13 (4): e1002128. https://doi.org/10.1371/journal.pbio.1002128."
  },
  {
    "objectID": "content/03-06-grammar-of-graphics.html",
    "href": "content/03-06-grammar-of-graphics.html",
    "title": "11  The Grammar of Graphics",
    "section": "",
    "text": "Wickham, Navarro, and Pedersen (n.d.), ch. 13, “Mastering the Grammar”\nFurther reading: Wilkinson (1999); Wickham (2010)"
  },
  {
    "objectID": "content/03-06-grammar-of-graphics.html#artists-palette-model",
    "href": "content/03-06-grammar-of-graphics.html#artists-palette-model",
    "title": "11  The Grammar of Graphics",
    "section": "11.2 Artist’s palette model",
    "text": "11.2 Artist’s palette model\nMany programming languages use the artist’s palette model of computer graphics\n\nDefine some low-level tools for drawing on a canvas: points(), lines(), curve()\nOne-liners that bundle these tools into standard plots: hist(), barplot()\nThis is used by base graphics in R, Python packages including matplotlib and seaborn, MATLAB, etc.\nIn R: base graphics are side effects, not first-class objects\n\n(ref:matplotlib) matplotlib, a popular Python package for graphing, uses the artist's palette model. Source: https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplot.html\n\n\n\n\n\n(ref:matplotlib)\n\n\n\n\n\n\n\n(ref:matplotlib)"
  },
  {
    "objectID": "content/03-06-grammar-of-graphics.html#grammar-of-graphics",
    "href": "content/03-06-grammar-of-graphics.html#grammar-of-graphics",
    "title": "11  The Grammar of Graphics",
    "section": "11.3 Grammar of graphics",
    "text": "11.3 Grammar of graphics\nThe grammar of graphics is a fundamentally different approach\n\nImplemented by ggplot2 in the tidyverse\n\n\n\n\n\n\nThe Grammar of Graphics regards graphs as mappings from variables in the data to features of geometric objects. This means that every graph is a model."
  },
  {
    "objectID": "content/03-06-grammar-of-graphics.html#graphs-are-maps",
    "href": "content/03-06-grammar-of-graphics.html#graphs-are-maps",
    "title": "11  The Grammar of Graphics",
    "section": "11.4 Graphs are maps",
    "text": "11.4 Graphs are maps\n\nA graph is a mapping from variables in the data to features of geometric objects\nAn individual mapping is called an aesthetic\nSo a graph is a collection of aesthetics applied to a dataset\n\n\n\n\n\noakland_df = read_csv(file.path('data', 'oakland.zip'), \n                      show_col_types = FALSE)\n\nggplot(data = oakland_df, \n       aes(x = subject_race, fill = search_conducted)) +\n    geom_bar(position = 'fill') +\n    scale_fill_brewer(palette = 'Set1')"
  },
  {
    "objectID": "content/03-06-grammar-of-graphics.html#graphs-are-models",
    "href": "content/03-06-grammar-of-graphics.html#graphs-are-models",
    "title": "11  The Grammar of Graphics",
    "section": "11.5 Graphs are models",
    "text": "11.5 Graphs are models\n\nSimplifications of the data\nThat provide us with cognitive affordances\n\nFit with our cognitive capabilities\nSupport pattern detection, inference\n\nDirect our attention to certain things and away from others\nInvolve researcher degrees of freedom\n\n\nConsider the bar plots vs. dots + CI plots vs. violin plots from the last chapter\n\nHow do these simplify the data?\nIn doing so, how do they provide us with cognitive affordances?\nHow do these different in terms of where they direct our attention?\nWhat researcher degrees of freedom does each involve?"
  },
  {
    "objectID": "content/03-06-grammar-of-graphics.html#references",
    "href": "content/03-06-grammar-of-graphics.html#references",
    "title": "11  The Grammar of Graphics",
    "section": "11.6 References",
    "text": "11.6 References\n\n\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. n.d. Ggplot2: Elegant Graphics for Data Analysis. Third edition. Accessed September 20, 2021. https://ggplot2-book.org/index.html.\n\n\nWilkinson, Leland. 1999. The Grammar of Graphics. Statistics and Computing. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4757-3100-2."
  },
  {
    "objectID": "content/03-07-covid-eda.html",
    "href": "content/03-07-covid-eda.html",
    "title": "12  Covid EDA",
    "section": "",
    "text": "This EDA will start in class and continue on to the lab\nWe’re interested in the role of social distancing in the July 2020 Covid wave in California"
  },
  {
    "objectID": "content/03-07-covid-eda.html#the-standard-narrative",
    "href": "content/03-07-covid-eda.html#the-standard-narrative",
    "title": "12  Covid EDA",
    "section": "12.2 The standard narrative",
    "text": "12.2 The standard narrative\nThe standard narrative about the July 2020 wave goes like this:\n\nCalifornia had some of the first confirmed cases of Covid-19 in the US\nCalifornia was also the first state to institute a stay-at-home order and encourage social distancing\nThese social distancing policies are why California did not experience the large first wave in March 2020 (contrast NYC)\nStarting in May 2020, these policies were relaxed and “lockdown fatigue” meant that people were ignoring them anyways\nThis is why California did experience a more significant wave in July 2020\n\n\nOur research question: Is reduced social distancing (measured using cellphone tracking data) correlated with increased Covid-19 case counts 4 weeks later?\n\nIn class, we’ll just be looking at the case counts."
  },
  {
    "objectID": "content/03-07-covid-eda.html#reflexivity",
    "href": "content/03-07-covid-eda.html#reflexivity",
    "title": "12  Covid EDA",
    "section": "12.3 Reflexivity",
    "text": "12.3 Reflexivity\n(For time, you’ll do these on your own as part of the lab.)"
  },
  {
    "objectID": "content/03-07-covid-eda.html#setup",
    "href": "content/03-07-covid-eda.html#setup",
    "title": "12  Covid EDA",
    "section": "12.4 Setup",
    "text": "12.4 Setup\n\nlibrary(tidyverse)\nlibrary(tidylog)\nlibrary(skimr)\nlibrary(visdat)\n\nlibrary(assertthat)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "content/03-07-covid-eda.html#data",
    "href": "content/03-07-covid-eda.html#data",
    "title": "12  Covid EDA",
    "section": "12.5 Data",
    "text": "12.5 Data\nTo save some time in class, I’ve prepared a CSV that combines three datasets:\n\nCovid-19 case and death counts, at the county level, collected by the New York Times\nEstimated county populations in 2018, from the US Census’ American Community Survey\n“Mobility data,” based on cellphone tracking, from Google\n\nThe dataset has also been filtered down to California."
  },
  {
    "objectID": "content/03-07-covid-eda.html#get-the-data",
    "href": "content/03-07-covid-eda.html#get-the-data",
    "title": "12  Covid EDA",
    "section": "12.6 Get the data",
    "text": "12.6 Get the data\n\nThis week’s lab on GitHub\n\nhttps://github.com/data-science-methods/lab_w06_covid\n\ndata -> covid.csv -> “Download” (right-click and copy)\n\n\ncovid_file = file.path('data', 'covid.csv')\ncovid_url = 'https://github.com/data-science-methods/lab_w06_covid/raw/main/data/covid.csv'\nif (!file.exists(covid_file)) {\n    download.file(covid_url, covid_file)\n}\n## Original version\n# covid_df = read_csv(covid_file, show_col_types = FALSE)\n## With daily change\ncovid_df = read_csv(covid_file, show_col_types = FALSE) |> \n    group_by(county) |>\n    mutate(across(.cols = c(cases, deaths),\n                  .fns = list(new = daily_new), date)) |> \n    ungroup() |> \n    mutate(across(.cols = c(matches('cases'), matches('deaths')), \n                  .fns = list(rate = ~ .x / population * 100000)))\n\ngroup_by: one grouping variable (county)\n\n\nmutate (grouped): new variable 'cases_new' (double) with 1,737 unique values and <1% NA\n\n\n                  new variable 'deaths_new' (double) with 168 unique values and <1% NA\n\n\nungroup: no grouping variables\n\n\nmutate: new variable 'cases_rate' (double) with 24,351 unique values and 0% NA\n\n\n        new variable 'cases_new_rate' (double) with 9,753 unique values and <1% NA\n\n\n        new variable 'deaths_rate' (double) with 8,485 unique values and 0% NA\n\n\n        new variable 'deaths_new_rate' (double) with 1,056 unique values and <1% NA"
  },
  {
    "objectID": "content/03-07-covid-eda.html#some-quick-data-checking",
    "href": "content/03-07-covid-eda.html#some-quick-data-checking",
    "title": "12  Covid EDA",
    "section": "12.7 Some quick data checking",
    "text": "12.7 Some quick data checking\n\nskim(covid_df)\n\n\nData summary\n\n\nName\ncovid_df\n\n\nNumber of rows\n33322\n\n\nNumber of columns\n19\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nDate\n1\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncounty\n0\n1\n4\n15\n0\n58\n0\n\n\nstate\n0\n1\n10\n10\n0\n1\n0\n\n\nfips\n0\n1\n5\n5\n0\n58\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2020-01-25\n2021-10-12\n2020-12-29\n627\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ncases\n0\n1.00\n38007.85\n128692.55\n1.00\n479.25\n4458.00\n23744.50\n1473123.00\n▇▁▁▁▁\n\n\ndeaths\n0\n1.00\n590.84\n2290.48\n0.00\n5.00\n55.00\n299.00\n26346.00\n▇▁▁▁▁\n\n\npopulation\n0\n1.00\n708598.07\n1508235.90\n1146.00\n53932.00\n186661.00\n765935.00\n10098052.00\n▇▁▁▁▁\n\n\nretail\n15286\n0.54\n-25.50\n16.78\n-100.00\n-35.00\n-25.00\n-16.00\n63.00\n▁▃▇▁▁\n\n\ngrocery\n16225\n0.51\n-5.84\n13.46\n-74.00\n-14.00\n-7.00\n1.00\n86.00\n▁▆▇▁▁\n\n\nparks\n18301\n0.45\n2.16\n40.35\n-83.00\n-22.00\n-3.00\n18.00\n323.00\n▇▆▁▁▁\n\n\ntransit\n18639\n0.44\n-29.67\n27.29\n-88.00\n-49.00\n-32.00\n-15.00\n97.00\n▃▇▂▁▁\n\n\nworkplaces\n14112\n0.58\n-30.74\n15.01\n-87.00\n-39.00\n-31.00\n-20.00\n11.00\n▁▂▇▇▁\n\n\nresidential\n17022\n0.49\n10.82\n6.33\n-7.00\n6.00\n10.00\n14.00\n37.00\n▁▇▆▂▁\n\n\ncases_new\n58\n1.00\n144.62\n680.67\n-1157.00\n0.00\n13.00\n77.00\n29174.00\n▇▁▁▁▁\n\n\ndeaths_new\n58\n1.00\n2.11\n12.92\n-423.00\n0.00\n0.00\n1.00\n930.00\n▁▇▁▁▁\n\n\ncases_rate\n0\n1.00\n4714.32\n4521.80\n0.01\n541.96\n3567.44\n7764.35\n23466.41\n▇▃▂▁▁\n\n\ncases_new_rate\n58\n1.00\n20.11\n45.88\n-211.66\n0.00\n6.53\n21.91\n2055.69\n▇▁▁▁▁\n\n\ndeaths_rate\n0\n1.00\n58.83\n65.99\n0.00\n5.53\n38.53\n88.80\n422.27\n▇▂▁▁▁\n\n\ndeaths_new_rate\n58\n1.00\n0.24\n1.04\n-31.56\n0.00\n0.00\n0.05\n48.63\n▁▇▁▁▁"
  },
  {
    "objectID": "content/03-07-covid-eda.html#variables",
    "href": "content/03-07-covid-eda.html#variables",
    "title": "12  Covid EDA",
    "section": "12.8 Variables",
    "text": "12.8 Variables\n\ncounty, state, and fips are all geographic identifiers, and complete\n\n58 unique values for both county and fips\nHow many counties does California have?\n\ndate gives the date, is also complete\ncases and deaths come from the NYT covid data\npopulation comes from the Census\nretail, grocery, parks, transit, workplaces, and residential come from the Google Mobility data\n\nMuch lower completion rate"
  },
  {
    "objectID": "content/03-07-covid-eda.html#too-many-timeseries-plots",
    "href": "content/03-07-covid-eda.html#too-many-timeseries-plots",
    "title": "12  Covid EDA",
    "section": "12.9 Too many timeseries plots",
    "text": "12.9 Too many timeseries plots\n\nggplot(covid_df, aes(date, cases)) +\n    geom_line() +\n    facet_wrap(vars(county), \n               scales = 'free_y')"
  },
  {
    "objectID": "content/03-07-covid-eda.html#number-of-observations-per-county",
    "href": "content/03-07-covid-eda.html#number-of-observations-per-county",
    "title": "12  Covid EDA",
    "section": "12.10 Number of observations per county",
    "text": "12.10 Number of observations per county\n\nggplot(covid_df, aes(fct_rev(fct_infreq(county)))) +\n    # geom_bar() +\n    geom_point(stat = 'count') +\n    geom_segment(stat = 'count', \n                 aes(xend = county),\n                 yend = 0) +\n    coord_flip() +\n    labs(x = 'county')\n\n\n\n\nWhy do different counties have different numbers of observations?"
  },
  {
    "objectID": "content/03-07-covid-eda.html#no-more-than-1-observation-per-county-per-day",
    "href": "content/03-07-covid-eda.html#no-more-than-1-observation-per-county-per-day",
    "title": "12  Covid EDA",
    "section": "12.11 No more than 1 observation per county per day",
    "text": "12.11 No more than 1 observation per county per day\n\ncovid_df |> \n    count(county, date) |> \n    filter(n > 1) |> \n    nrow() |> \n    identical(0L) |> \n    assert_that(msg = 'More than 1 observation per county per day')\n\ncount: now 33,322 rows and 3 columns, ungrouped\n\n\nfilter: removed all rows (100%)\n\n\n[1] TRUE"
  },
  {
    "objectID": "content/03-07-covid-eda.html#cumulative-vs.-daily-cases",
    "href": "content/03-07-covid-eda.html#cumulative-vs.-daily-cases",
    "title": "12  Covid EDA",
    "section": "12.12 Cumulative vs. daily cases",
    "text": "12.12 Cumulative vs. daily cases\nBoth cases and deaths are cumulative, not the daily new value\n\ncovid_df |> \n    filter(county == 'Merced') |> \n    ggplot(aes(date, cases)) +\n    geom_line()\n\nfilter: removed 32,754 rows (98%), 568 rows remaining\n\n\n\n\n\n\n\nWe’ll write a little function to calculate the differences\nIncorporate it into the pipe that loads the data\n\nThen restart R and rerun\n\n\n\ndaily_new = function(x, order_var) {\n    diff = x - dplyr::lag(x, order_by = order_var)\n    return(diff)\n}\n\n\n\nTest it on Orange and LA Counties\n\n\ncovid_df |>\n    filter(county %in% c('Orange', 'Los Angeles')) |>\n    group_by(county) |>\n    mutate(across(.cols = c(cases, deaths),\n                  .fns = list(new = daily_new), date)) |>\n    select(date, county, matches('cases'), matches('deaths')) |> \n    View()\n\n\nNow we have daily values\n\ncovid_df |> \n    filter(county == 'Merced') |> \n    ggplot(aes(date, cases_new)) +\n    geom_line()\n\nfilter: removed 32,754 rows (98%), 568 rows remaining\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path)."
  },
  {
    "objectID": "content/03-07-covid-eda.html#add-an-assertion",
    "href": "content/03-07-covid-eda.html#add-an-assertion",
    "title": "12  Covid EDA",
    "section": "12.13 Add an assertion",
    "text": "12.13 Add an assertion\n\nFor each county, the first daily diff should be NA\nBut none of the others\n\n\ncovid_df |> \n    group_by(county) |> \n    slice(-1) |> \n    pull(cases_new) |> \n    is.na() |> \n    any() |> \n    magrittr::not() |> \n    assert_that(msg = 'missing values in cases_new')\n\ngroup_by: one grouping variable (county)\n\n\nslice (grouped): removed 58 rows (<1%), 33,264 rows remaining\n\n\n[1] TRUE"
  },
  {
    "objectID": "content/03-07-covid-eda.html#distribution-of-cases-by-county",
    "href": "content/03-07-covid-eda.html#distribution-of-cases-by-county",
    "title": "12  Covid EDA",
    "section": "12.14 Distribution of cases by county",
    "text": "12.14 Distribution of cases by county\nFirst pass is hard to read\n\nggplot(covid_df, aes(county, cases_new)) +\n    geom_boxplot()\n\nWarning: Removed 58 rows containing non-finite values (stat_boxplot)."
  },
  {
    "objectID": "content/03-07-covid-eda.html#revisions",
    "href": "content/03-07-covid-eda.html#revisions",
    "title": "12  Covid EDA",
    "section": "12.15 Revisions",
    "text": "12.15 Revisions\n\ncoef = 1000\ny-axis on a log scale\nflip the coordinates\nreorder the counties by median number of cases\nfilter out cases_new == 0\nmeaningful axis labels\n\n\ncovid_df |> \n    filter(cases_new > 0) |>\n    ggplot(aes(fct_reorder(county, cases_new, \n                           .fun = median, na.rm = TRUE),\n                     cases_new)) +\n    geom_boxplot(coef = 1000) +\n    scale_y_log10() +\n    coord_flip() +\n    labs(x = 'county', y = 'daily new cases')\n\nfilter: removed 9,133 rows (27%), 24,189 rows remaining"
  },
  {
    "objectID": "content/03-07-covid-eda.html#plots-of-the-same-plot",
    "href": "content/03-07-covid-eda.html#plots-of-the-same-plot",
    "title": "12  Covid EDA",
    "section": "12.16 9 plots of the same plot",
    "text": "12.16 9 plots of the same plot\n\nggplot(covid_df, aes(x = cases_new, y = deaths_new)) +\n    # geom_point() +\n    # geom_point(alpha = .2) +\n    # geom_count(alpha = .5) +\n    # geom_bin2d() +\n    # geom_hex() +\n    # geom_hex(aes(color = after_stat(count))) +\n    # geom_density2d(size = 1) +\n    # stat_density2d(contour = FALSE, geom = 'raster',\n    #                aes(fill = after_stat(density)),\n    #                show.legend = FALSE) +\n    stat_density2d(contour = TRUE, geom = 'polygon',\n                   aes(fill = after_stat(level)),\n                   show.legend = FALSE) +\n    scale_x_log10() +\n    scale_y_log10() +\n    scale_color_viridis_c(aesthetics = c('color', 'fill'))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 25048 rows containing non-finite values (stat_density2d)."
  },
  {
    "objectID": "content/03-07-covid-eda.html#why-so-many-dropped-values",
    "href": "content/03-07-covid-eda.html#why-so-many-dropped-values",
    "title": "12  Covid EDA",
    "section": "12.17 Why so many dropped values?",
    "text": "12.17 Why so many dropped values?\n\nThe binned plots drop 25,048 rows, or 75% of the data\nWhy?\n\n\ncovid_df |> \n    mutate(log_cases_new = log(cases_new)) |> \n    select(cases_new, log_cases_new) |> \n    filter(!is.finite(log_cases_new)) |> \n    count(cases_new) |> \n    arrange(desc(n))\n\nWarning in log(cases_new): NaNs produced\n\n\nmutate: new variable 'log_cases_new' (double) with 1,682 unique values and 1% NA\n\n\nselect: dropped 18 variables (date, county, state, fips, cases, …)\n\n\nfilter: removed 24,189 rows (73%), 9,133 rows remaining\n\n\ncount: now 58 rows and 2 columns, ungrouped\n\n\n# A tibble: 58 × 2\n   cases_new     n\n       <dbl> <int>\n 1         0  8829\n 2        -1   113\n 3        NA    58\n 4        -2    27\n 5        -3    11\n 6        -6     9\n 7        -8     8\n 8       -17     4\n 9       -12     4\n10        -5     4\n# … with 48 more rows"
  },
  {
    "objectID": "content/03-07-covid-eda.html#cases-vs.-population",
    "href": "content/03-07-covid-eda.html#cases-vs.-population",
    "title": "12  Covid EDA",
    "section": "12.18 Cases vs. population",
    "text": "12.18 Cases vs. population\n\nggplot(covid_df, aes(population, cases_new)) +\n    geom_point()\n\nWarning: Removed 58 rows containing missing values (geom_point)."
  },
  {
    "objectID": "content/03-07-covid-eda.html#absolute-counts-to-rates",
    "href": "content/03-07-covid-eda.html#absolute-counts-to-rates",
    "title": "12  Covid EDA",
    "section": "12.19 Absolute counts to rates",
    "text": "12.19 Absolute counts to rates\n\ncases and deaths are absolute counts\nBut county populations differ over orders of magnitude\nConstruct a lollipop plot to illustrate this\n\n\ncovid_df |> \n    select(county, population) |> \n    distinct() |> \n    ggplot(aes(x = fct_reorder(county, population), \n               y = population)) +\n    geom_point() +\n    geom_segment(aes(xend = county), yend = 0) +\n    scale_y_log10() +\n    coord_flip()\n\nselect: dropped 17 variables (date, state, fips, cases, deaths, …)\n\n\ndistinct: removed 33,264 rows (>99%), 58 rows remaining\n\n\n\n\n\n\nLet’s calculate rates for new and cumulative cases and deaths\n\ntest_df = covid_df |> \n    mutate(across(.cols = c(matches('cases'), matches('deaths')), \n                  .fns = list(rate = ~ .x / population * 100000)))\n\nmutate: new variable 'cases_rate_rate' (double) with 24,355 unique values and 0% NA\n\n\n        new variable 'cases_new_rate_rate' (double) with 9,755 unique values and <1% NA\n\n\n        new variable 'deaths_rate_rate' (double) with 8,485 unique values and 0% NA\n\n\n        new variable 'deaths_new_rate_rate' (double) with 1,056 unique values and <1% NA\n\ntest_df |> \n    filter(county %in% c('Merced', 'Fresno', 'Los Angeles')) |> \n    ggplot(aes(date, cases_new_rate, \n               group = county, color = county)) +\n    geom_line()\n\nfilter: removed 31,543 rows (95%), 1,779 rows remaining\n\n\nWarning: Removed 3 row(s) containing missing values (geom_path).\n\n\n\n\n\nIncorporate this into the pipe when we load covid_df"
  },
  {
    "objectID": "content/03-07-covid-eda.html#rates-vs.-population",
    "href": "content/03-07-covid-eda.html#rates-vs.-population",
    "title": "12  Covid EDA",
    "section": "12.20 Rates vs. population",
    "text": "12.20 Rates vs. population\n\nggplot(covid_df, aes(population, cases_new_rate)) +\n    geom_point()\n\nWarning: Removed 58 rows containing missing values (geom_point)."
  },
  {
    "objectID": "content/03-07-covid-eda.html#counties-by-maximum-rate-of-new-cases",
    "href": "content/03-07-covid-eda.html#counties-by-maximum-rate-of-new-cases",
    "title": "12  Covid EDA",
    "section": "12.21 Counties by maximum rate of new cases",
    "text": "12.21 Counties by maximum rate of new cases\n\ncovid_df |> \n    group_by(county) |> \n    summarize(population = max(population),\n              cases = max(cases_new_rate, na.rm = TRUE)) |> \n    arrange(desc(cases))\n\ngroup_by: one grouping variable (county)\n\n\nsummarize: now 58 rows and 3 columns, ungrouped\n\n\n# A tibble: 58 × 3\n   county   population cases\n   <chr>         <dbl> <dbl>\n 1 Tulare       460477 2056.\n 2 Alpine         1146 1832.\n 3 Imperial     180216 1640.\n 4 Lassen        31185 1260.\n 5 Shasta       179085 1032.\n 6 Tehama        63373 1023.\n 7 Fresno       978130  950.\n 8 Glenn         27897  921.\n 9 Butte        227075  875.\n10 Kings        150075  782.\n# … with 48 more rows"
  },
  {
    "objectID": "content/03-07-covid-eda.html#cases-vs.-deaths-when-we-control-for-population",
    "href": "content/03-07-covid-eda.html#cases-vs.-deaths-when-we-control-for-population",
    "title": "12  Covid EDA",
    "section": "12.22 Cases vs. deaths when we control for population",
    "text": "12.22 Cases vs. deaths when we control for population\n\nggplot(covid_df, aes(x = cases_new_rate, y = deaths_new_rate)) +\n    # geom_point() +\n    geom_point(alpha = .2) +\n    # geom_count(alpha = .5) +\n    # geom_bin2d() +\n    # geom_hex() +\n    # geom_hex(aes(color = after_stat(count))) +\n    # geom_density2d(size = 1) +\n    # stat_density2d(contour = FALSE, geom = 'raster',\n    #                aes(fill = after_stat(density)),\n    #                show.legend = FALSE) +\n    # stat_density2d(contour = TRUE, geom = 'polygon',\n    #                aes(fill = after_stat(level)),\n    #                show.legend = FALSE) +\n    scale_x_log10() +\n    scale_y_log10() +\n    scale_color_viridis_c(aesthetics = c('color', 'fill'))\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous x-axis\n\n\nWarning in self$trans$transform(x): NaNs produced\n\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 556 rows containing missing values (geom_point)."
  },
  {
    "objectID": "content/04-02-readability.html",
    "href": "content/04-02-readability.html",
    "title": "13  Readable code is reliable code",
    "section": "",
    "text": "(BryanCodeSmellsFeels2018?)\nPostolovski (2020)\nBuckens (2019)"
  },
  {
    "objectID": "content/04-02-readability.html#writing-code-is-writing",
    "href": "content/04-02-readability.html#writing-code-is-writing",
    "title": "13  Readable code is reliable code",
    "section": "13.2 Writing code is writing",
    "text": "13.2 Writing code is writing\n\nMultiple target audiences\n\nCollaborators\n(Some) reviewers and readers of the paper\nPeers who want to analyze and extend your methods\nYourself in two months/one year/five years\n\nYour code is readable to the extent that people can use it to easily and reliably\n\npredict,\ndiagnose, and\nextend your code"
  },
  {
    "objectID": "content/04-02-readability.html#code-style",
    "href": "content/04-02-readability.html#code-style",
    "title": "13  Readable code is reliable code",
    "section": "13.3 Code style",
    "text": "13.3 Code style\n\niNéwritteNélanguagEáconventionSéfoRîpunctuatioNøcapitalizatioNîaiDécomprehensioNébYéindicatinGéstructurE\n\nthis is what it’s like to read poorly-styled code\nconventions only work if they’re shared conventions\n\nStyle guides provide shared conventions for readable code\n\nIn-line spacing makes it easier to pick out distinguish functions, operators, and variables in a line\nReturns distinguish arguments in a function call\nIndentation corresponds to structure of complex expressions\nCommon conventions for naming, assignment reduce cognitive load\n\nTidyverse style guide: https://style.tidyverse.org/"
  },
  {
    "objectID": "content/04-02-readability.html#highlights-from-the-tidyverse-style-guide",
    "href": "content/04-02-readability.html#highlights-from-the-tidyverse-style-guide",
    "title": "13  Readable code is reliable code",
    "section": "13.4 Highlights from the Tidyverse style guide",
    "text": "13.4 Highlights from the Tidyverse style guide\n\nPlace all package() calls at the top of the script\nLimit your code to 80 characters per line\nUse at least 4 spaces for indenting multiline expression\n\nControl-I in RStudio will do automagic indenting\n\nIn multiline function calls, 1 argument = 1 line\nUse comments to explain “why” rather than “what” or “how”\n\nDH’s comment convention:\nSingle # is for code that isn’t currently used but might need to be brought back (eg, for debugging)\n## is for substantial comments"
  },
  {
    "objectID": "content/04-02-readability.html#spaces",
    "href": "content/04-02-readability.html#spaces",
    "title": "13  Readable code is reliable code",
    "section": "13.5 Spaces",
    "text": "13.5 Spaces\n\nAlways put spaces after commas, and never before (like English)\nBut not between a function name and the parentheses (like math)\nAnd surrounding infix operators (==, +, -, <-, =)\nPipes %>% |> should have a space before and be at the end of the line"
  },
  {
    "objectID": "content/04-02-readability.html#code-blocks",
    "href": "content/04-02-readability.html#code-blocks",
    "title": "13  Readable code is reliable code",
    "section": "13.6 Code blocks",
    "text": "13.6 Code blocks\nWhen you put a block of code in curly braces {}:\n\n{ should be the last character on a line\n} should be the first character on the line\n\n\nif (y == 0) {\n  if (x > 0) {\n    log(x)\n  } else {\n    message(\"x is negative or zero\")\n  }\n} else {\n  y^x\n}"
  },
  {
    "objectID": "content/04-02-readability.html#boolean-variables-vs.-control-flow",
    "href": "content/04-02-readability.html#boolean-variables-vs.-control-flow",
    "title": "13  Readable code is reliable code",
    "section": "13.7 Boolean variables vs. control flow",
    "text": "13.7 Boolean variables vs. control flow\n\nFunctions that return vectors:\n\n&, |, ==, ifelse(), dplyr::if_else()\n\nFunctions that return a single value:\n\n&&, ||, identical\n\nif (x) a else b only looks at the first (hopefully single) value of x"
  },
  {
    "objectID": "content/04-02-readability.html#references",
    "href": "content/04-02-readability.html#references",
    "title": "13  Readable code is reliable code",
    "section": "13.8 References",
    "text": "13.8 References\n\n\n\n\nBuckens, Wouter. 2019. “Self-Documenting Is a Myth, and How to Make Your Code Self-Documenting.” woubuc. August 3, 2019. https://blog.woubuc.be/post/self-documenting-code-is-a-myth/.\n\n\nPostolovski, Tash. 2020. “Your Code Review Checklist: 14 Things to Include.” July 6, 2020. https://www.codementor.io/blog/code-review-checklist-76q7ovkaqj."
  },
  {
    "objectID": "content/04-03-code-review.html",
    "href": "content/04-03-code-review.html",
    "title": "14  A code review checklist",
    "section": "",
    "text": "Postolovski (2020)\n\n\nReadability\nMaintainability\nSecurity\nSpeed and Performance\nDocumentation\nReinventing the Wheel\nReliability\nScalability\nReusability\nPatterns [style and readability]\nTest coverage and quality\nFit for purpose\nNotice what’s missing\nZoom out"
  },
  {
    "objectID": "content/04-03-code-review.html#readability-understandability",
    "href": "content/04-03-code-review.html#readability-understandability",
    "title": "14  A code review checklist",
    "section": "14.2 1. Readability / understandability",
    "text": "14.2 1. Readability / understandability\n\nFile naming\n\nWhat order should the scripts be run in?\n\nFollows a style guide\n\nOr at least uses consistent style\n\nWhitespace, whitespace, whitespace\nMeaningful names / self-documenting code (Buckens 2019)\nFor data science: Functionality promotes understandability"
  },
  {
    "objectID": "content/04-03-code-review.html#maintainability",
    "href": "content/04-03-code-review.html#maintainability",
    "title": "14  A code review checklist",
    "section": "14.3 2. Maintainability",
    "text": "14.3 2. Maintainability\n\nDRY\nAvoid hard-coded configuration\nCheck strong assumptions about structure and content of inputs\nAvoid deprecated or removed features"
  },
  {
    "objectID": "content/04-03-code-review.html#security",
    "href": "content/04-03-code-review.html#security",
    "title": "14  A code review checklist",
    "section": "14.4 3. Security",
    "text": "14.4 3. Security\n\nHow are passwords for websites, APIs, and sensitive data handled?"
  },
  {
    "objectID": "content/04-03-code-review.html#speed-and-performance",
    "href": "content/04-03-code-review.html#speed-and-performance",
    "title": "14  A code review checklist",
    "section": "14.5 4. Speed and Performance",
    "text": "14.5 4. Speed and Performance\n\n14.5.1 Performance of the script\n\nAvoid (repeating) large downloads\nCache the results of slow calculations\n\nWith an easy way to clear the cache and re-run the calculations\nOr break them up in to separate steps of the pipeline\n\nUse tictoc for simple wall time checks\n\n\n\n14.5.2 Resource use\n\nDoes this really need to be run on a compute cluster?\nHow much will this cost if it’s accidentally run 1,000 times?\nCache raw returns from web scraping and API queries"
  },
  {
    "objectID": "content/04-03-code-review.html#documentation",
    "href": "content/04-03-code-review.html#documentation",
    "title": "14  A code review checklist",
    "section": "14.6 5. Documentation",
    "text": "14.6 5. Documentation\n\nScripts: Section markers, signposting what’s happening and why\nFunctions: Assumptions about inputs, explanation of output\n\nConsider using roxygen2 + R’s package infrastructure\nhttps://r-pkgs.org/man.html#man-functions\n\nData: Provenance, date and time retrieved, codebook\nProject: README, NEWS"
  },
  {
    "objectID": "content/04-03-code-review.html#reinventing-the-wheel",
    "href": "content/04-03-code-review.html#reinventing-the-wheel",
    "title": "14  A code review checklist",
    "section": "14.7 6. Reinventing the Wheel",
    "text": "14.7 6. Reinventing the Wheel\n\nIs there a (well-maintained, checked) package for this?\nDRY"
  },
  {
    "objectID": "content/04-03-code-review.html#reliability",
    "href": "content/04-03-code-review.html#reliability",
    "title": "14  A code review checklist",
    "section": "14.8 7. Reliability",
    "text": "14.8 7. Reliability\nWhat happens if\n\nPackages aren’t installed?\n\nReminder: require() doesn’t stop if the package is missing\n\nData can’t be found?\nData has missing values or errors?\nData has been altered or corrupted?\nA function returns a missing or zero-length value?\nOne script in the middle of the pipeline is altered?"
  },
  {
    "objectID": "content/04-03-code-review.html#skipping-a-few",
    "href": "content/04-03-code-review.html#skipping-a-few",
    "title": "14  A code review checklist",
    "section": "14.9 Skipping a few",
    "text": "14.9 Skipping a few\nBecause I have less to say, not because they’re less important\n\nScalability\nReusability\nPatterns [style and readability]\nTest coverage and quality\nFit for purpose"
  },
  {
    "objectID": "content/04-03-code-review.html#notice-whats-missing",
    "href": "content/04-03-code-review.html#notice-whats-missing",
    "title": "14  A code review checklist",
    "section": "14.10 13. Notice what’s missing",
    "text": "14.10 13. Notice what’s missing\n\nUsing packages that haven’t been loaded\nData validation\n\nEdge cases, unexpected inputs\nMissing values\n\nError handling"
  },
  {
    "objectID": "content/04-03-code-review.html#zoom-out",
    "href": "content/04-03-code-review.html#zoom-out",
    "title": "14  A code review checklist",
    "section": "14.11 14. Zoom out",
    "text": "14.11 14. Zoom out\n\nDoes the code fit the analytic approach described in the paper?\nDoes the analytic approach fit the the research question?"
  },
  {
    "objectID": "content/04-03-code-review.html#references",
    "href": "content/04-03-code-review.html#references",
    "title": "14  A code review checklist",
    "section": "14.12 References",
    "text": "14.12 References\n\n\n\n\n\nBuckens, Wouter. 2019. “Self-Documenting Is a Myth, and How to Make Your Code Self-Documenting.” woubuc. August 3, 2019. https://blog.woubuc.be/post/self-documenting-code-is-a-myth/.\n\n\nPostolovski, Tash. 2020. “Your Code Review Checklist: 14 Things to Include.” July 6, 2020. https://www.codementor.io/blog/code-review-checklist-76q7ovkaqj."
  },
  {
    "objectID": "content/04-04-project-organization.html",
    "href": "content/04-04-project-organization.html",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "",
    "text": "Noble (2009)\n\n\n\n\nThe Economist (2011), “Video: Keith Baggerly, \"When Is Reproducibility an Ethical Issue? Genomics, Personalized Medicine, and Human Error\"” (n.d.)\nHerndon, Ash, and Pollin (2014), but you can just read Bailey and Borwein (Jon) (n.d.), Cassidy (n.d.), and/or watch Reinhart & Rogoff - Growth in a Time of Debt - EXERCISE! (2019)\nLaskowski (n.d.), Viglione (2020), Pennisi (2020)\n“How Excel May Have Caused Loss of 16,000 Covid Tests in England” (2020)"
  },
  {
    "objectID": "content/04-04-project-organization.html#dumpster-organization",
    "href": "content/04-04-project-organization.html#dumpster-organization",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.2 Dumpster organization",
    "text": "15.2 Dumpster organization\n(ref:dumpster) 😱 Source: https://pbs.twimg.com/media/DFca5SRXsAAx1NA\n\n\n\n\n\n(ref:dumpster)\n\n\n\n\n\nDump all of your files into one place\nUse search tools to find what you want\nJust assume that things aren’t getting corrupted\nThe way many Gen Z students think about their files? (Chin 2021)"
  },
  {
    "objectID": "content/04-04-project-organization.html#project-organization",
    "href": "content/04-04-project-organization.html#project-organization",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.3 Project organization",
    "text": "15.3 Project organization\n\nKeep your project self-contained\nLocate files quickly\nPlay nicely with version control\nSelf-document key relationships between project files\n\n\n\n\n\n\nFolder organization is your friend. I have top-level folders for teaching, coding, and writing projects. Then each project or collection of projects lives in its own folder. An ‘Archives’ folder is good for tucking old projects out of the way."
  },
  {
    "objectID": "content/04-04-project-organization.html#model-1-noblequickguideorganizing2009",
    "href": "content/04-04-project-organization.html#model-1-noblequickguideorganizing2009",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.4 Model 1: Noble (2009)",
    "text": "15.4 Model 1: Noble (2009)\n\n\n\n\n\nNoble’s (2009) sample folder structure is designed for experimental biologists.\n\n\n\n\n\nIncludes code for\n\nrunning experiments\ncleaning and analyzing data\ntypesetting a LaTeX file into a PDF\n\nOther features\n\nnotebook file, updated regularly\nchronological folders for experimental runs\nbin for compiled code and source for corresponding source files"
  },
  {
    "objectID": "content/04-04-project-organization.html#model-2-some-of-dhs-projects",
    "href": "content/04-04-project-organization.html#model-2-some-of-dhs-projects",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.5 Model 2: Some of DH’s projects",
    "text": "15.5 Model 2: Some of DH’s projects\nGitHub repo: https://github.com/dhicks/p_curve\n14 directories, 121 files\n.\n├── DESCRIPTION\n├── Makefile\n├── README.md\n├── _deploy.sh\n├── out\n│   ├── estimates_meta.png\n│   ├── estimates_study.png\n│   ├── fig_1_samples_young.png\n│   ├── fig_2_young_composite.png\n│   ├── fig_3_evidence_severity.png\n│   ├── fig_4_evidence_likelihood_zero.png\n│   ├── fig_5_evidence_likelihood_mix.png\n│   ├── likelihood.tex\n│   ├── linearity.png\n│   ├── linearity.tex\n│   ├── samples_schsp.png\n│   ├── samples_simonsohn.png\n│   ├── severity.tex\n│   ├── slopes.png\n│   ├── slopes.tex\n│   ├── slopes_scatter.png\n│   ├── test.png\n│   └── test_out.png\n├── p.curve\n│   ├── DESCRIPTION\n│   ├── NAMESPACE\n│   ├── R\n│   │   └── p-curve.R\n│   ├── man\n│   │   ├── draw_samples.Rd\n│   │   ├── draw_studies.Rd\n│   │   ├── flatten_to_chr.Rd\n│   │   ├── likelihood_ratio.Rd\n│   │   ├── many_metas.Rd\n│   │   ├── p_gap.Rd\n│   │   ├── p_value.Rd\n│   │   ├── qq_linear.Rd\n│   │   ├── qq_plot.Rd\n│   │   ├── qq_slope.Rd\n│   │   ├── schsp_curve.Rd\n│   │   ├── schsp_slope.Rd\n│   │   ├── simonsohn_curve.Rd\n│   │   ├── t_test.Rd\n│   │   ├── young_composite.Rd\n│   │   ├── young_curve.Rd\n│   │   └── young_slope.Rd\n│   └── p.curve.Rproj\n├── paper\n│   ├── *enviro\\ epi\n│   │   └── EE\\ Submission\\ Confirmation\\ for\\ Young's\\ p-value\\ plot\\ does\\ not\\ provide\\ evidence\\ against\\ air\\ pollution\\ hazards.eml\n│   ├── Young\\ papers.gsheet\n│   ├── Young.bib\n│   ├── cover\\ letter.pdf\n│   ├── diff.pdf\n│   ├── ehp\n│   │   ├── A\\ manuscript\\ number\\ has\\ been\\ assigned\\ to\\ Young's\\ p-value\\ plot\\ does\\ not\\ provide\\ evidence\\ against\\ air\\ pollution\\ hazards\\ -\\ [EMID_0ef854c3bb0b5cae].eml\n│   │   ├── Decision\\ on\\ EHP\\ Submission\\ EHP8013\\ -\\ [EMID_932e44ac2192c44f].eml\n│   │   ├── EHP-CFI-form.pdf\n│   │   ├── cover\\ letter.txt\n│   │   ├── paper_2020-07-31.docx\n│   │   └── title\\ page.md\n│   ├── example-refs.bib\n│   ├── fig_1_samples_young.png\n│   ├── fig_2_young_composite.png\n│   ├── fig_3_evidence_severity.png\n│   ├── fig_4_evidence_likelihood_zero.png\n│   ├── fig_5_evidence_likelihood_mix.png\n│   ├── header.yaml\n│   ├── paper.md\n│   ├── paper.pdf\n│   ├── paper.synctex.gz\n│   ├── paper.tex\n│   ├── paper.zip\n│   ├── paper_20201211.md\n│   ├── peerj\n│   │   ├── comments.md\n│   │   └── peerj.pdf\n│   ├── phil\\ med\n│   │   ├── [philmed]\\ Editor\\ Decision.eml\n│   │   └── [philmed]\\ Submission\\ Acknowledgement.eml\n│   ├── render.R\n│   ├── summary.md\n│   ├── summary.pdf\n│   ├── summary.tex\n│   ├── supplement.md\n│   ├── supplement.pdf\n│   ├── title.md\n│   ├── title.pdf\n│   ├── vancouver-superscript.csl\n│   └── wlpeerj.cls\n├── scripts\n│   ├── Makefile\n│   ├── run_metas.R\n│   ├── run_metas.html\n│   ├── run_metas_cache\n│   │   └── html\n│   │       ├── __packages\n│   │       ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.RData\n│   │       ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.rdb\n│   │       ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.rdx\n│   │       ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.RData\n│   │       ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.rdb\n│   │       ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.rdx\n│   │       ├── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.RData\n│   │       ├── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.rdb\n│   │       └── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.rdx\n│   ├── run_metas_files\n│   │   └── figure-html\n│   │       ├── QQ\\ linearity\\ tests-1.png\n│   │       ├── gaps-1.png\n│   │       ├── gaps-2.png\n│   │       ├── likelihood\\ analysis-1.png\n│   │       ├── likelihood\\ analysis-3.png\n│   │       ├── model\\ validation-1.png\n│   │       ├── model\\ validation-2.png\n│   │       ├── power_sim-1.png\n│   │       ├── power_sim-2.png\n│   │       ├── sample\\ plots-1.png\n│   │       ├── sample\\ plots-2.png\n│   │       ├── sample\\ plots-3.png\n│   │       ├── sample\\ plots-4.png\n│   │       ├── severity\\ analysis-1.png\n│   │       ├── severity\\ analysis-2.png\n│   │       ├── slopes-1.png\n│   │       ├── slopes-2.png\n│   │       ├── slopes-3.png\n│   │       ├── slopes-4.png\n│   │       ├── slopes-5.png\n│   │       ├── slopes-6.png\n│   │       ├── slopes-7.png\n│   │       ├── unnamed-chunk-2-1.png\n│   │       ├── unnamed-chunk-4-1.png\n│   │       └── unnamed-chunk-6-1.png\n│   └── scripts.Rproj\n└── tree.md"
  },
  {
    "objectID": "content/04-04-project-organization.html#just-the-directories",
    "href": "content/04-04-project-organization.html#just-the-directories",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.6 Just the directories",
    "text": "15.6 Just the directories\n.\n├── out\n├── p.curve\n│   ├── R\n│   └── man\n├── paper\n│   ├── *enviro\\ epi\n│   ├── ehp\n│   ├── peerj\n│   └── phil\\ med\n└── scripts\n    ├── run_metas_cache\n    │   └── html\n    └── run_metas_files\n        └── figure-html\n\nscripts, paper, and out\np.curve, a little package containing the simulation code\nsimulation and analysis automatically reproduced: https://dhicks.github.io/p_curve/"
  },
  {
    "objectID": "content/04-04-project-organization.html#a-larger-text-mining-project",
    "href": "content/04-04-project-organization.html#a-larger-text-mining-project",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.7 A larger text-mining project",
    "text": "15.7 A larger text-mining project\nPublished paper: https://doi.org/10.1162/qss_a_00150\nGitHub repo: https://github.com/dhicks/orus\n23 directories, 274 files (plus 160k data files)\n.\n├── Makefile\n├── ORU\\ faculty\n│   ├── ORU\\ Faculty.docx\n│   ├── ORU\\ Faculty.html\n│   ├── ORU\\ Publications.docx\n│   ├── ORU\\ Publications.fld\n│   │   ├── colorschememapping.xml\n│   │   ├── filelist.xml\n│   │   ├── header.html\n│   │   ├── image001.png\n│   │   ├── item0001.xml\n│   │   ├── props002.xml\n│   │   └── themedata.thmx\n│   ├── ORU\\ Publications.html\n│   └── auids.csv\n├── ORU\\ founding\\ dates.gsheet\n├── QSS\\ forms\n│   ├── QSS-Checklist-AcceptedManuscripts.docx\n│   ├── QSS_pub_agreement.pdf\n│   └── Quantitative\\ Science\\ Studies\\ -\\ Decision\\ on\\ Manuscript\\ ID\\ QSS-2021-0014.R2.eml\n├── R\n│   ├── api_keys.R\n│   └── hellinger.R\n├── auid\\ flow.txt\n├── data\n│   ├── *ORUs\\ -\\ DSL\\ -\\ Google\\ Drive.webloc\n│   ├── 00_UCD_2016.csv\n│   ├── 00_UCD_2017.csv\n│   ├── 00_UCD_2018.csv\n│   ├── 00_faculty_list.html\n│   ├── 00_manual_matches.csv\n│   ├── 00_publications_list.html\n│   ├── 01_departments.csv\n│   ├── 01_departments_canonical.csv\n│   ├── 01_faculty.Rds\n│   ├── 02_pubs.Rds\n│   ├── 03_codepartmentals.Rds\n│   ├── 03_dropout.Rds\n│   ├── 03_matched.Rds\n│   ├── 03_unmatched.Rds\n│   ├── 04_author_meta.Rds\n│   ├── 04_dropouts.Rds\n│   ├── 04_genderize\n│   ├── 04_namsor.Rds\n│   ├── 05_author_meta.Rds\n│   ├── 05_dept_dummies.Rds\n│   ├── 05_dropouts.Rds\n│   ├── 05_layout.Rds\n│   ├── 05_matched.Rds\n│   ├── 06_author_histories.Rds\n│   ├── 07_coauth_count.Rds\n│   ├── 07_parsed_histories.Rds\n│   ├── 08_phrases.Rds\n│   ├── 09_H.Rds\n│   ├── 09_atm.csv\n│   ├── 09_vocab.tex\n│   ├── 10_atm.csv\n│   ├── 10_atm_pc.Rds\n│   ├── 10_aytm.csv\n│   ├── 10_aytm_comp.csv\n│   ├── 10_aytm_did.csv\n│   ├── 10_model_stats.Rds\n│   ├── 10_models.Rds\n│   ├── 11_au_dept_xwalk.Rds\n│   ├── 11_departments.csv\n│   ├── 11_departments_canonical.csv\n│   ├── 11_dept_dummies.Rds\n│   ├── 11_dept_gamma.Rds\n│   ├── 11_dept_term_matrix.Rds\n│   ├── 11_oru_gamma.Rds\n│   ├── 11_oru_term_matrix.Rds\n│   ├── 11_test_train.Rds\n│   ├── 12_layout.Rds\n│   ├── author_histories [7665 entries exceeds filelimit, not opening dir]\n│   ├── authors_meta [6020 entries exceeds filelimit, not opening dir]\n│   ├── docs [145144 entries exceeds filelimit, not opening dir]\n│   ├── ldatuning_results\n│   │   ├── tuningResult_comp.Rds\n│   │   ├── tuningResult_comp.docx\n│   │   ├── tuningResult_comp.pdf\n│   │   ├── tuningResult_did.Rds\n│   │   └── tuningResult_did.pdf\n│   ├── ldatuning_results-20190415T164055Z-001.zip\n│   ├── parsed_blocks [430 entries exceeds filelimit, not opening dir]\n│   ├── pubs [282 entries exceeds filelimit, not opening dir]\n│   └── temp\n├── interdisciplinarity\\ project\\ notes.gdoc\n├── notes.txt\n├── paper\n│   ├── QSS_a_00150-Hicks_Proof1.pdf\n│   ├── apa-6th-edition.csl\n│   ├── cover\\ letter.txt\n│   ├── diff.pdf\n│   ├── header.yaml\n│   ├── img\n│   │   ├── ORU_DAG.png\n│   │   ├── cites_regression.png\n│   │   ├── coauths_regression.png\n│   │   ├── conceptual_model.png\n│   │   ├── dept_dist_fixed_reg.png\n│   │   ├── dept_dist_reg.png\n│   │   ├── dept_gamma.png\n│   │   ├── dept_hell_net.png\n│   │   ├── dept_hell_net_50.png\n│   │   ├── entropies.png\n│   │   ├── entropies_selected.png\n│   │   ├── entropy_regression.png\n│   │   ├── gender.png\n│   │   ├── mds.png\n│   │   ├── mds_dept.png\n│   │   ├── network.png\n│   │   ├── oru_dept_entropy.png\n│   │   ├── oru_dept_min_dist.png\n│   │   ├── oru_dept_min_dist_ridges.png\n│   │   ├── oru_dept_network.png\n│   │   ├── oru_dept_org_dist.png\n│   │   ├── oru_dept_org_dist_ridges.png\n│   │   ├── oru_gamma.png\n│   │   ├── pub_regression.png\n│   │   └── sample.png\n│   ├── lit\\ review\\ notes.txt\n│   ├── oru_paper.aux\n│   ├── oru_paper.log\n│   ├── oru_paper.md\n│   ├── oru_paper.out\n│   ├── oru_paper.pdf\n│   ├── oru_paper.synctex.gz\n│   ├── oru_paper.tex\n│   ├── oru_paper.zip\n│   ├── oru_paper_20200616.pdf\n│   ├── oru_paper_20210805.pdf\n│   ├── oru_project.bib\n│   ├── oru_project.yaml\n│   ├── response1.gdoc\n│   ├── response1.pdf\n│   ├── response2.gdoc\n│   ├── response2.pdf\n│   ├── scraps\n│   │   ├── Hellinger.md\n│   │   ├── Holbrook.md\n│   │   ├── table.md\n│   │   └── table.pdf\n│   ├── supplement.md\n│   └── supplement.pdf\n├── plots\n│   ├── 12_beta.tex\n│   ├── 12_cites_regression.png\n│   ├── 12_coauths_regression.png\n│   ├── 12_dept_dist_fixed_reg.png\n│   ├── 12_dept_dist_reg.png\n│   ├── 12_dept_gamma.png\n│   ├── 12_dept_hell_net.png\n│   ├── 12_dept_hell_net_50.png\n│   ├── 12_dept_topics.png\n│   ├── 12_entropies.png\n│   ├── 12_entropies_selected.png\n│   ├── 12_entropy_regression.png\n│   ├── 12_gender.png\n│   ├── 12_mds.png\n│   ├── 12_mds_dept.png\n│   ├── 12_mds_wide.png\n│   ├── 12_network.png\n│   ├── 12_oru_dept_entropy.png\n│   ├── 12_oru_dept_mean_dist.png\n│   ├── 12_oru_dept_mean_dist_ridges.png\n│   ├── 12_oru_dept_min_dist.png\n│   ├── 12_oru_dept_min_dist_ridges.png\n│   ├── 12_oru_dept_network.png\n│   ├── 12_oru_dept_org_dist.png\n│   ├── 12_oru_dept_org_dist_ridges.png\n│   ├── 12_oru_entropy.png\n│   ├── 12_oru_gamma.png\n│   ├── 12_pub_regression.png\n│   ├── 12_sample.png\n│   └── ORU_DAG.png\n├── presentations\n│   └── 2019-06-07\\ for\\ Paul\\ Dodd.gslides\n├── questions\\ for\\ jane.md\n├── scripts\n│   ├── 01_parse_faculty_list.R\n│   ├── 02_Scopus_search_results.R\n│   ├── 03_match.R\n│   ├── 03_matched.csv\n│   ├── 04_author_meta.R\n│   ├── 05_filtering.R\n│   ├── 06_author_histories.R\n│   ├── 07_complete_histories.R\n│   ├── 08_text_annotation.R\n│   ├── 09_build_vocab.R\n│   ├── 10_topic_modeling.R\n│   ├── 11_depts.R\n│   ├── 11_depts.html\n│   ├── 12_analysis\\ copy.html\n│   ├── 12_analysis-matched.html\n│   ├── 12_analysis.R\n│   ├── 12_analysis.html\n│   ├── 12_analysis_cache\n│   │   └── html\n│   │       ├── __packages\n│   │       ├── mds_viz_efd9009c794d667852b2549df2bccf96.RData\n│   │       ├── mds_viz_efd9009c794d667852b2549df2bccf96.rdb\n│   │       ├── mds_viz_efd9009c794d667852b2549df2bccf96.rdx\n│   │       ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.RData\n│   │       ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.rdb\n│   │       ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.rdx\n│   │       ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.RData\n│   │       ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.rdb\n│   │       ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.rdx\n│   │       ├── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.RData\n│   │       ├── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdb\n│   │       └── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdx\n│   ├── 12_analysis_files\n│   │   └── figure-html\n│   │       ├── author-dept\\ distance-1.png\n│   │       ├── author-dept\\ distance-2.png\n│   │       ├── author-dept\\ distance-3.png\n│   │       ├── author-dept\\ distance-4.png\n│   │       ├── author-dept\\ distance-5.png\n│   │       ├── desc_plots_tabs-1.png\n│   │       ├── desc_plots_tabs-2.png\n│   │       ├── desc_plots_tabs-3.png\n│   │       ├── desc_plots_tabs-4.png\n│   │       ├── h3-1.png\n│   │       ├── h3-2.png\n│   │       ├── h3-3.png\n│   │       ├── h3-4.png\n│   │       ├── h3-5.png\n│   │       ├── h3-6.png\n│   │       ├── mds_viz-1.png\n│   │       ├── mds_viz-10.png\n│   │       ├── mds_viz-11.png\n│   │       ├── mds_viz-12.png\n│   │       ├── mds_viz-13.png\n│   │       ├── mds_viz-14.png\n│   │       ├── mds_viz-2.png\n│   │       ├── mds_viz-3.png\n│   │       ├── mds_viz-4.png\n│   │       ├── mds_viz-5.png\n│   │       ├── mds_viz-6.png\n│   │       ├── mds_viz-7.png\n│   │       ├── mds_viz-8.png\n│   │       ├── mds_viz-9.png\n│   │       ├── network-1.png\n│   │       ├── network-2.png\n│   │       ├── productivity-1.png\n│   │       ├── productivity-2.png\n│   │       ├── productivity-3.png\n│   │       ├── productivity-4.png\n│   │       ├── productivity-5.png\n│   │       ├── productivity-6.png\n│   │       ├── productivity-7.png\n│   │       ├── productivity-8.png\n│   │       ├── productivity-9.png\n│   │       ├── silhouette-1.png\n│   │       ├── topic_models-1.png\n│   │       ├── topic_models-10.png\n│   │       ├── topic_models-11.png\n│   │       ├── topic_models-12.png\n│   │       ├── topic_models-13.png\n│   │       ├── topic_models-2.png\n│   │       ├── topic_models-3.png\n│   │       ├── topic_models-4.png\n│   │       ├── topic_models-5.png\n│   │       ├── topic_models-6.png\n│   │       ├── topic_models-7.png\n│   │       ├── topic_models-8.png\n│   │       ├── topic_models-9.png\n│   │       ├── topic_viz-1.png\n│   │       └── topic_viz-2.png\n│   ├── api_key.R\n│   └── scraps\n│       ├── 02_parse_pubs_list.R\n│       ├── 03_coe_pubs.R\n│       ├── 03_match_auids.R\n│       ├── 07.R\n│       ├── 12_regressions.R\n│       ├── BML-CMSI\\ deep\\ dive.R\n│       ├── Hellinger_low_memory.R\n│       ├── dept_hell_net.R\n│       ├── divergence\\ against\\ lagged\\ distributions.R\n│       ├── exploring\\ topics.R\n│       ├── fractional_authorship.R\n│       ├── hellinger.R\n│       ├── model_scratch.R\n│       ├── multicore.R\n│       ├── net_viz.R\n│       ├── prcomp.R\n│       ├── propensity.R\n│       ├── rs_diversity.R\n│       ├── spacyr.R\n│       ├── topic\\ counts\\ rather\\ than\\ entropies.R\n│       ├── topic_cosine_sim.R\n│       ├── unit-level.R\n│       ├── weighted\\ regression.R\n│       ├── word-topic_distance.R\n│       ├── xx_construct_samples.R\n│       └── xx_oru_complete_histories.R\n└── tree.md"
  },
  {
    "objectID": "content/04-04-project-organization.html#just-the-directories-1",
    "href": "content/04-04-project-organization.html#just-the-directories-1",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.8 Just the directories",
    "text": "15.8 Just the directories\n.\n├── ORU\\ faculty\n│   └── ORU\\ Publications.fld\n├── QSS\\ forms\n├── R\n├── data\n│   ├── author_histories\n│   ├── authors_meta\n│   ├── docs\n│   ├── ldatuning_results\n│   ├── parsed_blocks\n│   ├── pubs\n│   └── temp\n├── paper\n│   ├── img\n│   └── scraps\n├── plots\n├── presentations\n└── scripts\n    ├── 12_analysis_cache\n    │   └── html\n    ├── 12_analysis_files\n    │   └── figure-html\n    └── scraps"
  },
  {
    "objectID": "content/04-04-project-organization.html#dhs-project-template",
    "href": "content/04-04-project-organization.html#dhs-project-template",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.9 DH’s Project Template",
    "text": "15.9 DH’s Project Template\n\nhttps://github.com/dhicks/project_template\nConfigured as a GitHub “template,” making it easy to create new repositories for new projects\nDesignated folders for data, plots/outputs, and utility functions"
  },
  {
    "objectID": "content/04-04-project-organization.html#a-reminder-on-paths",
    "href": "content/04-04-project-organization.html#a-reminder-on-paths",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.10 A reminder on paths",
    "text": "15.10 A reminder on paths\n\nWindows and Unix-based systems write paths differently\nUse file.path() or the here package to construct paths\n.. in a path means “go up to the parent folder”\n\nso ../data/00_raw_data.csv goes up one level (eg, from the scripts folder), then down to the data folder, then the file 00_raw_data.csv"
  },
  {
    "objectID": "content/04-04-project-organization.html#references",
    "href": "content/04-04-project-organization.html#references",
    "title": "15  The life-changing magic of tidying your projects",
    "section": "15.11 References",
    "text": "15.11 References\n\n\n\n\nBailey, David H., and Jonathan Borwein (Jon). n.d. “The Reinhart-Rogoff Error – or How Not to Excel at Economics.” The Conversation. Accessed May 16, 2020. http://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646.\n\n\nCassidy, John. n.d. “The Reinhart and Rogoff Controversy: A Summing Up.” The New Yorker. Accessed September 27, 2020. https://www.newyorker.com/news/john-cassidy/the-reinhart-and-rogoff-controversy-a-summing-up.\n\n\nChin, Monica. 2021. “Students Who Grew up with Search Engines Might Change STEM Education Forever.” The Verge. September 22, 2021. https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” Cambridge Journal of Economics 38 (2): 257–79. https://doi.org/10.1093/cje/bet075.\n\n\n“How Excel May Have Caused Loss of 16,000 Covid Tests in England.” 2020. The Guardian. October 5, 2020. http://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england.\n\n\nLaskowski, Kate. n.d. “What to Do When You Don’t Trust Your Data Anymore – Laskowski Lab at UC Davis.” Accessed January 29, 2020. https://laskowskilab.faculty.ucdavis.edu/2020/01/29/retractions/.\n\n\nNoble, William Stafford. 2009. “A Quick Guide to Organizing Computational Biology Projects.” PLOS Computational Biology 5 (7): e1000424. https://doi.org/10.1371/journal.pcbi.1000424.\n\n\nPennisi, Elizabeth. 2020. “Prominent Spider Biologist Spun a Web of Questionable Data.” Science 367 (6478): 613–14. https://doi.org/10.1126/science.367.6478.613.\n\n\nReinhart & Rogoff - Growth in a Time of Debt - EXERCISE! 2019. https://www.youtube.com/watch?v=ItGMz0ERvcw.\n\n\nThe Economist. 2011. “An Array of Errors,” September 10, 2011. https://www.economist.com/science-and-technology/2011/09/10/an-array-of-errors.\n\n\n“Video: Keith Baggerly, \"When Is Reproducibility an Ethical Issue? Genomics, Personalized Medicine, and Human Error\".” n.d. Accessed September 23, 2020. http://www.birs.ca/events/2013/5-day-workshops/13w5083/videos/watch/201308141121-Baggerly.html.\n\n\nViglione, Giuliana. 2020. “‘Avalanche’ of Spider-Paper Retractions Shakes Behavioural-Ecology Community.” Nature, February. https://doi.org/10.1038/d41586-020-00287-y."
  },
  {
    "objectID": "content/04-05-data-management.html",
    "href": "content/04-05-data-management.html",
    "title": "16  Managing and publishing data",
    "section": "",
    "text": "Hudon (2018)\nWilkinson et al. (2016)"
  },
  {
    "objectID": "content/04-05-data-management.html#the-first-rule-of-data-management",
    "href": "content/04-05-data-management.html#the-first-rule-of-data-management",
    "title": "16  Managing and publishing data",
    "section": "16.2 The first rule of data management",
    "text": "16.2 The first rule of data management\nDo not edit your data."
  },
  {
    "objectID": "content/04-05-data-management.html#documentation",
    "href": "content/04-05-data-management.html#documentation",
    "title": "16  Managing and publishing data",
    "section": "16.3 Documentation",
    "text": "16.3 Documentation\n\nMany social science fields have a tradition of writing codebooks for their data\n\nStanford Open Policing codebook\n“Codebook-like summary” of the covdata package, automatically generated using skimr\nCaitlin Hudon’s approach (Hudon 2018)\n\nTable and field name, both verbatim\nField example value\nNotes for both table and field\n\n\n\n(ref:Hudson-tbl) Example of Caitlin Hudon’s approach to building a data dictionary. Source: https://caitlinhudon.com/2018/10/30/data-dictionaries/\n\n\n\n\n\n(ref:Hudson-tbl)"
  },
  {
    "objectID": "content/04-05-data-management.html#questions-a-codebook-should-answer",
    "href": "content/04-05-data-management.html#questions-a-codebook-should-answer",
    "title": "16  Managing and publishing data",
    "section": "16.4 Questions a codebook should answer",
    "text": "16.4 Questions a codebook should answer\n\nWhat does this field mean? How should I use it?\nWhat is the data [journey]?\n\nWhere does this data come from?\nHow exactly is it collected?\nHow often is it updated?\nWhere does it go next?\n\nWhat does the data in this field actually look like?\nAre there any caveats to keep in mind when using this data?\nWhere can I go for more information?\n\n(Hudon 2018)"
  },
  {
    "objectID": "content/04-05-data-management.html#major-codebook-elements",
    "href": "content/04-05-data-management.html#major-codebook-elements",
    "title": "16  Managing and publishing data",
    "section": "16.5 Major codebook elements",
    "text": "16.5 Major codebook elements\n(https://afit-r.github.io/codebook)\n\nOriginal source of the data\nSampling information\n\nWhere and how the data were generated\n\nVariable-level metadata and summaries\nStructure of the data"
  },
  {
    "objectID": "content/04-05-data-management.html#data-management-plans",
    "href": "content/04-05-data-management.html#data-management-plans",
    "title": "16  Managing and publishing data",
    "section": "16.6 Data management plans",
    "text": "16.6 Data management plans\n\nMuch like a research plan, data management plans provide an overview of the steps you’ll take to gather, publish, and maintain your data\n\nSince 2011, NSF has required a 2-page data management plan for most types of proposals\n\nExamples and resources\n\nUCM Library \nUCSD NSF examples\n\nSBE example 1\nSBE example 2\n\nNSF policy summary\n\nSBE-specific guidance"
  },
  {
    "objectID": "content/04-05-data-management.html#data-management-plan-common-elements",
    "href": "content/04-05-data-management.html#data-management-plan-common-elements",
    "title": "16  Managing and publishing data",
    "section": "16.7 Data management plan: Common elements",
    "text": "16.7 Data management plan: Common elements\n\nWho is responsible for data management\nWho else will have access to which data\nHow data will be collected\nData formatting standards\nWhether and how data will be archived and made available for reuse"
  },
  {
    "objectID": "content/04-05-data-management.html#fair-principles-for-published-data",
    "href": "content/04-05-data-management.html#fair-principles-for-published-data",
    "title": "16  Managing and publishing data",
    "section": "16.8 FAIR principles for published data",
    "text": "16.8 FAIR principles for published data\n\nFindable\n\nF1. (meta)data are assigned a globally unique and persistent identifier\nF2. data are described with rich metadata (defined by R1 below)\nF3. metadata clearly and explicitly include the identifier of the data it describes\nF4. (meta)data are registered or indexed in a searchable resource\n\nAccessible\n\nA1. (meta)data are retrievable by their identifier using a standardized communications protocol\n\nA1.1 the protocol is open, free, and universally implementable\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\n\nA2. metadata are accessible, even when the data are no longer available\n\nInteroperable\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nI2. (meta)data use vocabularies that follow FAIR principles\nI3. (meta)data include qualified references to other (meta)data\n\nReusable\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes\n\nR1.1. (meta)data are released with a clear and accessible data usage license\nR1.2. (meta)data are associated with detailed provenance\nR1.3. (meta)data meet domain-relevant community standards"
  },
  {
    "objectID": "content/04-05-data-management.html#dois-for-data",
    "href": "content/04-05-data-management.html#dois-for-data",
    "title": "16  Managing and publishing data",
    "section": "16.9 DOIs for data",
    "text": "16.9 DOIs for data\n\nInstructions for OSF\nNotes for Zenodo\n\nZenodo also plays nicely with GitHub for minting DOIs for code\n\nCitation models at Harvard Dataverse"
  },
  {
    "objectID": "content/04-05-data-management.html#references",
    "href": "content/04-05-data-management.html#references",
    "title": "16  Managing and publishing data",
    "section": "16.10 References",
    "text": "16.10 References\n\n\n\n\nHudon, Caitlin. 2018. “Field Notes: Building Data Dictionaries.” Haystacks. October 30, 2018. https://caitlinhudon.com/2018/10/30/data-dictionaries/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1, 1): 1–9. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "content/04-07-renv.html",
    "href": "content/04-07-renv.html",
    "title": "18  Tracking package versions with renv",
    "section": "",
    "text": "renv homepage\nTwo date-based alternatives\n\ncheckpoint\ngroundhog"
  },
  {
    "objectID": "content/04-07-renv.html#the-problem-renv-tries-to-solve",
    "href": "content/04-07-renv.html#the-problem-renv-tries-to-solve",
    "title": "18  Tracking package versions with renv",
    "section": "18.2 The problem renv tries to solve",
    "text": "18.2 The problem renv tries to solve\n(ref:datacolada) dplyr 0.5.0 introduced a breaking change to distinct() in June 2016. Source: https://datacolada.org/95\n\n\n\n\n\n(ref:datacolada)"
  },
  {
    "objectID": "content/04-07-renv.html#how-r-locates-packages",
    "href": "content/04-07-renv.html#how-r-locates-packages",
    "title": "18  Tracking package versions with renv",
    "section": "18.3 How R locates packages",
    "text": "18.3 How R locates packages\n\n.libPaths()\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.1/Resources/library\""
  },
  {
    "objectID": "content/04-07-renv.html#renv-workflow",
    "href": "content/04-07-renv.html#renv-workflow",
    "title": "18  Tracking package versions with renv",
    "section": "18.4 renv workflow",
    "text": "18.4 renv workflow\n\nInitialize renv for a project with renv::init()\nTrack renv.lock in version control\nrenv::snapshot() to update the lockfile\nrenv::restore() to install local copies of the packages to match the lockfile"
  },
  {
    "objectID": "content/04-07-renv.html#example-with-the-learning-make-project",
    "href": "content/04-07-renv.html#example-with-the-learning-make-project",
    "title": "18  Tracking package versions with renv",
    "section": "18.5 Example: with the learning-make project",
    "text": "18.5 Example: with the learning-make project\n\nCheck .libPaths()\nrenv::init() (and snapshot()?)\nWhat did this do?\n\n.libPaths()\ngit status\n\nDelete renv/library and renv::restore()"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016.\n“Machine Bias: There’s Software Used\nAcross the Country to Predict Future\nCriminals. And It’s Biased Against\nBlacks.” ProPublica. May 23, 2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nBailey, David H., and Jonathan Borwein (Jon). n.d. “The\nReinhart-Rogoff Error – or How Not to\nExcel at Economics.” The Conversation.\nAccessed May 16, 2020. http://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646.\n\n\nBrown, James Robert. 2002. Smoke and Mirrors: How\nScience Reflects Reality. Routledge.\n\n\nBryan, Jenny. 2020. “Object of Type ‘Closure’ Is Not\nSubsettable.” Presented at the\nRSTUDIO::CONF 2020, January 31. https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/.\n\n\nBuckens, Wouter. 2019. “Self-Documenting Is a Myth, and How to\nMake Your Code Self-Documenting.” woubuc. August 3,\n2019. https://blog.woubuc.be/post/self-documenting-code-is-a-myth/.\n\n\nCassidy, John. n.d. “The Reinhart and Rogoff\nControversy: A Summing Up.” The New\nYorker. Accessed September 27, 2020. https://www.newyorker.com/news/john-cassidy/the-reinhart-and-rogoff-controversy-a-summing-up.\n\n\nChambers, John M. 2014. “Object-Oriented Programming,\nFunctional Programming and R.”\nStatistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452.\n\n\nChin, Monica. 2021. “Students Who Grew up with Search Engines\nMight Change STEM Education Forever.” The\nVerge. September 22, 2021. https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z.\n\n\nCorbett-Davies, Sam, Emma Pierson, Avi Feller, and Sharad Goel. 2016.\n“A Computer Program Used for Bail and Sentencing Decisions Was\nLabeled Biased Against Blacks. It’s Actually Not That\nClear.” Washington Post. October 17, 2016. https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/.\n\n\nDaston, Lorraine, and Peter Galison. 2007. Objectivity.\nNew York : Cambridge, Mass: Zone Books ; Distributed\nby the MIT Press.\n\n\nGentleman, Robert, and Duncan Temple Lang. 2004. “Statistical\nAnalyses and Reproducible Research.”\nBioconductor Project Working Papers, May. https://biostats.bepress.com/bioconductor/paper2.\n\n\nHanna, Alex, Emily Denton, Andrew Smart, and Jamila Smith-Loud. 2019.\n“Towards a Critical Race Methodology in\nAlgorithmic Fairness.” December 7, 2019. https://doi.org/10.1145/3351095.3372826.\n\n\nHealy, Kieran. 2018. Data Visualization: A\nPractical Introduction. Princeton University\nPress. https://books.google.com?id=3XOYDwAAQBAJ.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High\nPublic Debt Consistently Stifle Economic Growth? A Critique of\nReinhart and Rogoff.” Cambridge\nJournal of Economics 38 (2): 257–79. https://doi.org/10.1093/cje/bet075.\n\n\nHester, Jim, the STAT 545 TAs. n.d. Happy Git and\nGitHub for the useR.\nAccessed March 1, 2020. https://happygitwithr.com/.\n\n\nHoffmann, Anna Lauren. 2019. “Where Fairness Fails: Data,\nAlgorithms, and the Limits of Antidiscrimination Discourse.”\nInformation, Communication & Society 22 (7): 900–915. https://doi.org/10.1080/1369118X.2019.1573912.\n\n\n“How Excel May Have Caused Loss of 16,000\nCovid Tests in England.” 2020.\nThe Guardian. October 5, 2020. http://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england.\n\n\nHudon, Caitlin. 2018. “Field Notes: Building\nData Dictionaries.” Haystacks. October\n30, 2018. https://caitlinhudon.com/2018/10/30/data-dictionaries/.\n\n\nHuebner, Marianne, Werner Vach, and Saskia le Cessie. 2016. “A\nSystematic Approach to Initial Data Analysis Is Good Research\nPractice.” The Journal of Thoracic and Cardiovascular\nSurgery 151 (1): 25–27. https://doi.org/10.1016/j.jtcvs.2015.09.085.\n\n\nKleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan. 2016.\n“Inherent Trade-Offs in the Fair\nDetermination of Risk Scores.” November 17,\n2016. http://arxiv.org/abs/1609.05807.\n\n\nLaskowski, Kate. n.d. “What to Do When You Don’t Trust Your Data\nAnymore – Laskowski Lab at UC Davis.”\nAccessed January 29, 2020. https://laskowskilab.faculty.ucdavis.edu/2020/01/29/retractions/.\n\n\nLiboiron, Max. 2021. Pollution Is Colonialism.\nDuke University Press. https://books.google.com?id=NL4lEAAAQBAJ.\n\n\nLyon, A. 2014. “Why Are Normal Distributions\nNormal?” The British Journal for the Philosophy of\nScience 65 (3): 621–49. https://doi.org/10.1093/bjps/axs046.\n\n\nMatejka, Justin, and George Fitzmaurice. n.d. “Same\nStats, Different Graphs: Generating\nDatasets with Varied Appearance and\nIdentical Statistics Through Simulated\nAnnealing (The Datasaurus Dozen) | Autodesk\nResearch.” Accessed May 21, 2017. https://www.autodeskresearch.com/publications/samestats.\n\n\nMcElreath, Richard, dir. 2020. Science as Amateur Software\nDevelopment. https://www.youtube.com/watch?v=zwRdO9_GGhY.\n\n\nNoble, William Stafford. 2009. “A Quick Guide to\nOrganizing Computational Biology Projects.” PLOS\nComputational Biology 5 (7): e1000424. https://doi.org/10.1371/journal.pcbi.1000424.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of\nData Science: A Guide for Anyone Who\nWorks with Data. Leanpub. http://leanpub.com/artofdatascience.\n\n\nPennisi, Elizabeth. 2020. “Prominent Spider Biologist Spun a Web\nof Questionable Data.” Science 367 (6478): 613–14. https://doi.org/10.1126/science.367.6478.613.\n\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel\nJenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A\nLarge-Scale Analysis of Racial Disparities in Police Stops Across the\nUnited States.” Nature Human Behaviour,\nMay, 1–10. https://doi.org/10.1038/s41562-020-0858-1.\n\n\nPostolovski, Tash. 2020. “Your Code Review Checklist:\n14 Things to Include.” July 6, 2020. https://www.codementor.io/blog/code-review-checklist-76q7ovkaqj.\n\n\n“R Faq - How to Make a Great R\nReproducible Example.” n.d. Stack Overflow. Accessed\nAugust 31, 2018. https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example.\n\n\nReinhart & Rogoff - Growth in a\nTime of Debt - EXERCISE!\n2019. https://www.youtube.com/watch?v=ItGMz0ERvcw.\n\n\nSelbst, Andrew D., Danah Boyd, Sorelle Friedler, Suresh\nVenkatasubramanian, and Janet Vertesi. 2018. “Fairness and\nAbstraction in Sociotechnical Systems.”\nSSRN Scholarly Paper ID 3265913. Rochester, NY:\nSocial Science Research Network. https://papers.ssrn.com/abstract=3265913.\n\n\nTanweer, Anissa, Emily Kalah Gade, P. M. Krafft, and Sarah K. Dreier.\n2021. “Why the Data Revolution Needs Qualitative\nThinking.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.eee0b0da.\n\n\nThe Economist. 2011. “An Array of Errors,”\nSeptember 10, 2011. https://www.economist.com/science-and-technology/2011/09/10/an-array-of-errors.\n\n\nTukey, John W. 1980. “We Need Both Exploratory and\nConfirmatory.” The American Statistician 34\n(1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis.\nAddison-Wesley Series in Behavioral Science. Reading,\nMass: Addison-Wesley Pub. Co. https://archive.org/details/exploratorydataa00tuke_0.\n\n\n“Video: Keith Baggerly, \"When Is\nReproducibility an Ethical Issue? Genomics,\nPersonalized Medicine, and Human\nError\".” n.d. Accessed September 23, 2020. http://www.birs.ca/events/2013/5-day-workshops/13w5083/videos/watch/201308141121-Baggerly.html.\n\n\nViglione, Giuliana. 2020. “‘Avalanche’\nof Spider-Paper Retractions Shakes Behavioural-Ecology\nCommunity.” Nature, February. https://doi.org/10.1038/d41586-020-00287-y.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der\nMaas, and Rogier A. Kievit. 2012. “An Agenda for\nPurely Confirmatory Research.” Perspectives on\nPsychological Science 7 (6): 632–38. https://doi.org/10.1177/1745691612463078.\n\n\nWang, Tricia. 2016. “Why Big Data Needs Thick\nData.” Medium. January 20, 2016. https://medium.com/ethnography-matters/why-big-data-needs-thick-data-b4b3e75e3d7#.ur5gl84nj.\n\n\nWeissgerber, Tracey L., Natasa M. Milic, Stacey J. Winham, and Vesna D.\nGarovic. 2015. “Beyond Bar and Line\nGraphs: Time for a New Data Presentation\nParadigm.” PLoS Biol 13 (4): e1002128. https://doi.org/10.1371/journal.pbio.1002128.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of\nGraphics.” Journal of Computational and\nGraphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098.\n\n\n———. 2014. Advanced R. CRC Press. https://adv-r.hadley.nz.\n\n\n———. 2019. “Debugging.” In Advanced R,\nSecond Edition, ch. 22. CRC Press. https://adv-r.hadley.nz/debugging.html.\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. n.d.\nGgplot2: Elegant Graphics for Data\nAnalysis. Third edition. Accessed September 20, 2021. https://ggplot2-book.org/index.html.\n\n\nWilkinson, Leland. 1999. The Grammar of\nGraphics. Statistics and Computing.\nNew York, NY: Springer New York. https://doi.org/10.1007/978-1-4757-3100-2.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg,\nGabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.\n2016. “The FAIR Guiding Principles for Scientific\nData Management and Stewardship.” Scientific Data 3 (1,\n1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex\nNederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in\nScientific Computing.” PLOS Computational Biology 13\n(6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "content/01-01-intro.html#references",
    "href": "content/01-01-intro.html#references",
    "title": "1  Introduction",
    "section": "1.7 References",
    "text": "1.7 References\n\n\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1, 1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "content/01-02-setup.html#r-and-rstudio",
    "href": "content/01-02-setup.html#r-and-rstudio",
    "title": "2  Software, hardware, and accounts",
    "section": "2.2 R and RStudio",
    "text": "2.2 R and RStudio\nR and RStudio\n\n\nIn 2000 R had a major release (version 4)\nAddressing some bugs has led to some rapid development in R and RStudio\nFor consistency in class, I encourage you to use the latest version of both R and RStudio\nHelp on switching between different versions of R"
  },
  {
    "objectID": "content/01-02-setup.html#os",
    "href": "content/01-02-setup.html#os",
    "title": "2  Software, hardware, and accounts",
    "section": "2.3 OS",
    "text": "2.3 OS\n[package managers]\n\nAll of the tools we’ll use are available on all major OSes\n\n(Except Sourcetree, which isn’t essential. List of git GUIs)\n\nI have much less experience debugging Windows and Linux, but will do what I can"
  },
  {
    "objectID": "content/01-02-setup.html#github",
    "href": "content/01-02-setup.html#github",
    "title": "2  Software, hardware, and accounts",
    "section": "2.4 GitHub",
    "text": "2.4 GitHub\n\nYou’ll need a GitHub account and Sourcetree\n\n\n\nGitHub is owned by Microsoft\nAnd there are concerns about how it mined public repositories to develop a code completion tool\nBut it offers some key features that I want to use in this course\nYou can do everything this course requires in private repositories, so long as you give access to the people who need access"
  },
  {
    "objectID": "content/01-02-setup.html#hardware",
    "href": "content/01-02-setup.html#hardware",
    "title": "2  Software, hardware, and accounts",
    "section": "2.5 Hardware",
    "text": "2.5 Hardware\n\nMost sessions will involve writing code in class\nI strongly recommend against tablets (iPad, Surface) for this course\nI recommend that your machine have at least\n\n200 GB free hard drive space\n8 GB RAM\nIntel i5 processor\nOperating system capable of running R 4.1 or higher\n\nMac: 10.13 (High Sierra) or more recent\nWindows: See here\nIf you’re running Linux I’m going to assume you can figure this out yourself"
  },
  {
    "objectID": "content/01-02-setup.html#accessibility",
    "href": "content/01-02-setup.html#accessibility",
    "title": "2  Software, hardware, and accounts",
    "section": "2.6 Accessibility",
    "text": "2.6 Accessibility\nI chose the tools and platforms for this course in part because they’re industry-standard. If you pursue a career as a data scientist in industry, you’ll be expected to use GitHub (or something similar) on a daily basis. And RStudio is by far the most commonly used IDE (“integrated development environment”) for R.\nHowever, like many other technologies, they were originally developed using ableist assumptions about “normal” computer users. In response to criticism, the developers of these systems and tools have gone back and made their technologies more accessible. But there may still be barriers to accessibility that I have not anticipated.\nIf you encounter a barrier to participating in this course — even a small inconvenience — please let me know. Similarly, if you have ideas for making the course more accessible, please share them with me."
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#references",
    "href": "content/01-03-data-sci-wtf.html#references",
    "title": "3  Data Science: What and why",
    "section": "3.5 References",
    "text": "3.5 References\n\n\n\n\nMcElreath, Richard, dir. 2020. Science as Amateur Software Development. https://www.youtube.com/watch?v=zwRdO9_GGhY.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "content/02-02-git.html#initial-commit",
    "href": "content/02-02-git.html#initial-commit",
    "title": "4  Git for version control",
    "section": "4.4 Initial commit",
    "text": "4.4 Initial commit\n\nInstall Sourcetree and go through the configuration steps\nThen you’ll see the (empty) repository browser\nCreate a folder called something like learning-git\nThen, in Sourcetree:\n\nOn the Local tab\nSelect New … > Create a local repository (or) Drag and drop the folder on to the browser\n\n\n\n\n\nCreating a new local repository in learning-git\n\n\n\n\nYou can open the repository in Sourcetree, but it’s not interesting yet\nIn learning-git, create a text file, eg, testing.txt\nSourcetree’s File Status panel shows the new file, ready to be tracked"
  },
  {
    "objectID": "content/02-02-git.html#tracking-changes",
    "href": "content/02-02-git.html#tracking-changes",
    "title": "4  Git for version control",
    "section": "4.5 Tracking changes",
    "text": "4.5 Tracking changes\n\nTracking changes to a file involves two steps: Adding and committing\nAdd: Click the checkmark\n\nThis tells git that we want to store these changes to the file in its archive\n\nCommit: Type a message in the comment field and click Commit\n\nThis tells git to go ahead and do the archiving process\n\n\n\n\n\nMaking your initial commit\n\n\n\n\nThe commit is now displayed in the History panel\n\n\n\n\nThe History panel after our first commit\n\n\nMake a few more changes to the file. Practice adding and committing them and note how the changes accumulate in the History panel."
  },
  {
    "objectID": "content/02-02-git.html#working-with-your-own-repos",
    "href": "content/02-02-git.html#working-with-your-own-repos",
    "title": "4  Git for version control",
    "section": "4.10 Working with your own repos",
    "text": "4.10 Working with your own repos\n\nOn GitHub, click “New” and walk through the steps to create a new repository\n\nThe name on GitHub doesn’t need to match the local name\nBut why wouldn’t you use the same name?\n\nCopy the URL: https://github.com/username/learning-git\nBack in Sourcetree\n\nGo to Settings in the toolbar\nGo to the Remotes tab\nClick Add to add a new remote\n\nThe standard name for a default remote is origin\n\nPaste in the GitHub URL\n\n\n\n\n\nAdding a new remote\n\n\n\n\nAfter adding the remote, we can send the repository (including its full history) up to GitHub using push\n\nLook for Push in the toolbar\nSourcetree will ask which branches to push\n\nWhen the push is complete, refresh GitHub in your browser\n\nYou can edit files directly in GitHub\n\nNow we’ll fetch and pull the changes from GitHub back to our local copy\n\nFetch: Compare the local tree to the remote tree, noting any differences. Use this to preview the changes on the remote\nIn Sourcetree, hit Fetch in the toolbar. Note that the GitHub commit shows up in the History panel.\nPull: Actually download the remote changes and incorporate them into your local tree."
  },
  {
    "objectID": "content/02-02-git.html#lab-working-with-someone-elses-repos",
    "href": "content/02-02-git.html#lab-working-with-someone-elses-repos",
    "title": "4  Git for version control",
    "section": "4.11 Lab: Working with someone else’s repos",
    "text": "4.11 Lab: Working with someone else’s repos\n\nGitHub lets you download someone else’s repo (clone), and modify it locally, but not upload directly.\n\nYou can suggest a change to someone else’s code by submitting a pull request, which first requires forking the repository.\n\n\n\n\nForking copies a repository to your GitHub account. Then you clone the copy to your local machine. You can push to your remote copy as usual. You can suggest changes to the original using a pull request. Source: https://happygitwithr.com/fork-and-clone.html\n\n\n\n\nStart with the repo for this week’s lab: https://github.com/data-science-methods/lab-w02-git\nFork: Look for the fork button in the upper-right\n\n\n\n\nThe fork button is near the upper-right corner of a GitHub repository page. I wasn’t able to find a keyboard shortcut for this. :-(\n\n\n\n\nClone: After creating the fork, you need to download a copy to your machine.\n\nIn Sourcetree’s repository browser, select New… > Clone from URL\n\nThis lesson continues in the lab. Open lab.html and lab.R to continue."
  }
]