[
  {
    "objectID": "content/01-01-intro.html#land-acknowledgements",
    "href": "content/01-01-intro.html#land-acknowledgements",
    "title": "Introduction",
    "section": "Land acknowledgements",
    "text": "Land acknowledgements\n\nCampus land acknowledgment\nWe pause to acknowledge all local indigenous peoples, including the Yokuts and Miwuk, who inhabited this land. We embrace their continued connection to this region and thank them for allowing us to live, work, learn, and collaborate on their traditional homeland. Let us now take a moment of silence to pay respect to their elders and to all Yokuts and Miwuk people, past and present.\n\n\nInstructor’s land acknowledgment\nUC Merced and the City of Merced are on the traditional territory of the Yokut people. This land was stolen by Spanish, Mexican, and American settlers through acts of slavery and genocide. In addition, UC Merced is strongly associated with Ahwahne, known as Yosemite Valley. This valley was the traditional home of the Ahwahnechee people, who were the victims of some especially horrific, state-sponsored genocidal acts. For more on the history of Ahwahne, see https://tinyurl.com/y879jw8s. For more information on land acknowledgments, see https://native-land.ca."
  },
  {
    "objectID": "content/01-01-intro.html#about-the-instructor",
    "href": "content/01-01-intro.html#about-the-instructor",
    "title": "Introduction",
    "section": "About the instructor",
    "text": "About the instructor\nDan Hicks is a philosopher turned data scientist turned philosopher.\nI use they/them pronouns and identify as nonbinary. I grew up in Placerville, about two hours north of Merced in the Sierra Foothills. One branch of my family came to California during the Gold Rush, so I identify heavily as a Californian and have some complicated feelings about the genocide. I finished my PhD in philosophy of science at Notre Dame in 2012. After that I worked in a series of research positions in academia and the federal government. During 2015-2019 I was using data science methods at least half-time. I joined the faculty at UC Merced in Fall 2019.\n\nEmail: dhicks4@ucmerced.edu\nStudent hours: By appointment: https://doodle.com/mm/danhicks/office-hours\nWebsite: https://dhicks.github.io/"
  },
  {
    "objectID": "content/01-01-intro.html#what-this-course-isnt-and-is",
    "href": "content/01-01-intro.html#what-this-course-isnt-and-is",
    "title": "Introduction",
    "section": "What this course isn’t, and is",
    "text": "What this course isn’t, and is\nIs Not:\n\na statistics course (in the way you think)\na general introduction to software engineering\na basic introduction to R\n\nIs:\n\nan introduction to data science\nabout exploratory data analysis, data management, and reproducibility\nhabituation to some good software engineering practices that are especially valuable for data science work"
  },
  {
    "objectID": "content/01-01-intro.html#learning-outcomes",
    "href": "content/01-01-intro.html#learning-outcomes",
    "title": "Introduction",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of the course, students will be able to\n\napply concepts from software engineering and philosophy of science to methodological decisions in data science, such as\n\nobject-oriented and functional programming paradigms\nexploratory data analysis and the data-phenomenon-theory distinction\nreproducibility vs. replicability\n\nuse exploratory data analysis techniques and tools to identify potential data errors and potential phenomena for further analysis\nclean raw data and produce a reproducible codebook for both downstream analysis and public release of data according to FAIR standards (Wilkinson et al. 2016)\nmanage data, analysis, and outputs for reproducibility, using practices such as\n\nreproducible data cleaning\nclear directory structure (Wilson et al. 2017)\nself-documenting code\nversion control\nbuild automation"
  },
  {
    "objectID": "content/01-01-intro.html#prerequisites",
    "href": "content/01-01-intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course assumes basic competence with introductory R.\n\n“Introductory R”\n\nLessons 1-5 of the Carpentries “R for Social Scientists” curriculum - Installing R and packages - Working in the R Studio IDE - Common data types - Reading and writing CSV files - Tidyverse R: mutate(), filter(), select(); plotting with ggplot2\n\n“Basic competence”\n\nGiven time and a reference (cheatsheet, Stack Exchange, mentor) you can figure out how to solve a problem"
  },
  {
    "objectID": "content/01-01-intro.html#requirements-and-weekly-routine",
    "href": "content/01-01-intro.html#requirements-and-weekly-routine",
    "title": "Introduction",
    "section": "Requirements and weekly routine",
    "text": "Requirements and weekly routine\nThis is a very rough approximation\n\nTuesdays: Mix of discussion and lecture based on assigned readings\nThursdays: Live coding leading into your own work\nLabs: 6, done in pairs and submitted via GitHub for automated feedback\nRunning project: Practicing ideas from the course on a data set of your choice"
  },
  {
    "objectID": "content/01-01-intro.html#course-materials",
    "href": "content/01-01-intro.html#course-materials",
    "title": "Introduction",
    "section": "Course materials",
    "text": "Course materials\n\nCourse website: https://data-science-methods.github.io/\nAll readings are linked on the schedule\nLecture notes takes you to a pretty version of my slides and notes for class"
  },
  {
    "objectID": "content/01-01-intro.html#references",
    "href": "content/01-01-intro.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References\n\n\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1, 1): 1–9. https://doi.org/10.1038/sdata.2016.18.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "content/01-02-setup.html#checklist",
    "href": "content/01-02-setup.html#checklist",
    "title": "Software, hardware, and accounts",
    "section": "Checklist",
    "text": "Checklist\n\nR 4.1 or higher\nRStudio\nPackage manager for your OS\ngit\npandoc\nGitHub account\nSourcetree"
  },
  {
    "objectID": "content/01-02-setup.html#hardware",
    "href": "content/01-02-setup.html#hardware",
    "title": "Software, hardware, and accounts",
    "section": "Hardware",
    "text": "Hardware\n\nMost sessions will involve writing code in class\nI strongly recommend against tablets (iPad, Surface) for this course\nI recommend that your machine have at least\n\n200 GB free hard drive space\n8 GB RAM\nIntel i5 processor\nOperating system capable of running R 4.1 or higher\n\nMac: 10.13 (High Sierra) or more recent\nWindows: See here\nIf you’re running Linux I’m going to assume you can figure this out yourself"
  },
  {
    "objectID": "content/01-02-setup.html#accessibility",
    "href": "content/01-02-setup.html#accessibility",
    "title": "Software, hardware, and accounts",
    "section": "Accessibility",
    "text": "Accessibility\nI chose the tools and platforms for this course in part because they’re industry-standard. If you pursue a career as a data scientist in industry, you’ll be expected to use GitHub (or something similar) on a daily basis. And RStudio is by far the most commonly used IDE (“integrated development environment”) for R.\nHowever, like many other technologies, they were originally developed using ableist assumptions about “normal” computer users. In response to criticism, the developers of these systems and tools have gone back and made their technologies more accessible. But there may still be barriers to accessibility that I have not anticipated.\nIf you encounter a barrier to participating in this course — even a small inconvenience — please let me know. Similarly, if you have ideas for making the course more accessible, please share them with me."
  },
  {
    "objectID": "content/01-02-setup.html#r-and-rstudio",
    "href": "content/01-02-setup.html#r-and-rstudio",
    "title": "Software, hardware, and accounts",
    "section": "R and RStudio",
    "text": "R and RStudio\nR and RStudio\n\n\nIn 2020 R had a major release (version 4)\nAddressing some bugs surrounding version 4 has led to rapid development in R and RStudio\nFor consistency in class, I encourage you to use the latest version of both R and RStudio\nHelp on switching between different versions of R"
  },
  {
    "objectID": "content/01-02-setup.html#compilers",
    "href": "content/01-02-setup.html#compilers",
    "title": "Software, hardware, and accounts",
    "section": "Compilers",
    "text": "Compilers\nSometimes you’ll need to compile a package in order to install it.\n\nMac\n\nIn a terminal, simply enter\n\nsudo xcode-select --install\nYou’ll need to enter an administrator password\n\nIf you also need a Fortran compiler\n(personally I’ve never needed one)\n\nWindows\n\nRtools42 (R 4.2 and higher) or\nRtools4 (older versions of R)\n\nLinux\n\nI’m pretty sure Linux and the like come with the necessary compilers"
  },
  {
    "objectID": "content/01-02-setup.html#os",
    "href": "content/01-02-setup.html#os",
    "title": "Software, hardware, and accounts",
    "section": "OS",
    "text": "OS\n\nAll of the tools we’ll use are available on all major OSes\n\nExcept Sourcetree, which isn’t essential\nYou can find an alternative git GUI here\n\nI have much less experience debugging Windows and Linux, but will do what I can"
  },
  {
    "objectID": "content/01-02-setup.html#package-manager",
    "href": "content/01-02-setup.html#package-manager",
    "title": "Software, hardware, and accounts",
    "section": "Package manager",
    "text": "Package manager\n\nSoftware to facilitate installing, upgrading, and removing other software\nHighly recommended for installing git, pandoc, and some other tools we’ll use later\nMac OS: Homebrew or MacPorts\n\nPros and cons of these two main options\nI use Homebrew\n\nWindows: Chocolatey\n\nI guess there are others, but this is what previous students have recommended\n\nEither: Anaconda is popular among Python folks, and works as a package manager for multiple OSes\n\nBut sometimes packages aren’t up to date\nEx: Most recent version of R appears to be 3.6.0, released April 2019\n\n\n\nOnce your package manager is ready to go, install git and pandoc\n\nMac, Homebrew:\nbrew install git\nbrew install pandoc\nWindows, Chocolatey:\n\nNeeds to be in elevated/administrator mode\nIf prompted, Yes to running additional scripts\n\nchoco install git\nchoco install pandoc"
  },
  {
    "objectID": "content/01-02-setup.html#latex",
    "href": "content/01-02-setup.html#latex",
    "title": "Software, hardware, and accounts",
    "section": "LaTeX",
    "text": "LaTeX\n\nWe don’t use it directly, but it turns Rmarkdown into PDFs\nIf you have no idea what that meant, don’t worry about it for now\nI recommend the TinyTeX distribution, which you can install via R:\ninstall.packages('tinytex')\ntinytex::install_tinytex()"
  },
  {
    "objectID": "content/01-02-setup.html#github-and-sourcetree",
    "href": "content/01-02-setup.html#github-and-sourcetree",
    "title": "Software, hardware, and accounts",
    "section": "GitHub and Sourcetree",
    "text": "GitHub and Sourcetree\n\nYou’ll need a GitHub account and Sourcetree\n\n\n\nGitHub is owned by Microsoft\nAnd there are concerns about\n\nhow it mined public repositories to develop a code completion tool, and\na recent change to its privacy policy for Enterprise (business) customers\n\nBut it offers some key features that I want to use in this course\nYou can do everything this course requires in private repositories, so long as you give access to the people who need access"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#reading",
    "href": "content/01-03-data-sci-wtf.html#reading",
    "title": "Data Science: What and why",
    "section": "Reading",
    "text": "Reading\n\nWilson et al. (2017)\nMcElreath (2020)"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#discussion-question",
    "href": "content/01-03-data-sci-wtf.html#discussion-question",
    "title": "Data Science: What and why",
    "section": "Discussion question",
    "text": "Discussion question\nWhy did you decide to take a class called “data science”?"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#standard-definition",
    "href": "content/01-03-data-sci-wtf.html#standard-definition",
    "title": "Data Science: What and why",
    "section": "Standard definition",
    "text": "Standard definition\n\n\n\nData science, defined as the intersection of CS, stats, and “business knowledge.” Source: https://www.kdnuggets.com/2020/08/top-10-lists-data-science.html\n\n\n\nThe intersection of computer science/software engineering, statistics, and “business knowledge”\nBut this defines data science in terms of tools and techniques, not epistemic and practical goals. Compare:\n\nAn ecologist is someone who spends most of their time collecting specimens in the field and processing them in a lab, vs. \nAn ecologist is someone who studies interactions among organisms and their environment"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#discussion-questions",
    "href": "content/01-03-data-sci-wtf.html#discussion-questions",
    "title": "Data Science: What and why",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nWhat are the epistemic and practical goals of your scientific field?\nHow do you think “data science” will be useful for pursuing those goals?"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#references",
    "href": "content/01-03-data-sci-wtf.html#references",
    "title": "Data Science: What and why",
    "section": "References",
    "text": "References\n\n\n\n\nMcElreath, Richard, dir. 2020. Science as Amateur Software Development. https://www.youtube.com/watch?v=zwRdO9_GGhY.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "content/02-02-git.html#some-motivating-examples",
    "href": "content/02-02-git.html#some-motivating-examples",
    "title": "Git for version control",
    "section": "Some motivating examples",
    "text": "Some motivating examples\n\n\nWhile working on your analysis code, you accidentally erase the first 35 lines of the script. You only discover this three days later, when you restart R and try to run the script from the top.\nThe code you wrote last week is suddenly giving you different results. A bunch of variables are missing, and the standard errors are huge. When you’re talking with your collaborator, he says that he was cleaning the data a few days ago and everything seemed fine. When you go to check, you discover that he’s replaced your 827 survey responses with 6 group summaries.\n\n\n\n\n\n\n\nPiled Higher and Deeper comic on file names. Source: https://phdcomics.com/comics.php?f=1531\n\n\n\nYou’re working on a paper with two coauthors. You prepare the final draft to send for submission: paper final.docx. But one of your coauthors discovers a typo. Now it’s paper final fixed typo.docx. Another realizes six references are missing. paper final fixed typo refs.docx. That’s getting confusing so you change it to paper 2 Aug 2021.docx. Once it comes back from review you need to make revisions. Now you have paper 30 Jan 2022.docx and, after your collaborators make their changes, paper 12 February 2022 DJH.docx and paper 12 February 20222 final.docx."
  },
  {
    "objectID": "content/02-02-git.html#version-control",
    "href": "content/02-02-git.html#version-control",
    "title": "Git for version control",
    "section": "Version control",
    "text": "Version control\n\nBasic idea: Tools for tracking and reversing changes to code over time\nUseful for identifying and reversing breaking changes\nImplementations upload to cloud, track who contributes code, control who can suggest vs. actually change code\nGood for collaboration, publishing code"
  },
  {
    "objectID": "content/02-02-git.html#git",
    "href": "content/02-02-git.html#git",
    "title": "Git for version control",
    "section": "Git",
    "text": "Git\n\nOne of many version control systems\nVery popular in part thanks to GitHub, which provides free hosting git repositories\n\nResources for students (and teachers): https://education.github.com/"
  },
  {
    "objectID": "content/02-02-git.html#gitting-started",
    "href": "content/02-02-git.html#gitting-started",
    "title": "Git for version control",
    "section": "Gitting started",
    "text": "Gitting started\n\ngit is very hard\nWe’re going to use the Sourcetree GUI to ease into things"
  },
  {
    "objectID": "content/02-02-git.html#initial-commit",
    "href": "content/02-02-git.html#initial-commit",
    "title": "Git for version control",
    "section": "Initial commit",
    "text": "Initial commit\n\n\n\n\n\nCreating a new local repository in learning-git\n\n\n\n\nInstall Sourcetree and go through the configuration steps\nThen you’ll see the (empty) repository browser\nCreate a folder called something like learning-git\nThen, in Sourcetree:\n\nOn the Local tab\nSelect New … > Create a local repository (or) Drag and drop the folder on to the browser\n\n\n\n\n\n\nYou can open the repository in Sourcetree, but it’s not interesting yet\nIn learning-git, create a text file, eg, testing.txt\nSourcetree’s File Status panel shows the new file, ready to be tracked"
  },
  {
    "objectID": "content/02-02-git.html#tracking-changes",
    "href": "content/02-02-git.html#tracking-changes",
    "title": "Git for version control",
    "section": "Tracking changes",
    "text": "Tracking changes\n\n\n\nMaking your initial commit\n\n\n\nTracking changes to a file involves two steps: Adding and committing\nAdd: Click the checkmark\n\nThis tells git that we want to store these changes to the file in its archive\n\nCommit: Type a message in the comment field and click Commit\n\nThis tells git to go ahead and do the archiving process\n\n\n\n\nThe commit is now displayed in the History panel\n\n\n\n\nThe History panel after our first commit\n\n\nMake a few more changes to the file. Practice adding and committing them and note how the changes accumulate in the History panel."
  },
  {
    "objectID": "content/02-02-git.html#time-travel",
    "href": "content/02-02-git.html#time-travel",
    "title": "Git for version control",
    "section": "Time travel",
    "text": "Time travel\n\nWe can checkout previous commits to work with old versions of our files\nIn the example, suppose I made a commit with a mistake (my code stopped working or whatever)\nIn the History panel, right-click on a previous commit and select Checkout…\n\n\n\n\nChecking out an old commit to travel through time\n\n\n\n\n\n\nTrying (and failing) to change the past. My current HEAD commit will disappear as soon as I check out main.\n\n\n\nSourcetree warns us that we’ll be in an undetached head state\nTo see what this means, try making a change to the file, adding and committing it, then checking out the commit with the main or master tag"
  },
  {
    "objectID": "content/02-02-git.html#the-garden-of-forking-branches",
    "href": "content/02-02-git.html#the-garden-of-forking-branches",
    "title": "Git for version control",
    "section": "The garden of forking branches",
    "text": "The garden of forking branches\n\nTo actually change the past, we’ll use a branch\n\nBranches allow git to track multiple distinct “timelines” for files\nFor example, most major software projects will have separate “dev” (development) and “release” branches\nIndividual branches will also be created for work on specific areas of the project\nThis allows each area of active work to be isolated from work happening in other areas\n\n\n\n\n\n\n\n\nMerging fixing-mistake into main\n\n\n\n\nAfter checking out the previous commit, click on Branch in the toolbar\n\nName your new branch something like fixing-mistake (no spaces!)\n\nStart to work on fixing the mistake in the file, then add and commit as usual\nNow checkout main. Notice:\n\nYour commits on fixing-mistake don’t disappear\nThe state of your file changes to the main version\nThe History panel shows the split between the two branches\n\nAfter we’ve finished fixing the mistake, we want to merge these changes back into main\n\nMake sure you’re current on main\nRight-click on fixing-mistake and select Merge…\n\n\n\n\n\n\nSourcetree will bring up a message about Merge Conflicts\n\nThis just means that the files you’re combining have conflicting histories, and git wants you to sort out what to keep and what to throw away\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt’s not obvious (there isn’t a big red status symbol anywhere), but git is now in a special conflict-resolution state.\nUntil you resolve the conflicts and finish the merge, a lot of standard git functionality either won’t work at all or will cause weird problems.\nIf git starts giving you a bunch of weird errors, check to see if you’re in the middle of a merge and need to resolve conflicts.\n\n\n\n\n\n\nSourcetree’s File status panel indicates which files have conflicts\n\n\nAfter starting the merge, Sourcetree’s File status panel will indicate exactly which files have conflicts.\n\nYour file will look something like this\ntest\nanother line\n<<<<<<< HEAD\nthis line has a mistake\n=======\nno mistake this time\n>>>>>>> fixing-mistake\n\nThe <<<<<<< and >>>>>>> surround each area of conflict.\n\nThe top part (marked HEAD) shows the state of the current branch\nThe bottom part (marked fixing-mistake) shows the state of the branch you’re merging\n\nSimply edit these sections so they’re in the state you want,\n\nThen save, go back to Sourcetree’s File status panel\nSourcetree generates a commit message indicating that you’re resolving conflicts to complete a merge\n\n\n\nAfterwards the History panel shows the two branches merging back together\n\n\n\nThe History panel shows the branches merging back together"
  },
  {
    "objectID": "content/02-02-git.html#authenticating-sourcetree-with-github",
    "href": "content/02-02-git.html#authenticating-sourcetree-with-github",
    "title": "Git for version control",
    "section": "Authenticating Sourcetree with GitHub",
    "text": "Authenticating Sourcetree with GitHub\n\nMake sure you’re logged in to GitHub in your default web browser\nIn Sourcetree\n\nGo to a New Tab\nThen Remote (just underneath the tabs)\nAdd an account …\nHosting Service: GitHub\nAuthentication: OAuth\nUsername: [shouldn’t be able to enter anything]\nClick “Refresh OAuth Token”\n\nFocus should switch to a new tab in your web browser\n\nScroll down and click “Authorize atlassian” green button\n\nBrowser should now say “Authentication Successful” and GitHub should send you a security notice via email\nBack in Sourcetree, click okay"
  },
  {
    "objectID": "content/02-02-git.html#working-with-github-remotes",
    "href": "content/02-02-git.html#working-with-github-remotes",
    "title": "Git for version control",
    "section": "Working with GitHub remotes",
    "text": "Working with GitHub remotes\n\n\n\nThe remote origin lives on GitHub. Source: https://happygitwithr.com/common-remote-setups.html#ours-you\n\n\nA remote is a copy of a repository that lives on a server somewhere else"
  },
  {
    "objectID": "content/02-02-git.html#working-with-your-own-repos",
    "href": "content/02-02-git.html#working-with-your-own-repos",
    "title": "Git for version control",
    "section": "Working with your own repos",
    "text": "Working with your own repos\n\nOn GitHub, click “New” and walk through the steps to create a new repository\n\nThe name on GitHub doesn’t need to match the local name\nBut why wouldn’t you use the same name?\n\nCopy the URL: https://github.com/[username]/learning-git\n\n\n\n\nBack in Sourcetree\n\nGo to Settings in the toolbar\nGo to the Remotes tab\nClick Add to add a new remote\n\nThe standard name for a default remote is origin\n\nPaste in the GitHub URL\n\n\n\n\n\n\nAdding a new remote\n\n\n\n\n\n\nAfter adding the remote, we can send the repository (including its full history) up to GitHub using push\n\nLook for Push in the toolbar\nSourcetree will ask which branches to push\n\nWhen the push is complete, refresh GitHub in your browser\n\nYou can edit files directly in GitHub\n\nNow we’ll fetch and pull the changes from GitHub back to our local copy\n\nFetch: Compare the local tree to the remote tree, noting any differences. Use this to preview the changes on the remote\nIn Sourcetree, hit Fetch in the toolbar. Note that the GitHub commit shows up in the History panel.\nPull: Actually download the remote changes and incorporate them into your local tree."
  },
  {
    "objectID": "content/02-02-git.html#lab-working-with-someone-elses-repos",
    "href": "content/02-02-git.html#lab-working-with-someone-elses-repos",
    "title": "Git for version control",
    "section": "Lab: Working with someone else’s repos",
    "text": "Lab: Working with someone else’s repos\n\nGitHub lets you download someone else’s repo (clone), and modify it locally, but not upload directly.\n\nYou can suggest a change to someone else’s code by submitting a pull request, which first requires forking the repository.\n\n\n\n\nForking copies a repository to your GitHub account. Then you clone the copy to your local machine. You can push to your remote copy as usual. You can suggest changes to the original using a pull request. Source: https://happygitwithr.com/fork-and-clone.html\n\n\n\n\nStart with the repo for this week’s lab: https://github.com/data-science-methods/lab-w02-git\nFork: Look for the fork button in the upper-right\n\n\n\n\nThe fork button is near the upper-right corner of a GitHub repository page. I wasn’t able to find a keyboard shortcut for this. :-(\n\n\n\n\nClone: After creating the fork, you need to download a copy to your machine.\n\nIn Sourcetree’s repository browser, select New… > Clone from URL\n\nThis lesson continues in the lab. Open lab.html and lab.Rmd to continue."
  },
  {
    "objectID": "content/02-03-getting-help.html#reading",
    "href": "content/02-03-getting-help.html#reading",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Reading",
    "text": "Reading\n\nBryan (2020)\n“R Faq - How to Make a Great R Reproducible Example” (n.d.)\nWickham (2019)"
  },
  {
    "objectID": "content/02-03-getting-help.html#dependencies",
    "href": "content/02-03-getting-help.html#dependencies",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Dependencies",
    "text": "Dependencies\n\ninstall.packages('lubridate', 'assertthat', 'reprex')"
  },
  {
    "objectID": "content/02-03-getting-help.html#messages-warnings-and-errors",
    "href": "content/02-03-getting-help.html#messages-warnings-and-errors",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Messages, warnings, and errors",
    "text": "Messages, warnings, and errors\n\nMessage: Things are fine, but here’s some information you should know\nWarning: Uhhhh I’m gonna keep going, but maybe this isn’t what you want\nError: Nope. I’m stopping here. You need to fix the thing.\n\n\nmessage('Hey, just FYI')\n\nHey, just FYI\n\nwarning('Uhhhh might want to check this out')\n\nWarning: Uhhhh might want to check this out\n\nstop('Noooooo')\n\nError in eval(expr, envir, enclos): Noooooo"
  },
  {
    "objectID": "content/02-03-getting-help.html#where-to-go-for-help",
    "href": "content/02-03-getting-help.html#where-to-go-for-help",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Where to go for help",
    "text": "Where to go for help\n\nRubber duck debugging\nIsolate the problem\nRestart your session: Session \\(\\to\\) Restart R\nLocal help: ?fun\nStackOverflow: https://stackoverflow.com/questions/tagged/r\nCRAN \\(\\to\\) BugReports (usually GitHub Issues)"
  },
  {
    "objectID": "content/02-03-getting-help.html#example-dates-are-often-problems",
    "href": "content/02-03-getting-help.html#example-dates-are-often-problems",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Example: Dates are often problems",
    "text": "Example: Dates are often problems\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nadd_six_months = function(date_str) {\n    parsed_dates = parse_date_time(date_str, orders = 'mdY')\n    parsed_dates %m+% months(6) \n}\n\nsome_data = c('June 2002', 'May 15, 2007', 'August 2007')\nadd_six_months(some_data)\n\nWarning: 2 failed to parse.\n\n\n[1] NA               \"2007-11-15 UTC\" NA              \n\n\n\n\nNote that this is a warning, not an error\n\nR won’t stop running here (unless we tell it to)\nErrors might not show up until much later in our code, making it hard to identify the root cause\nOr they might cause invisible problems, eg, by default lm() silently drops observations with missing values"
  },
  {
    "objectID": "content/02-03-getting-help.html#to-catch-warnings",
    "href": "content/02-03-getting-help.html#to-catch-warnings",
    "title": "Warnings, Errors, and Getting Help",
    "section": "To catch warnings",
    "text": "To catch warnings\n\nSet options(warn = 2) to turn all warnings into errors\nUse tryCatch() with the warning argument\n\nExample: https://stackoverflow.com/questions/8217901/breaking-loop-when-warnings-appear-in-r/8218794#8218794\n\nWrite a unit test\nMy preferred approach: Add an assertion to your primary code\n\n\n## Using an assertion to prevent warnings from cascading\nlibrary(assertthat)\n\nsix_months_later = add_six_months(some_data)\n\nWarning: 2 failed to parse.\n\nassert_that(all(!is.na(six_months_later)), \n            msg = 'Missing values in `six_months_later`')\n\nError: Missing values in `six_months_later`"
  },
  {
    "objectID": "content/02-03-getting-help.html#debugging",
    "href": "content/02-03-getting-help.html#debugging",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Debugging",
    "text": "Debugging\n\nLet’s start by using the RStudio debugger to isolate the problem\n\n\ndebugonce(add_six_months)\nadd_six_months(some_data)\n\ndebugging in: add_six_months(some_data)\ndebug at <text>#3: {\n    parsed_dates = parse_date_time(date_str, orders = \"mdY\")\n    parsed_dates %m+% months(6)\n}\ndebug at <text>#4: parsed_dates = parse_date_time(date_str, orders = \"mdY\")\n\n\nWarning: 2 failed to parse.\n\n\ndebug at <text>#5: parsed_dates %m+% months(6)\nexiting from: add_six_months(some_data)\n\n\n[1] NA               \"2007-11-15 UTC\" NA              \n\n\n\nThe problem is in lubridate::parse_date_time().\n\nSpend a few minutes reading the documentation for this function and playing around with the call.\n\nWhat does the argument orders do?\n\n\n\n?parse_date_time\n\n\nparse_date_time(some_data, orders = 'mdY')\n\nWarning: 2 failed to parse.\n\n\n[1] NA               \"2007-05-15 UTC\" NA              \n\n\n\n\nLet’s try SO: https://stackoverflow.com/search?q=%5BR%5D+lubridate+month-year\n\n\nparse_date_time(some_data, orders = c('mY', 'mdY'))\n\n[1] \"2002-06-01 UTC\" \"2007-05-15 UTC\" \"2007-08-01 UTC\"\n\n\n\nMake this change in add_six_months() and confirm it no longer trips the assertion."
  },
  {
    "objectID": "content/02-03-getting-help.html#another-example-more-fun-with-dates",
    "href": "content/02-03-getting-help.html#another-example-more-fun-with-dates",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Another example: More fun with dates",
    "text": "Another example: More fun with dates\n\nmore_data = c('May 7, 2017', 'May 19, 2017', 'May Fifth, 2017')\nmdy(more_data)\n\n[1] \"2017-05-07\" \"2017-05-19\" \"2017-05-20\"\n\n\n\nSO doesn’t seem so helpful: https://stackoverflow.com/search?q=%5BR%5D+lubridate+written+days\n\n\n\nThe CRAN page for lubridate includes a link to report bugs: https://cran.r-project.org/web/packages/lubridate/index.html\n\n\n\n\nScreenshot of lubridate on CRAN, highlighting the BugReports field\n\n\n\n\nTrying a couple of searches gives us a promising result: https://github.com/tidyverse/lubridate/issues?q=is%3Aissue+is%3Aopen+mdy\n\n\n\n\nScreenshot of lubridate issues page, showing a relevant search result (August 2021)\n\n\n\nThis is a known bug; it looks like they’re thinking about doing something about it, but the only workaround is to create an NA: https://github.com/tidyverse/lubridate/issues/685"
  },
  {
    "objectID": "content/02-03-getting-help.html#writing-a-reproducible-example-reprex",
    "href": "content/02-03-getting-help.html#writing-a-reproducible-example-reprex",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Writing a reproducible example: reprex",
    "text": "Writing a reproducible example: reprex\n\nhttps://reprex.tidyverse.org/\nhttps://reprex.tidyverse.org/articles/articles/learn-reprex.html\nhttps://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\nPractice by writing a reprex for one of our two examples"
  },
  {
    "objectID": "content/02-03-getting-help.html#do-not-do-these-things-in-your-reprex-or-anywhere-else",
    "href": "content/02-03-getting-help.html#do-not-do-these-things-in-your-reprex-or-anywhere-else",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Do not do these things in your reprex (or anywhere else)",
    "text": "Do not do these things in your reprex (or anywhere else)\nOr Jenny Bryan will come to your office and set your computer on fire.\n\nsetwd('/users/danhicks/projects/catsaregreat/myscript/')\n\nUsed to ensure that R is running where your file is\nUnnecessary if you’re opening different projects in different RStudio sessions\nWill cause irrelevant errors on any other system\nInstead, use file.path() or here::here() to build paths\n\nrm(list=ls())\n\nUsed because people think it clears out the global environment\nUnnecessary if you’re regularly using Session \\(\\to\\) Restart R\nAlso unnecessary at the top of a Rmd file, which is always knit in a new session\nDoesn’t actually clear out the global environment\n\neg, doesn’t unload packages or reset options()\n\n\n\nNot on Bryan’s list, but also don’t do it:\n\nrequire(package)\n\nIf package is installed, will act just like library()\nIf not, will return FALSE\n\nThe script will keep going until there’s an error about a missing function 300 lines later\nProbably not the error you wanted help with\nAnnoying to debug because I have no idea where the function is supposed to come from\n\nIf library() can’t find the package, it immediately raises an error\n\nI can tell right away what package needs to be installed"
  },
  {
    "objectID": "content/02-03-getting-help.html#debugging-in-rstudio",
    "href": "content/02-03-getting-help.html#debugging-in-rstudio",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Debugging in RStudio",
    "text": "Debugging in RStudio\nThis week’s lab introduces you to some of RStudio’s debugging tools.\n\n\n\n\nBryan, Jenny. 2020. “Object of Type ‘Closure’ Is Not Subsettable.” Presented at the RSTUDIO::CONF 2020, January 31. https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/.\n\n\n“R Faq - How to Make a Great R Reproducible Example.” n.d. Stack Overflow. Accessed August 31, 2018. https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example.\n\n\nWickham, Hadley. 2019. “Debugging.” In Advanced R, Second Edition, ch. 22. CRC Press. https://adv-r.hadley.nz/debugging.html."
  },
  {
    "objectID": "content/02-04-paradigms.html#reading",
    "href": "content/02-04-paradigms.html#reading",
    "title": "Programming Paradigms",
    "section": "Reading",
    "text": "Reading\n\nIntroductions to parts II and III of Wickham (2014)\nChambers (2014)"
  },
  {
    "objectID": "content/02-04-paradigms.html#dependencies",
    "href": "content/02-04-paradigms.html#dependencies",
    "title": "Programming Paradigms",
    "section": "Dependencies",
    "text": "Dependencies\n\ninstall.packages('sloop')"
  },
  {
    "objectID": "content/02-04-paradigms.html#programming-paradigms",
    "href": "content/02-04-paradigms.html#programming-paradigms",
    "title": "Programming Paradigms",
    "section": "Programming paradigms",
    "text": "Programming paradigms\n\nProcedural or imperative\n\nSoftware is a series of instructions (“procedures”), which the computer carries out in order. Special instructions (if-then, loops) are used to change the order based on inputs or other conditions.\n- Examples: FORTRAN, BASIC, C, a calculator\n\nObject-oriented\n\nSoftware is made up of objects, which have properties (“attributes,” including other objects) and do things (“methods”).\n- Examples: Python, Java\n\nFunctional\n\nSoftware is made up of functions, which are run sequentially on the inputs.\n- Examples: Lisp, Haskell"
  },
  {
    "objectID": "content/02-04-paradigms.html#r-is-both-object-oriented-and-functional",
    "href": "content/02-04-paradigms.html#r-is-both-object-oriented-and-functional",
    "title": "Programming Paradigms",
    "section": "R is both object-oriented and functional",
    "text": "R is both object-oriented and functional\n\nObject-oriented: Everything that exists is an object\nFunctional: Everything that happens is a function call"
  },
  {
    "objectID": "content/02-04-paradigms.html#object-oriented-programming",
    "href": "content/02-04-paradigms.html#object-oriented-programming",
    "title": "Programming Paradigms",
    "section": "Object-oriented programming",
    "text": "Object-oriented programming\n\nObjects have properties (aka fields, attributes) and functions (procedures, methods)\nObjects interact with each other using functions\nObjects change state (values of their properties) as the program runs\n\n\n\n\n\n\nclassDiagram\n   class Game{\n      players: Player\n      board: Board\n      deck: Deck\n      __init__()\n      startGame()\n      runTurn()\n      checkVictory()\n      endGame()\n   }\n   Game <.. Player\n   Game <.. Board\n   Game <.. Deck   \n   class Player{\n      name: String\n      points: Int\n      tokenLocation: Int\n      hand: Deck\n      playedCards: Deck\n      __init__()\n      drawCard()\n      playCard()\n      moveToken()\n      updatePoints()\n      takeTurn()\n   }\n   class Board{\n      locations: Location\n      __init__()\n      moveToken()\n   }\n   Board <.. Location\n   class Location{\n        name: String\n        landingEffect()\n   }\n   class Deck{\n      stack: Card\n      discard: Card\n      __init__()\n      shuffle()\n      drawCard()\n   }\n   Deck <.. Card\n   class Card{\n      pointValue: int\n      __init__()\n      drawEffect()\n      playEffect()\n      discardEffect()\n   }\n\n\n\n\n\nRepresenting a board game for object-oriented programming. Each class of objects has its own properties and functions, and instances of these classes will interact with each other as the program runs."
  },
  {
    "objectID": "content/02-04-paradigms.html#the-oop-youre-used-to",
    "href": "content/02-04-paradigms.html#the-oop-youre-used-to",
    "title": "Programming Paradigms",
    "section": "The OOP you’re used to",
    "text": "The OOP you’re used to\n\nClasses are defined by their elements and methods\nChanging/adding elements and methods requires changing the class definition\nFor \\(x\\) to be an \\(F\\), \\(x\\) must be created as an \\(F\\)\n\n\n\n## <https://vegibit.com/python-class-examples/>\nclass Vehicle:\n    def __init__(self, brand, model, type):\n        self.brand = brand\n        self.model = model\n        self.type = type\n        self.gas_tank_size = 14\n        self.fuel_level = 0\n    \n    def fuel_up(self):\n        self.fuel_level = self.gas_tank_size\n        print('Gas tank is now full.')\n    \n    def drive(self):\n        if self.fuel_level > 0:\n            print(f'The {self.model} is now driving.')\n            self.fuel_level -= 1\n        else:\n            print(f'The {self.model} is out of gas!')\n\ndhCar = Vehicle('Honda', 'Fit', 'Hatchback')\ndhCar.gas_tank_size = 10\ndhCar.fuel_up()\n\nGas tank is now full.\n\ndhCar.fuel_level\n\n10\n\ndhCar.drive()\n\nThe Fit is now driving.\n\ndhCar.fuel_level\n\n9"
  },
  {
    "objectID": "content/02-04-paradigms.html#s3-is-oop-but-weird",
    "href": "content/02-04-paradigms.html#s3-is-oop-but-weird",
    "title": "Programming Paradigms",
    "section": "S3 is OOP, but weird",
    "text": "S3 is OOP, but weird\n\nS3 classes can be changed on the fly, with no attempt to validate any assumptions.\n\n\ndh_car = list(brand = 'Honda', model = 'Fit', type = 'Hatchback')\nclass(dh_car)\n\n[1] \"list\"\n\nclass(dh_car) = c('vehicle', class(dh_car))\nclass(dh_car)\n\n[1] \"vehicle\" \"list\"   \n\ninherits(dh_car, 'vehicle')\n\n[1] TRUE\n\n\n\n\nnotamodel = list()\nnotamodel$residuals = 1:10\nclass(notamodel) = 'lm'\nresiduals(notamodel)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nprint(notamodel)\n\n\nCall:\nNULL\n\nNo coefficients\n\n\n\n\nmodel = lm(log(mpg) ~ log(disp), data = mtcars)\nclass(model)\n\n[1] \"lm\"\n\nprint(model)\n\n\nCall:\nlm(formula = log(mpg) ~ log(disp), data = mtcars)\n\nCoefficients:\n(Intercept)    log(disp)  \n     5.3810      -0.4586  \n\nclass(model) = 'Date'\nclass(model)\n\n[1] \"Date\"\n\ntry(print(model))\n\nError in as.POSIXlt.Date(x) : \n  'list' object cannot be coerced to type 'double'\n\n\n\n\nWickham discusses good practices to reduce this chaos in S3\n\nwrite constructor, validator, and helper functions\n\nS4 and R6 provide more conventional OOP structure"
  },
  {
    "objectID": "content/02-04-paradigms.html#models-as-objects",
    "href": "content/02-04-paradigms.html#models-as-objects",
    "title": "Programming Paradigms",
    "section": "Models as objects",
    "text": "Models as objects\n\n\n\n\nflowchart LR\n    subgraph RegressionModel\n       data \n       specification\n       \n       data & specification --> fit[[fit]] \n       fit --> coefficients\n       fit --> gof[R2]\n    \n       coefficients --> predict[[predict]]\n       coefficients & gof --> print[[print]]\n    end\n\n\n\n\n\nA regression model as an OOP object, showing how different attributes and methods work together.\n\n\n\n\n\nmodel = lm(mpg ~ disp, data = mtcars)\nstr(model, give.attr = FALSE)\n\nList of 12\n $ coefficients : Named num [1:2] 29.5999 -0.0412\n $ residuals    : Named num [1:32] -2.01 -2.01 -2.35 2.43 3.94 ...\n $ effects      : Named num [1:32] -113.65 -28.44 -1.79 2.65 3.92 ...\n $ rank         : int 2\n $ fitted.values: Named num [1:32] 23 23 25.1 19 14.8 ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:32, 1:2] -5.657 0.177 0.177 0.177 0.177 ...\n  ..$ qraux: num [1:2] 1.18 1.09\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n $ df.residual  : int 30\n $ xlevels      : Named list()\n $ call         : language lm(formula = mpg ~ disp, data = mtcars)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ disp\n $ model        :'data.frame':  32 obs. of  2 variables:\n  ..$ mpg : num [1:32] 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n  ..$ disp: num [1:32] 160 160 108 258 360 ..."
  },
  {
    "objectID": "content/02-04-paradigms.html#s3-generics",
    "href": "content/02-04-paradigms.html#s3-generics",
    "title": "Programming Paradigms",
    "section": "S3 generics",
    "text": "S3 generics\n\nIn S3, methods (class-specific functions) are managed using generic functions\n\n\nreg_model = lm(log(mpg) ~ log(disp), data = mtcars)\naov_model = aov(log(mpg) ~ log(disp), data = mtcars)\n\nclass(reg_model)\n\n[1] \"lm\"\n\nclass(aov_model)\n\n[1] \"aov\" \"lm\" \n\ninherits(aov_model, 'lm')\n\n[1] TRUE\n\nprint(reg_model)\n\n\nCall:\nlm(formula = log(mpg) ~ log(disp), data = mtcars)\n\nCoefficients:\n(Intercept)    log(disp)  \n     5.3810      -0.4586  \n\nprint(aov_model)\n\nCall:\n   aov(formula = log(mpg) ~ log(disp), data = mtcars)\n\nTerms:\n                log(disp) Residuals\nSum of Squares  2.2559626 0.4927736\nDeg. of Freedom         1        30\n\nResidual standard error: 0.1281631\nEstimated effects may be unbalanced\n\nresiduals(aov_model)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n       -0.009136238        -0.009136238        -0.107135015         0.228829658 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n        0.246731745        -0.001408610        -0.021532242         0.101128004 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n        0.014481527        -0.077467905        -0.153179727        -0.006685912 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n        0.046739255        -0.082671818        -0.215771303        -0.227580582 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n        0.098076936         0.099131472         0.017593403         0.097817784 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n       -0.117145641         0.002161632        -0.038029398        -0.106946015 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n        0.321433494        -0.070395535         0.073660971         0.122216133 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n        0.066608217        -0.118181538        -0.055822446        -0.118384067 \n\nresiduals(reg_model)\n\n          Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive \n       -0.009136238        -0.009136238        -0.107135015         0.228829658 \n  Hornet Sportabout             Valiant          Duster 360           Merc 240D \n        0.246731745        -0.001408610        -0.021532242         0.101128004 \n           Merc 230            Merc 280           Merc 280C          Merc 450SE \n        0.014481527        -0.077467905        -0.153179727        -0.006685912 \n         Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental \n        0.046739255        -0.082671818        -0.215771303        -0.227580582 \n  Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla \n        0.098076936         0.099131472         0.017593403         0.097817784 \n      Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28 \n       -0.117145641         0.002161632        -0.038029398        -0.106946015 \n   Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa \n        0.321433494        -0.070395535         0.073660971         0.122216133 \n     Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E \n        0.066608217        -0.118181538        -0.055822446        -0.118384067 \n\n\n\n\nBoth aov_model and reg_model inherit from lm\nprint() and residuals() are both generics\n\n(There can be) different versions of each function for different classes\nDifferent output for print()\nSame output for residuals()\n\n\n\n\nprint() and residuals() are generics:\nthe call just passes us off to a class-specific function using UseMethod()\n\n\nprint\n\nfunction (x, ...) \nUseMethod(\"print\")\n<bytecode: 0x7faa0d23e690>\n<environment: namespace:base>\n\nresiduals\n\nfunction (object, ...) \nUseMethod(\"residuals\")\n<bytecode: 0x7faa0c529448>\n<environment: namespace:stats>\n\n\n\n\nsloop package is is designed to figure out which specific function is called\n\n\nlibrary(sloop)\n\ns3_dispatch(print(reg_model))\n\n=> print.lm\n * print.default\n\ns3_dispatch(print(aov_model))\n\n=> print.aov\n * print.lm\n * print.default\n\n\n\nNote that the methods are hidden\n\ntry(print.lm)\n\nError in try(print.lm) : object 'print.lm' not found\n\ntry(stats::print.lm)\n\nError : 'print.lm' is not an exported object from 'namespace:stats'\n\nstats:::print.lm\n\nfunction (x, digits = max(3L, getOption(\"digits\") - 3L), ...) \n{\n    cat(\"\\nCall:\\n\", paste(deparse(x$call), sep = \"\\n\", collapse = \"\\n\"), \n        \"\\n\\n\", sep = \"\")\n    if (length(coef(x))) {\n        cat(\"Coefficients:\\n\")\n        print.default(format(coef(x), digits = digits), print.gap = 2L, \n            quote = FALSE)\n    }\n    else cat(\"No coefficients\\n\")\n    cat(\"\\n\")\n    invisible(x)\n}\n<bytecode: 0x7faa17356f20>\n<environment: namespace:stats>\n\ns3_get_method(print.lm)\n\nfunction (x, digits = max(3L, getOption(\"digits\") - 3L), ...) \n{\n    cat(\"\\nCall:\\n\", paste(deparse(x$call), sep = \"\\n\", collapse = \"\\n\"), \n        \"\\n\\n\", sep = \"\")\n    if (length(coef(x))) {\n        cat(\"Coefficients:\\n\")\n        print.default(format(coef(x), digits = digits), print.gap = 2L, \n            quote = FALSE)\n    }\n    else cat(\"No coefficients\\n\")\n    cat(\"\\n\")\n    invisible(x)\n}\n<bytecode: 0x7faa17356f20>\n<environment: namespace:stats>\n\n\n\nUse s3_dispatch() to explain why the two models have the same output for residuals().\n\n\nsloop::s3_methods_generic() lists all class-specific versions of generics\n\n\ns3_methods_generic('print')\n\n# A tibble: 264 × 4\n   generic class   visible source             \n   <chr>   <chr>   <lgl>   <chr>              \n 1 print   acf     FALSE   registered S3method\n 2 print   AES     FALSE   registered S3method\n 3 print   anova   FALSE   registered S3method\n 4 print   aov     FALSE   registered S3method\n 5 print   aovlist FALSE   registered S3method\n 6 print   ar      FALSE   registered S3method\n 7 print   Arima   FALSE   registered S3method\n 8 print   arima0  FALSE   registered S3method\n 9 print   AsIs    TRUE    base               \n10 print   aspell  FALSE   registered S3method\n# … with 254 more rows\n\n\n\nAnd similarly for all generics defined for a given class\n\n\ns3_methods_class('lm')\n\n# A tibble: 35 × 4\n   generic        class visible source             \n   <chr>          <chr> <lgl>   <chr>              \n 1 add1           lm    FALSE   registered S3method\n 2 alias          lm    FALSE   registered S3method\n 3 anova          lm    FALSE   registered S3method\n 4 case.names     lm    FALSE   registered S3method\n 5 confint        lm    TRUE    stats              \n 6 cooks.distance lm    FALSE   registered S3method\n 7 deviance       lm    FALSE   registered S3method\n 8 dfbeta         lm    FALSE   registered S3method\n 9 dfbetas        lm    FALSE   registered S3method\n10 drop1          lm    FALSE   registered S3method\n# … with 25 more rows"
  },
  {
    "objectID": "content/02-04-paradigms.html#functional-programming",
    "href": "content/02-04-paradigms.html#functional-programming",
    "title": "Programming Paradigms",
    "section": "Functional programming",
    "text": "Functional programming\n\n\n\n\nflowchart LR\n  pre1[\" \"] -- data --> extract[extract\\nDV & IV]\n  pre2[\" \"] -- specification --> extract\n  extract -- X --> QR[QR\\ndecomposition] -- \"Q, R\" --> combine\n  extract -- Y --> combine\n  combine -- \"Rβ = Q<sup>T</sup>Y\" --> backsolve\n  backsolve -- β --> post[\" \"]\n  style pre1 height:0px;\n  style pre2 height:0px;\n  style post height:0px;\n\n\n\n\n\nA regression model as a series of functions"
  },
  {
    "objectID": "content/02-04-paradigms.html#features-of-functional-programming",
    "href": "content/02-04-paradigms.html#features-of-functional-programming",
    "title": "Programming Paradigms",
    "section": "Features of functional programming",
    "text": "Features of functional programming\n\nFirst-class functions\n\nFunctions can be used like any other data type, including as inputs to and outputs from other functions\n\nDeterminism\n\nGiven a list of input values, a function always returns the same output value\n\nNo side effects\n\nA function doesn’t have any effects other than returning its output\n\nImmutability\n\nOnce a variable is assigned a value, that value cannot be changed\n\n\n\n\n\nR doesn’t enforce determinism, no side effects, or immutability\nBut writing your own code around them makes it easier to reason about how your code works"
  },
  {
    "objectID": "content/02-04-paradigms.html#pipes-and-the-tidyverse",
    "href": "content/02-04-paradigms.html#pipes-and-the-tidyverse",
    "title": "Programming Paradigms",
    "section": "Pipes (and the tidyverse)",
    "text": "Pipes (and the tidyverse)\n\n(Almost) every function is made up of smaller functions, chained together\nPipe syntax (%>% and |>) lets us chain functions together in a readable way\nTidyverse functions are designed to work well in pipes\n\n\nlibrary(dplyr)\n\n\nmtcars %>%\n    filter(cyl > 4) %>% \n    lm(mpg ~ disp, data = .) %>% \n    summary()\n\n\nCall:\nlm(formula = mpg ~ disp, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4010 -1.5419 -0.5121  1.1408  4.9873 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 23.623520   1.463181  16.145 1.50e-12 ***\ndisp        -0.023527   0.004682  -5.025 7.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.118 on 19 degrees of freedom\nMultiple R-squared:  0.5706,    Adjusted R-squared:  0.548 \nF-statistic: 25.25 on 1 and 19 DF,  p-value: 7.521e-05"
  },
  {
    "objectID": "content/02-04-paradigms.html#programming-paradigms-and-data-science",
    "href": "content/02-04-paradigms.html#programming-paradigms-and-data-science",
    "title": "Programming Paradigms",
    "section": "Programming paradigms and data science",
    "text": "Programming paradigms and data science\n\nOOP is most useful for developers\nFunctional programming rules are really useful for data cleaning and analysis\n\n“The analysis pipeline”\nReasoning about the state of our code\nEnsuring reproducibility"
  },
  {
    "objectID": "content/02-04-paradigms.html#lab",
    "href": "content/02-04-paradigms.html#lab",
    "title": "Programming Paradigms",
    "section": "Lab",
    "text": "Lab\nLab 3 explores functional programming in R:\n\nAssembling pipes using tidyverse functions\nWriting code that respects immutability\nUsing the map() functional to apply a function to each element of a list\nWriting a function factory to change the way a function behaves\n\n\n\n\n\n\nChambers, John M. 2014. “Object-Oriented Programming, Functional Programming and R.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452.\n\n\nWickham, Hadley. 2014. Advanced R. CRC Press. https://adv-r.hadley.nz."
  },
  {
    "objectID": "content/03-02-models-of-eda.html#section",
    "href": "content/03-02-models-of-eda.html#section",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation. (John Wilder Tukey 1977)\n\n\nExploratory Data Analysis (EDA) is “an attitude, AND a flexibility, AND some graph paper (or transparencies, or both)” (John W. Tukey 1980)"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#exploratory-and-confirmatory-research",
    "href": "content/03-02-models-of-eda.html#exploratory-and-confirmatory-research",
    "title": "Exploratory Data Analysis",
    "section": "Exploratory and Confirmatory Research",
    "text": "Exploratory and Confirmatory Research\n\nEspecially in the wake of the replication crisis, one common distinction is between exploratory and confirmatory research (Wagenmakers et al. 2012)\n\n\nExploratory vs. confirmatory research\n\n\n\n\n\n\nConfirmatory\nExploratory\n\n\n\n\nhypothesis testing\nhypothesis development\n\n\nspecified in advance\nadaptable\n\n\nalgorithmic\nfree, creative\n\n\nmechanical objectivity (Daston and Galison 2007)\npure subjectivity?\n\n\navoids inferential errors\nmakes errors?\n\n\nrigorous\nlacking rigor?\n\n\nreal science??\nh*cking around with data??\n\n\nassumes experimental methods\nrelevant to all methods\n\n\n\n\nI agree that it’s important to\n\nbe thoughtful about how much confidence we’re placing in our conclusions\ninterpret findings from one study in light of other studies\n\nBut the confirmatory/exploratory distinction can overemphasize the confirmatory side\n\nMaking us too rigid and narrow-minded about what counts as good science"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#better-models-for-eda-i-developing-phenomena",
    "href": "content/03-02-models-of-eda.html#better-models-for-eda-i-developing-phenomena",
    "title": "Exploratory Data Analysis",
    "section": "Better Models for EDA I: Developing Phenomena",
    "text": "Better Models for EDA I: Developing Phenomena\n\nBrown (2002)\n\n\nThe data/phenomena/theory distinction\n\n\n\n\n\n\n\nData\nPhenomena\nTheories/ Causal processes\n\n\n\n\nEx: spreadsheet of numbers, downloaded from Qualtrics\nEx: Correlation between partisanship and sharing Covid misinformation\nEx: Conservative susceptibility to anxiety hypothesis\n\n\n\n\n\n\n\ncollected\nabstracted or extracted from data\npostulated\n\n\nnot explained\nexplained by theories\nexplain phenomena\n\n\nhighly local to time, place, sample, procedure\nvarying scope\nuniversal?\n\n\n“raw,” messy, unwieldy\n“processed,” clean, stylized\n“laws of nature”?"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#eda-as-phenomena-development",
    "href": "content/03-02-models-of-eda.html#eda-as-phenomena-development",
    "title": "Exploratory Data Analysis",
    "section": "EDA as phenomena development",
    "text": "EDA as phenomena development\n\ncleaning messy data\n\nidentifying and mitigating (where possible) errors and idiosyncracies\n\nidentifying local patterns (“local phenomena”)\nnot yet claiming these will be stable and appear elsewhere\nnot yet worrying (much) about explanations"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#better-models-for-eda-ii-epicycle-of-analysis",
    "href": "content/03-02-models-of-eda.html#better-models-for-eda-ii-epicycle-of-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Better Models for EDA II: Epicycle of Analysis",
    "text": "Better Models for EDA II: Epicycle of Analysis\n\n\n\n\n\nEpicycle of analysis model, Peng and Matsui (2016), 5"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#pengartdatascience2016",
    "href": "content/03-02-models-of-eda.html#pengartdatascience2016",
    "title": "Exploratory Data Analysis",
    "section": "Peng and Matsui (2016)",
    "text": "Peng and Matsui (2016)\n\nData analysis is organized into 5 activities\nEach activity involves the same 3-step “epicycle” process\n\nDevelop expectations\nCollect information\nCompare and revise expectations\n\nNot “the scientific method”! (Peng and Matsui 2016, 4)\n\n“Highly iterative and non-linear”\n“information is learned at each step, which then informs\n\nwhether (and how) to refine, and redo, the [previous] step …, or\nwhether (and how) to proceed to the next step.”"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#section-1",
    "href": "content/03-02-models-of-eda.html#section-1",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Aligning the goals of EDA with steps in the “epicycle of analysis” (Peng and Matsui 2016)\n\n\n\n\n\n\nGoals of EDA\nEpicycle step\n\n\n\n\nDetermine if there are problems with the data\n2. Collecting information\n\n\nDetermine whether our question can be answered with these data\n3. Comparing and revising expectations\n\n\nDevelop sketch of an answer\n1. Developing expectations"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#discussion",
    "href": "content/03-02-models-of-eda.html#discussion",
    "title": "Exploratory Data Analysis",
    "section": "Discussion",
    "text": "Discussion\n\nFor each of these models, how well do they fit the way you’ve been taught to do science?\nHow do they challenge the way you’ve been taught to do science?"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#references",
    "href": "content/03-02-models-of-eda.html#references",
    "title": "Exploratory Data Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nBrown, James Robert. 2002. Smoke and Mirrors: How Science Reflects Reality. Routledge.\n\n\nDaston, Lorraine, and Peter Galison. 2007. Objectivity. New York : Cambridge, Mass: Zone Books ; Distributed by the MIT Press.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience.\n\n\nTukey, John W. 1980. “We Need Both Exploratory and Confirmatory.” The American Statistician 34 (1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley Series in Behavioral Science. Reading, Mass: Addison-Wesley Pub. Co. https://archive.org/details/exploratorydataa00tuke_0.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der Maas, and Rogier A. Kievit. 2012. “An Agenda for Purely Confirmatory Research.” Perspectives on Psychological Science 7 (6): 632–38. https://doi.org/10.1177/1745691612463078."
  },
  {
    "objectID": "content/03-03-checklists.html#pengartdatascience2016",
    "href": "content/03-03-checklists.html#pengartdatascience2016",
    "title": "Two EDA “Checklists”",
    "section": "Peng and Matsui (2016)",
    "text": "Peng and Matsui (2016)\n\nFormulate your question\nRead in your data\nCheck the packaging\nLook at the top and the bottom of your data\nCheck your “n”s\nValidate with at least one external data source\nMake a plot\nTry the easy solution first\nFollow up (Peng and Matsui 2016, 33)"
  },
  {
    "objectID": "content/03-03-checklists.html#huebnersystematicapproachinitial2016",
    "href": "content/03-03-checklists.html#huebnersystematicapproachinitial2016",
    "title": "Two EDA “Checklists”",
    "section": "Huebner, Vach, and le Cessie (2016)",
    "text": "Huebner, Vach, and le Cessie (2016)\n\nDuplicate records need to be eliminated\nDirection of numerical codes for categorical and ordinal variables\nInconsistencies in date and time stamps\nPresence of bimodal distributions\nPresence of skewed distributions\nPresence of ceilings and floors in continuous variables\nPresence of outliers\nDistribution of missing data\nIndications of recording errors in the main variables (after Huebner, Vach, and le Cessie 2016, 26)"
  },
  {
    "objectID": "content/03-03-checklists.html#references",
    "href": "content/03-03-checklists.html#references",
    "title": "Two EDA “Checklists”",
    "section": "References",
    "text": "References\n\n\n\n\nHuebner, Marianne, Werner Vach, and Saskia le Cessie. 2016. “A Systematic Approach to Initial Data Analysis Is Good Research Practice.” The Journal of Thoracic and Cardiovascular Surgery 151 (1): 25–27. https://doi.org/10.1016/j.jtcvs.2015.09.085.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience."
  },
  {
    "objectID": "content/03-04-oakland-eda.html#section",
    "href": "content/03-04-oakland-eda.html#section",
    "title": "Case Study: Police Stops in Oakland",
    "section": "",
    "text": "For this EDA, we’ll work with data on police stops in Oakland, California, that have been pre-cleaned and released by the Stanford Open Policing Project (Pierson et al. 2020).\nBecause this analysis focuses on categorical data and counts of observations, most of the elements in Huebner, Vach, and le Cessie (2016) don’t really fit.\n\nSo we’ll follow the checklist from Peng and Matsui (2016).\n\nWe’ll also be learning to use the skimr and visdat packages"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#formulate-your-question",
    "href": "content/03-04-oakland-eda.html#formulate-your-question",
    "title": "Case Study: Police Stops in Oakland",
    "section": "1. Formulate your question",
    "text": "1. Formulate your question\n\nThe Black Lives Matter protests over the last several years have made us aware of the racial aspects of policing.\nHere we’re specifically interested in\n\nWhether Black people in Oakland might be more likely to be stopped than White people\nWhether Black people who are stopped might be more likely to have contraband\n\nThese aren’t very precise, but that’s okay: Part of the goal of EDA is to clarify and refine our research questions"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#reflexivity",
    "href": "content/03-04-oakland-eda.html#reflexivity",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Reflexivity",
    "text": "Reflexivity\n\nWhether Black people in Oakland might be more likely to be stopped than White people\nWhether Black people who are stopped might be more likely to have contraband\n\n\nOnce we have a rough idea of what we want to know, we need to take a moment to think about why we want to know it\n\nClarify what “success” means to us\nShare with others to whom we’re accountable\nRecognize that we (academic researchers) often lack accountability to people who might be affected by our work\nespecially when we claim to be acting for their benefit\n\nWe’ll spend 3 minutes writing responses to each of these questions:\n\n\nWhat do I already know about this subject?\nWhy am I studying this?\nWhat do I expect or hope to find/learn, and why?\nWho is affected by this topic, and how am I connected to them?\n\n(Adapted from Tanweer et al. (2021), 14-15, and Liboiron (2021))"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#set-up-our-workspace",
    "href": "content/03-04-oakland-eda.html#set-up-our-workspace",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Set up our workspace",
    "text": "Set up our workspace\n\nDedicated project folder\nClean R session\nMore on project management and organization later in the semester"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#packages",
    "href": "content/03-04-oakland-eda.html#packages",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)   # for working with the data\nlibrary(lubridate)   # for working with datetime data\n\nlibrary(skimr)       # generate a text-based overview of the data\nlibrary(visdat)      # generate plots visualizing data types and missingness\nlibrary(plotly)      # quickly create interactive plots"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#get-the-data",
    "href": "content/03-04-oakland-eda.html#get-the-data",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Get the Data",
    "text": "Get the Data\n\nWe’ll be using data on police stops in Oakland, California, collected and published by the Stanford Open Policing Project.\nFor reproducibility, we’ll write a bit of code that automatically downloads the data\nTo get the download URL:\n\nhttps://openpolicing.stanford.edu/data/\nScroll down to Oakland\nRight-click on the file symbol to copy the URL\n\nREADME: https://github.com/stanford-policylab/opp/blob/master/data_readme.md.\n\n\ndata_dir = 'data'\ntarget_file = file.path(data_dir, 'oakland.zip')\n\nif (!dir.exists(data_dir)) {\n    dir.create(data_dir)\n}\nif (!file.exists(target_file)) {\n    download.file('https://stacks.stanford.edu/file/druid:yg821jf8611/yg821jf8611_ca_oakland_2020_04_01.csv.zip', \n                  target_file)\n}"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#read-in-your-data",
    "href": "content/03-04-oakland-eda.html#read-in-your-data",
    "title": "Case Study: Police Stops in Oakland",
    "section": "2. Read in your data",
    "text": "2. Read in your data\nThe dataset is a zipped csv or comma-separated value file. CSVs are structured like Excel spreadsheets, but are stored in plain text rather than Excel’s format.\n\ndataf = read_csv(target_file)\n\nRows: 133407 Columns: 28\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): raw_row_number, location, beat, subject_race, subject_sex, officer_assignment, type, ...\ndbl   (3): lat, lng, subject_age\nlgl   (7): arrest_made, citation_issued, warning_issued, contraband_found, contraband_drugs, con...\ndate  (1): date\ntime  (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "content/03-04-oakland-eda.html#check-the-packaging",
    "href": "content/03-04-oakland-eda.html#check-the-packaging",
    "title": "Case Study: Police Stops in Oakland",
    "section": "3. Check the packaging",
    "text": "3. Check the packaging\nPeng and Matsui (2016) use some base R functions to look at dimensions of the dataframe and column (variable) types. skimr is more powerful.\n\n## May take a couple of seconds\nskim(dataf)\n\n── Data Summary ────────────────────────\n                           Values\nName                       dataf \nNumber of rows             133407\nNumber of columns          28    \n_______________________          \nColumn type frequency:           \n  character                16    \n  Date                     1     \n  difftime                 1     \n  logical                  7     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────────────────────────\n   skim_variable                 n_missing complete_rate min max empty n_unique whitespace\n 1 raw_row_number                        0        1        1  71     0   133407          0\n 2 location                             51        1.00     1  78     0    60723          0\n 3 beat                              72424        0.457    3  19     0      129          0\n 4 subject_race                          0        1        5  22     0        5          0\n 5 subject_sex                          90        0.999    4   6     0        2          0\n 6 officer_assignment               121431        0.0898   5  97     0       20          0\n 7 type                              20066        0.850    9  10     0        2          0\n 8 outcome                           34107        0.744    6   8     0        3          0\n 9 search_basis                      92250        0.309    5  14     0        3          0\n10 reason_for_stop                       0        1       14 197     0      113          0\n11 use_of_force_description         116734        0.125   10  10     0        1          0\n12 raw_subject_sdrace                    0        1        1   1     0        7          0\n13 raw_subject_resultofencounter         0        1        7 213     0      315          0\n14 raw_subject_searchconducted           0        1        2  24     0       34          0\n15 raw_subject_typeofsearch          52186        0.609    2 112     0      417          0\n16 raw_subject_resultofsearch       111633        0.163    5  95     0      298          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min        max        median     n_unique\n1 date                  2          1.00 2013-04-01 2017-12-31 2015-07-19     1638\n\n── Variable type: difftime ─────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min    max        median n_unique\n1 time                  2          1.00 0 secs 86340 secs 16:12      1439\n\n── Variable type: logical ──────────────────────────────────────────────────────────────────────────\n  skim_variable      n_missing complete_rate   mean count                  \n1 arrest_made                0         1     0.121  FAL: 117217, TRU: 16190\n2 citation_issued            0         1     0.394  FAL: 80836, TRU: 52571 \n3 warning_issued             0         1     0.231  FAL: 102545, TRU: 30862\n4 contraband_found       92250         0.309 0.149  FAL: 35005, TRU: 6152  \n5 contraband_drugs       92250         0.309 0.0844 FAL: 37684, TRU: 3473  \n6 contraband_weapons     92250         0.309 0.0299 FAL: 39928, TRU: 1229  \n7 search_conducted           0         1     0.309  FAL: 92250, TRU: 41157 \n\n── Variable type: numeric ──────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   mean      sd     p0    p25    p50    p75   p100 hist \n1 lat                 114         0.999   37.8  0.0284   37.4   37.8   37.8   37.8   38.1 ▁▁▇▁▁\n2 lng                 114         0.999 -122.   0.0432 -122.  -122.  -122.  -122.  -119.  ▇▁▁▁▁\n3 subject_age      102724         0.230   33.2 13.3      10     23     29     41     97   ▇▆▃▁▁\n\n\n\n\n133k rows (observations); 28 columns (variables)\n16 variables are handled as characters\n\nraw_row_number has 1 unique value per row\n\nSo it’s probably some kind of identifier\n\nsubject_race and subject_sex have just 5 and 2 unique values\n\nThese are probably categorical variables represented as characters\n\nSimilarly with type, outcome, and search_basis\n\nThough these have lots of missing values (high n_missing, low complete_rate)\n\n\n1 variable represents the date, and another is difftime\n\n?difftime tells us that difftime is used to represent intervals or “time differences”\n\n7 logical variables\n\nA lot of these look like coded outcomes that we might be interested in, eg, search_conducted and contraband_found\nsearch_conducted has no missing values, but contraband_found has a lot of missing values"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#for-our-motivating-questions",
    "href": "content/03-04-oakland-eda.html#for-our-motivating-questions",
    "title": "Case Study: Police Stops in Oakland",
    "section": "For our motivating questions",
    "text": "For our motivating questions\n\nGood: subject_race is 100% complete\nAlso good: search_conducted is also 100% complete\nPotentially worrisome: contraband_found is only 31% complete"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#missing-values",
    "href": "content/03-04-oakland-eda.html#missing-values",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Missing values",
    "text": "Missing values\n\nLet’s use visdat::vis_miss()to\n\nvisualize missing values and\ncheck what’s up with contraband_found.\n\n\n But this raises a warning about large data\n\nvis_miss(dataf)\n\n\nSo we’ll use sample_n() to draw a subset\n\nset.seed(2021-09-28)\ndataf_smol = sample_n(dataf, 1000)\n\nvis_miss(dataf_smol)\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nPlease use `gather()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n\n\nArguments in vis_miss() are useful for picking up patterns in missing values\n\n## cluster = TRUE uses hierarchical clustering to order the rows\nvis_miss(dataf_smol, cluster = TRUE) +\n    coord_flip()\n\n\n\n\n\nSeveral variables related to search outcomes are missing together\n\ncontraband_found, contraband_drugs, contraband_weapons, search_basis, use_of_force_description, raw_subject_typeofsearch, and raw_subject_resultofsearch\n\nHowever, search_conducted is complete"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#a-critical-question",
    "href": "content/03-04-oakland-eda.html#a-critical-question",
    "title": "Case Study: Police Stops in Oakland",
    "section": "A critical question",
    "text": "A critical question\nWhen a search has been conducted, do we know whether contraband was found?\n\nOr: are there cases where a search was conducted, but contraband_found is missing?\n\n\ndataf  |>  \n    filter(search_conducted) |>\n    count(search_conducted, is.na(contraband_found))\n\n# A tibble: 1 × 3\n  search_conducted `is.na(contraband_found)`     n\n  <lgl>            <lgl>                     <int>\n1 TRUE             FALSE                     41157"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#look-at-the-top-and-the-bottom-of-your-data",
    "href": "content/03-04-oakland-eda.html#look-at-the-top-and-the-bottom-of-your-data",
    "title": "Case Study: Police Stops in Oakland",
    "section": "4. Look at the top and the bottom of your data",
    "text": "4. Look at the top and the bottom of your data\nWith 28 columns, the dataframe is too wide to print in a readable way.\nInstead we’ll use the base R function View() in an interactive session. This shows us an Excel-like spreadsheet presentation of a dataframe.\nView() can cause significant problems if you use it with a large dataframe on a slower machine. So we’ll use a pipe: first extract the head() or tail() of the dataset, and then View() it. We’ll also go ahead and view dataf_smol, the subset we created for visdat above.\n\ndataf |> \n    head() |> \n    View()\n\ndataf |> \n    tail() |> \n    View()\n\nView(dataf_smol)\n\n\nSome of my observations:\n\nThe ID variable raw_row_number can’t be turned into a numeric value\nlocation is a mix of addresses and intersections (“Bond St @ 48TH AVE”)\n\nIf we were going to generate a map using this column, geocoding might be tricky\nFortunately we also get latitude and longitude columns\n\nuse_of_force_description doesn’t seem to be a descriptive text field; instead it seems to be mostly missing or “handcuffed”\n\n\nWe can also use skimr to check data quality by looking at the minimum and maximum values. Do these ranges make sense for what we expect the variable to be?\n\nskim(dataf)\n\n── Data Summary ────────────────────────\n                           Values\nName                       dataf \nNumber of rows             133407\nNumber of columns          28    \n_______________________          \nColumn type frequency:           \n  character                16    \n  Date                     1     \n  difftime                 1     \n  logical                  7     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────────────────────────\n   skim_variable                 n_missing complete_rate min max empty n_unique whitespace\n 1 raw_row_number                        0        1        1  71     0   133407          0\n 2 location                             51        1.00     1  78     0    60723          0\n 3 beat                              72424        0.457    3  19     0      129          0\n 4 subject_race                          0        1        5  22     0        5          0\n 5 subject_sex                          90        0.999    4   6     0        2          0\n 6 officer_assignment               121431        0.0898   5  97     0       20          0\n 7 type                              20066        0.850    9  10     0        2          0\n 8 outcome                           34107        0.744    6   8     0        3          0\n 9 search_basis                      92250        0.309    5  14     0        3          0\n10 reason_for_stop                       0        1       14 197     0      113          0\n11 use_of_force_description         116734        0.125   10  10     0        1          0\n12 raw_subject_sdrace                    0        1        1   1     0        7          0\n13 raw_subject_resultofencounter         0        1        7 213     0      315          0\n14 raw_subject_searchconducted           0        1        2  24     0       34          0\n15 raw_subject_typeofsearch          52186        0.609    2 112     0      417          0\n16 raw_subject_resultofsearch       111633        0.163    5  95     0      298          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min        max        median     n_unique\n1 date                  2          1.00 2013-04-01 2017-12-31 2015-07-19     1638\n\n── Variable type: difftime ─────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min    max        median n_unique\n1 time                  2          1.00 0 secs 86340 secs 16:12      1439\n\n── Variable type: logical ──────────────────────────────────────────────────────────────────────────\n  skim_variable      n_missing complete_rate   mean count                  \n1 arrest_made                0         1     0.121  FAL: 117217, TRU: 16190\n2 citation_issued            0         1     0.394  FAL: 80836, TRU: 52571 \n3 warning_issued             0         1     0.231  FAL: 102545, TRU: 30862\n4 contraband_found       92250         0.309 0.149  FAL: 35005, TRU: 6152  \n5 contraband_drugs       92250         0.309 0.0844 FAL: 37684, TRU: 3473  \n6 contraband_weapons     92250         0.309 0.0299 FAL: 39928, TRU: 1229  \n7 search_conducted           0         1     0.309  FAL: 92250, TRU: 41157 \n\n── Variable type: numeric ──────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   mean      sd     p0    p25    p50    p75   p100 hist \n1 lat                 114         0.999   37.8  0.0284   37.4   37.8   37.8   37.8   38.1 ▁▁▇▁▁\n2 lng                 114         0.999 -122.   0.0432 -122.  -122.  -122.  -122.  -119.  ▇▁▁▁▁\n3 subject_age      102724         0.230   33.2 13.3      10     23     29     41     97   ▇▆▃▁▁\n\n\n\nMore observations:\n\nDate range is April 1, 2013 to December 31, 2017\n\nIf we break things down by year, we should expect 2013 to have fewer cases\nFor some purposes, we might need to exclude 2013 data: filter(dataf, date >= '2014-01-01')\n\nAge range is from 10 years old (!) to 97 (!)\n\nMedian (p50) is 29; 50% of values are between 23 and 41\nFor some purposes, we might need to restrict the analysis to working-age adults: filter(dataf, subject_age >= 18, subject_age < 65)"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#check-your-ns-and-6.-validate-with-at-least-one-external-data-source",
    "href": "content/03-04-oakland-eda.html#check-your-ns-and-6.-validate-with-at-least-one-external-data-source",
    "title": "Case Study: Police Stops in Oakland",
    "section": "5. Check your Ns (and) 6. Validate with at least one external data source",
    "text": "5. Check your Ns (and) 6. Validate with at least one external data source\n\nPeng and Matsui (2016) use an air quality example with a regular sampling rate\n\nso they can calculate exactly how many observations they should have\n\nWe can’t do that here\n\nSo we’ll combine steps 5 and 6 together\n\n\n\n\nA web search leads us to this City of Oakland page on police stop data: https://www.oaklandca.gov/resources/stop-data\n\nThe page mentions a Stanford study that was released in June 2016\nRecall we got our data from the Stanford Open Policing Project\nOur data run through December 2017\nSo there’s a good chance we’re using a superset of the “Stanford study” data\n\n\n\n\nThe page links to this report: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf\n\nPage 8 has two summary tables that we can compare to our data\n\n\n\n\n\nScreenshot of the two summary tables from the Oakland report. Source: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf, page 8"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#from-dates-to-years",
    "href": "content/03-04-oakland-eda.html#from-dates-to-years",
    "title": "Case Study: Police Stops in Oakland",
    "section": "From dates to years",
    "text": "From dates to years\n\nOur data has the particular date of each stop\n\nWe need to extract the year of each stop\n\nlubridate::year() does exactly this\n\nFilter to the years in our data that overlap with the tables\nAnd then aggregate by year and gender using count\n\n\n\n\ndataf |> \n    mutate(year = year(date)) |> \n    filter(year %in% c(2016, 2017)) |> \n    count(year)\n\n# A tibble: 2 × 2\n   year     n\n  <dbl> <int>\n1  2016 30268\n2  2017 30715\n\ndataf |> \n    mutate(year = year(date)) |> \n    filter(year %in% c(2016, 2017)) |> \n    count(year, subject_sex)\n\n# A tibble: 6 × 3\n   year subject_sex     n\n  <dbl> <chr>       <int>\n1  2016 female       7677\n2  2016 male        22563\n3  2016 <NA>           28\n4  2017 female       7879\n5  2017 male        22818\n6  2017 <NA>           18\n\n\n\n\nFor both years, we have fewer observations than the report table indicates\n\nCould our data have been pre-filtered?\nLet’s check the documentation for our data: https://github.com/stanford-policylab/opp/blob/master/data_readme.md#oakland-ca\n\n\n\n\n“Data is deduplicated on raw columns contactdate, contacttime, streetname, subject_sdrace, subject_sex, and subject_age, reducing the number of records by ~5.2%”\n\nThe difference with the report is on this order of magnitude,\nBut varies within groups by several percentage points\nSo deduplication might explain the difference\nBut in a more serious analysis we might want to check, eg, with the Stanford Open Policing folks\n\n\n\n## Men in 2016 in the report vs. our data: 8.2%\n(24576 - 22563) / 24576\n\n[1] 0.08190918\n\n## Women in 2016 in the report vs. our data: 3.6%\n(7965 - 7677) / 7965\n\n[1] 0.03615819\n\n## All of 2016 in the report vs. our data: 7.1%\n(32569 - 30268) / 32569\n\n[1] 0.07065"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#make-a-plot",
    "href": "content/03-04-oakland-eda.html#make-a-plot",
    "title": "Case Study: Police Stops in Oakland",
    "section": "7. Make a plot",
    "text": "7. Make a plot\nPeng and Matsui note that plots are useful for both checking and setting expectations\n\nAn expectation we formed earlier: fewer stops in 2013\nWe can combine pipes with ggplot() to get the year\ngeom_bar() gives us counts\nWhat’s up with the warnings?\n\n\ndataf |> \n    mutate(year = year(date)) |> \n    ggplot(aes(year)) +\n    geom_bar()\n\nWarning: Removed 2 rows containing non-finite values (stat_count).\n\n\n\n\n\n\nHow about counts per year by race/ethnicity?\n\nThis version is too hard to read\n\n\ndataf |> \n    mutate(year = year(date)) |> \n    ggplot(aes(year, fill = subject_race)) +\n    geom_bar()\n\nWarning: Removed 2 rows containing non-finite values (stat_count).\n\n\n\n\n\n\nLet’s switch from bars to points and lines and change up the color palette\n\nWhy does this produce 2 warnings?\n\n\ndataf |> \n    mutate(year = year(date)) |> \n    ggplot(aes(year, color = subject_race)) +\n    geom_point(stat = 'count') +\n    geom_line(stat = 'count') +\n    scale_color_brewer(palette = 'Set1')\n\nWarning: Removed 2 rows containing non-finite values (stat_count).\nRemoved 2 rows containing non-finite values (stat_count).\n\n\n\n\n\n\nplotly::ggplotly() creates an interactive version of a ggplot object\n\nWhy don’t I need to give it the plot we just created?\n\n\nggplotly()\n\nWarning: Removed 2 rows containing non-finite values (stat_count).\nRemoved 2 rows containing non-finite values (stat_count).\n\n\n\n\n\n\n\n\nThese plots confirm our expectation of lower counts in 2013\nDo they help us develop any new expectations as we move on to addressing our research questions?"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#try-the-easy-solution-first",
    "href": "content/03-04-oakland-eda.html#try-the-easy-solution-first",
    "title": "Case Study: Police Stops in Oakland",
    "section": "8. Try the easy solution first",
    "text": "8. Try the easy solution first\nLet’s translate our natural-language research questions into statistical questions:\n\nWhether Black people in Oakland might be more likely to be stopped than White people \\[ \\Pr(stopped | Black) \\textrm{ vs } \\Pr(stopped | White) \\]\nWhether Black people who are stopped might be more likely to have contraband \\[ \\Pr(contraband | stopped, searched, Black) \\textrm{ vs } \\Pr(contraband | stopped, searched, White) \\]\n\n\nThe easy solution is to estimate probabilities by calculating rates within groups"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#mathematical-aside",
    "href": "content/03-04-oakland-eda.html#mathematical-aside",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Mathematical aside",
    "text": "Mathematical aside\n\nFor Q1, we can’t calculate \\(\\Pr(stopped | Black)\\) directly:\neveryone in our data was stopped\nBut Bayes’ theorem lets us get at the comparison indirectly \\[ \\Pr(stopped | Black) = \\frac{\\Pr(Black | stopped) \\times \\Pr(stopped)}{\\Pr(Black)} \\]\n\n\\[ \\frac{\\Pr(stopped | Black)}{\\Pr(stopped | White)} = \\frac{\\Pr(Black|stopped)}{\\Pr(Black)} \\frac{\\Pr(White)}{\\Pr(White|stopped)} \\]\n\n\\(\\Pr(Black)\\) is the share of the statistical population that’s Black\nIf we assume the statistical population = residents of Oakland, can use Census data\nWhy is this assumption probably false? Is it okay to use it anyways?"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#stops-by-race",
    "href": "content/03-04-oakland-eda.html#stops-by-race",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Stops, by race",
    "text": "Stops, by race\n\\[ \\Pr(Black|stopped) \\]\n\ndataf |> \n    count(subject_race) |> \n    mutate(share = n / sum(n)) |> \n    arrange(desc(share)) |> \n    mutate(share = scales::percent(share, accuracy = 1))\n\n# A tibble: 5 × 3\n  subject_race               n share\n  <chr>                  <int> <chr>\n1 black                  78925 59%  \n2 hispanic               26257 20%  \n3 white                  15628 12%  \n4 asian/pacific islander  8099 6%   \n5 other                   4498 3%   \n\n\n\n\nPolice stop demographics\n\n59% of subjects stopped by police are Black\n20% are Hispanic\n12% are White\n6% are API\n\nOakland demographics in 2020: https://en.wikipedia.org/wiki/Oakland,_California#Race_and_ethnicity\n\n24% of residents are Black\n27% are Hispanic\n27% are non-Hispanic White\n16% are Asian\n\n\n\n\n\n\nr/e\nresidents\nstops\nratio\n\n\n\n\nBlack\n24%\n59%\n2.5\n\n\nHispanic\n27%\n20%\n0.7\n\n\nWhite\n27%\n12%\n0.4\n\n\nAPI\n16%\n6%\n0.4\n\n\n\n\nBlacks are severely overrepresented in police stops\nHispanics and API folks are slightly underrepresented\nWhites are significantly underrepresented"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#searches-by-race",
    "href": "content/03-04-oakland-eda.html#searches-by-race",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Searches, by race",
    "text": "Searches, by race\n\nWhat fraction of stops had a search? \\[ \\Pr(searched | stopped) \\]\nAre there disparities by race there? \\[ \\Pr(searched | stopped, Black) \\textrm{ vs } \\Pr(searched | stopped, White) \\]\n\n\n## What fraction of stops had a search? \ndataf |> \n    count(search_conducted) |> \n    mutate(share = n / sum(n))\n\n# A tibble: 2 × 3\n  search_conducted     n share\n  <lgl>            <int> <dbl>\n1 FALSE            92250 0.691\n2 TRUE             41157 0.309\n\n\nAcross all subjects, 31% of stops involved a search.\n\n\nNow we break down the search rate by race-ethnicity\n\n\nggplot(dataf, aes(subject_race, fill = search_conducted)) +\n    geom_bar(position = position_fill()) +\n    scale_fill_manual(values = c('transparent', 'red'))\n\n\n\ndataf |> \n    count(subject_race, search_conducted) |> \n    group_by(subject_race) |> \n    mutate(rate = n / sum(n)) |> \n    ungroup() |> \n    filter(search_conducted) |> \n    mutate(rate = scales::percent(rate, accuracy = 1))\n\n# A tibble: 5 × 4\n  subject_race           search_conducted     n rate \n  <chr>                  <lgl>            <int> <chr>\n1 asian/pacific islander TRUE              1304 16%  \n2 black                  TRUE             30025 38%  \n3 hispanic               TRUE              6722 26%  \n4 other                  TRUE               706 16%  \n5 white                  TRUE              2400 15%  \n\n\n\nFor all groups, most stops didn’t involve a search\nFor Black subjects, 38% of stops involved a search\nFor White subjects, 15% of stops involved a search\nBlack subjects were about 2.5x more likely to be searched than White subjects"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#contraband-finds-by-race",
    "href": "content/03-04-oakland-eda.html#contraband-finds-by-race",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Contraband finds, by race",
    "text": "Contraband finds, by race\n\\[ \\Pr(contraband | searched, stopped, Black) \\] We want to filter() down to only stops where a search was conducted\n\ndataf |> \n    filter(search_conducted) |> \n    ggplot(aes(subject_race, fill = contraband_found)) +\n    geom_bar(position = position_fill()) +\n    scale_fill_manual(values = c('transparent', 'blue')) +\n    ylim(0, .2)\n\nWarning: Removed 5 rows containing missing values (geom_bar).\n\n\n\n\ndataf |> \n    filter(search_conducted) |> \n    count(subject_race, contraband_found) |> \n    group_by(subject_race) |> \n    mutate(rate = n / sum(n)) |> \n    ungroup() |> \n    filter(contraband_found) |> \n    mutate(rate = scales::percent(rate, accuracy = 1))\n\n# A tibble: 5 × 4\n  subject_race           contraband_found     n rate \n  <chr>                  <lgl>            <int> <chr>\n1 asian/pacific islander TRUE               176 13%  \n2 black                  TRUE              4364 15%  \n3 hispanic               TRUE              1116 17%  \n4 other                  TRUE                85 12%  \n5 white                  TRUE               411 17%  \n\n\n\nFor Black subjects who were searched, contraband was found 15% of the time\nFor White subjects, 17% of the time"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#results",
    "href": "content/03-04-oakland-eda.html#results",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Results",
    "text": "Results\nThis preliminary analysis indicates that\n\nBlack subjects were more likely to be stopped and searched than White subjects; but,\nwhen they were searched, White subjects were more likely to have contraband."
  },
  {
    "objectID": "content/03-04-oakland-eda.html#follow-up",
    "href": "content/03-04-oakland-eda.html#follow-up",
    "title": "Case Study: Police Stops in Oakland",
    "section": "9. Follow up",
    "text": "9. Follow up\nWhat are some further directions we could take this analysis?\n\n\nInvestigate outstanding questions about quality and reliability of the data\n\neg, follow up with Stanford Open Policing Project about the difference in row counts\nFits with epicycle of analysis: checking expectations\n\nBreak down our question into more fine-grained analyses\n\neg, the Oakland web site and report talk about policy changes; do we see changes by year in the data?\nFits with epicycle of analysis: refine and specify research questions\n\nApply more sophisticated statistical analysis\n\neg, a regression model to control for age, gender, and other covariates\nFits with phenomena development: reducing data, eliminating noise, in order to identity local phenomena"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#discussion-questions",
    "href": "content/03-04-oakland-eda.html#discussion-questions",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nSuppose you’ve conducted this EDA because you’re working with an activist organization that promotes defunding the police and prison abolition. Should you share the preliminary findings above with your organization contacts?\nWhat influence would the following factors make to your answer?\n\nFunding: Whether you’re being paid as a consultant vs. volunteering your expertise\nValues: Your own views about policing and prisons\nRelationships: Whether you are friends with members of the activist organization and/or police\nCommunications: The degree to which you can control whether and how the organization will publish your preliminary findings\nTimeliness: Whether these findings are relevant to a pending law or policy change\n\nWhat other factors should be taken into account as you decide whether to share your findings? Or not taken into account?\nHow has this “raw data” been shaped by the journey of the data to get to us?"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#lab",
    "href": "content/03-04-oakland-eda.html#lab",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Lab",
    "text": "Lab\nThe lab related to this material is available at https://github.com/data-science-methods/lab-w06-eda."
  },
  {
    "objectID": "content/03-04-oakland-eda.html#references",
    "href": "content/03-04-oakland-eda.html#references",
    "title": "Case Study: Police Stops in Oakland",
    "section": "References",
    "text": "References\n\n\n\n\nHuebner, Marianne, Werner Vach, and Saskia le Cessie. 2016. “A Systematic Approach to Initial Data Analysis Is Good Research Practice.” The Journal of Thoracic and Cardiovascular Surgery 151 (1): 25–27. https://doi.org/10.1016/j.jtcvs.2015.09.085.\n\n\nLiboiron, Max. 2021. Pollution Is Colonialism. Duke University Press. https://books.google.com?id=NL4lEAAAQBAJ.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience.\n\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, May, 1–10. https://doi.org/10.1038/s41562-020-0858-1.\n\n\nTanweer, Anissa, Emily Kalah Gade, P. M. Krafft, and Sarah K. Dreier. 2021. “Why the Data Revolution Needs Qualitative Thinking.” Harvard Data Science Review, July. https://doi.org/10.1162/99608f92.eee0b0da."
  },
  {
    "objectID": "content/04-02-readability.html#reading",
    "href": "content/04-02-readability.html#reading",
    "title": "Readable code is reliable code",
    "section": "Reading",
    "text": "Reading\n\n(BryanCodeSmellsFeels2018?)\nPostolovski (2020)\nBuckens (2019)"
  },
  {
    "objectID": "content/04-02-readability.html#writing-code-is-writing",
    "href": "content/04-02-readability.html#writing-code-is-writing",
    "title": "Readable code is reliable code",
    "section": "Writing code is writing",
    "text": "Writing code is writing\n\nMultiple target audiences\n\nCollaborators\n(Some) reviewers and readers of the paper\nPeers who want to analyze and extend your methods\nYourself in two months/one year/five years\n\nYour code is readable to the extent that people can use it to easily and reliably\n\npredict,\ndiagnose, and\nextend your code"
  },
  {
    "objectID": "content/04-02-readability.html#code-style",
    "href": "content/04-02-readability.html#code-style",
    "title": "Readable code is reliable code",
    "section": "Code style",
    "text": "Code style\n\niNéwritteNélanguagEáconventionSéfoRîpunctuatioNøcapitalizatioNîaiDécomprehensioNébYéindicatinGéstructurE\n\nthis is what it’s like to read poorly-styled code\nconventions only work if they’re shared conventions\n\nStyle guides provide shared conventions for readable code\n\nIn-line spacing makes it easier to pick out distinguish functions, operators, and variables in a line\nReturns distinguish arguments in a function call\nIndentation corresponds to structure of complex expressions\nCommon conventions for naming, assignment reduce cognitive load\n\nTidyverse style guide: https://style.tidyverse.org/"
  },
  {
    "objectID": "content/04-02-readability.html#highlights-from-the-tidyverse-style-guide",
    "href": "content/04-02-readability.html#highlights-from-the-tidyverse-style-guide",
    "title": "Readable code is reliable code",
    "section": "Highlights from the Tidyverse style guide",
    "text": "Highlights from the Tidyverse style guide\n\nPlace all package() calls at the top of the script\nLimit your code to 80 characters per line\nUse at least 4 spaces for indenting multiline expression\n\nControl-I in RStudio will do automagic indenting\n\nIn multiline function calls, 1 argument = 1 line\nUse comments to explain “why” rather than “what” or “how”\n\nDH’s comment convention:\nSingle # is for code that isn’t currently used but might need to be brought back (eg, for debugging)\n## is for substantial comments"
  },
  {
    "objectID": "content/04-02-readability.html#spaces",
    "href": "content/04-02-readability.html#spaces",
    "title": "Readable code is reliable code",
    "section": "Spaces",
    "text": "Spaces\n\nAlways put spaces after commas, and never before (like English)\nBut not between a function name and the parentheses (like math)\nAnd surrounding infix operators (==, +, -, <-, =)\nPipes %>% |> should have a space before and be at the end of the line"
  },
  {
    "objectID": "content/04-02-readability.html#code-blocks",
    "href": "content/04-02-readability.html#code-blocks",
    "title": "Readable code is reliable code",
    "section": "Code blocks",
    "text": "Code blocks\nWhen you put a block of code in curly braces {}:\n\n{ should be the last character on a line\n} should be the first character on the line\n\n\nif (y == 0) {\n  if (x > 0) {\n    log(x)\n  } else {\n    message(\"x is negative or zero\")\n  }\n} else {\n  y^x\n}"
  },
  {
    "objectID": "content/04-02-readability.html#boolean-variables-vs.-control-flow",
    "href": "content/04-02-readability.html#boolean-variables-vs.-control-flow",
    "title": "Readable code is reliable code",
    "section": "Boolean variables vs. control flow",
    "text": "Boolean variables vs. control flow\n\nFunctions that return vectors:\n\n&, |, ==, ifelse(), dplyr::if_else()\n\nFunctions that return a single value:\n\n&&, ||, identical\n\nif (x) a else b only looks at the first (hopefully single) value of x"
  },
  {
    "objectID": "content/04-02-readability.html#references",
    "href": "content/04-02-readability.html#references",
    "title": "Readable code is reliable code",
    "section": "References",
    "text": "References\n\n\n\n\nBuckens, Wouter. 2019. “Self-Documenting Is a Myth, and How to Make Your Code Self-Documenting.” woubuc. August 3, 2019. https://blog.woubuc.be/post/self-documenting-code-is-a-myth/.\n\n\nPostolovski, Tash. 2020. “Your Code Review Checklist: 14 Things to Include.” July 6, 2020. https://www.codementor.io/blog/code-review-checklist-76q7ovkaqj."
  },
  {
    "objectID": "content/04-03-code-review.html#reading",
    "href": "content/04-03-code-review.html#reading",
    "title": "A code review checklist",
    "section": "Reading",
    "text": "Reading\n\nPostolovski (2020)\n\n\nReadability\nMaintainability\nSecurity\nSpeed and Performance\nDocumentation\nReinventing the Wheel\nReliability\nScalability\nReusability\nPatterns [style and readability]\nTest coverage and quality\nFit for purpose\nNotice what’s missing\nZoom out"
  },
  {
    "objectID": "content/04-03-code-review.html#readability-understandability",
    "href": "content/04-03-code-review.html#readability-understandability",
    "title": "A code review checklist",
    "section": "1. Readability / understandability",
    "text": "1. Readability / understandability\n\nFile naming\n\nWhat order should the scripts be run in?\n\nFollows a style guide\n\nOr at least uses consistent style\n\nWhitespace, whitespace, whitespace\nMeaningful names / self-documenting code (Buckens 2019)\nFor data science: Functionality promotes understandability"
  },
  {
    "objectID": "content/04-03-code-review.html#maintainability",
    "href": "content/04-03-code-review.html#maintainability",
    "title": "A code review checklist",
    "section": "2. Maintainability",
    "text": "2. Maintainability\n\nDRY\nAvoid hard-coded configuration\nCheck strong assumptions about structure and content of inputs\nAvoid deprecated or removed features"
  },
  {
    "objectID": "content/04-03-code-review.html#security",
    "href": "content/04-03-code-review.html#security",
    "title": "A code review checklist",
    "section": "3. Security",
    "text": "3. Security\n\nHow are passwords for websites, APIs, and sensitive data handled?"
  },
  {
    "objectID": "content/04-03-code-review.html#speed-and-performance",
    "href": "content/04-03-code-review.html#speed-and-performance",
    "title": "A code review checklist",
    "section": "4. Speed and Performance",
    "text": "4. Speed and Performance\n\nPerformance of the script\n\nAvoid (repeating) large downloads\nCache the results of slow calculations\n\nWith an easy way to clear the cache and re-run the calculations\nOr break them up in to separate steps of the pipeline\n\nUse tictoc for simple wall time checks\n\n\n\nResource use\n\nDoes this really need to be run on a compute cluster?\nHow much will this cost if it’s accidentally run 1,000 times?\nCache raw returns from web scraping and API queries"
  },
  {
    "objectID": "content/04-03-code-review.html#documentation",
    "href": "content/04-03-code-review.html#documentation",
    "title": "A code review checklist",
    "section": "5. Documentation",
    "text": "5. Documentation\n\nScripts: Section markers, signposting what’s happening and why\nFunctions: Assumptions about inputs, explanation of output\n\nConsider using roxygen2 + R’s package infrastructure\nhttps://r-pkgs.org/man.html#man-functions\n\nData: Provenance, date and time retrieved, codebook\nProject: README, NEWS"
  },
  {
    "objectID": "content/04-03-code-review.html#reinventing-the-wheel",
    "href": "content/04-03-code-review.html#reinventing-the-wheel",
    "title": "A code review checklist",
    "section": "6. Reinventing the Wheel",
    "text": "6. Reinventing the Wheel\n\nIs there a (well-maintained, checked) package for this?\nDRY"
  },
  {
    "objectID": "content/04-03-code-review.html#reliability",
    "href": "content/04-03-code-review.html#reliability",
    "title": "A code review checklist",
    "section": "7. Reliability",
    "text": "7. Reliability\nWhat happens if\n\nPackages aren’t installed?\n\nReminder: require() doesn’t stop if the package is missing\n\nData can’t be found?\nData has missing values or errors?\nData has been altered or corrupted?\nA function returns a missing or zero-length value?\nOne script in the middle of the pipeline is altered?"
  },
  {
    "objectID": "content/04-03-code-review.html#skipping-a-few",
    "href": "content/04-03-code-review.html#skipping-a-few",
    "title": "A code review checklist",
    "section": "Skipping a few",
    "text": "Skipping a few\nBecause I have less to say, not because they’re less important\n\nScalability\nReusability\nPatterns [style and readability]\nTest coverage and quality\nFit for purpose"
  },
  {
    "objectID": "content/04-03-code-review.html#notice-whats-missing",
    "href": "content/04-03-code-review.html#notice-whats-missing",
    "title": "A code review checklist",
    "section": "13. Notice what’s missing",
    "text": "13. Notice what’s missing\n\nUsing packages that haven’t been loaded\nData validation\n\nEdge cases, unexpected inputs\nMissing values\n\nError handling"
  },
  {
    "objectID": "content/04-03-code-review.html#zoom-out",
    "href": "content/04-03-code-review.html#zoom-out",
    "title": "A code review checklist",
    "section": "14. Zoom out",
    "text": "14. Zoom out\n\nDoes the code fit the analytic approach described in the paper?\nDoes the analytic approach fit the the research question?"
  },
  {
    "objectID": "content/04-03-code-review.html#references",
    "href": "content/04-03-code-review.html#references",
    "title": "A code review checklist",
    "section": "References",
    "text": "References\n\n\n\n\n\nBuckens, Wouter. 2019. “Self-Documenting Is a Myth, and How to Make Your Code Self-Documenting.” woubuc. August 3, 2019. https://blog.woubuc.be/post/self-documenting-code-is-a-myth/.\n\n\nPostolovski, Tash. 2020. “Your Code Review Checklist: 14 Things to Include.” July 6, 2020. https://www.codementor.io/blog/code-review-checklist-76q7ovkaqj."
  },
  {
    "objectID": "content/04-04-project-organization.html#reading",
    "href": "content/04-04-project-organization.html#reading",
    "title": "The life-changing magic of tidying your projects",
    "section": "Reading",
    "text": "Reading\n\nNoble (2009)\n\n\nSome data management disasters\n\nThe Economist (2011), “Video: Keith Baggerly, \"When Is Reproducibility an Ethical Issue? Genomics, Personalized Medicine, and Human Error\"” (n.d.)\nHerndon, Ash, and Pollin (2014), but you can just read Bailey and Borwein (Jon) (n.d.), Cassidy (n.d.), and/or watch Reinhart & Rogoff - Growth in a Time of Debt - EXERCISE! (2019)\nLaskowski (n.d.), Viglione (2020), Pennisi (2020)\n“How Excel May Have Caused Loss of 16,000 Covid Tests in England” (2020)"
  },
  {
    "objectID": "content/04-04-project-organization.html#dumpster-organization",
    "href": "content/04-04-project-organization.html#dumpster-organization",
    "title": "The life-changing magic of tidying your projects",
    "section": "Dumpster organization",
    "text": "Dumpster organization\n(ref:dumpster) 😱 Source: https://pbs.twimg.com/media/DFca5SRXsAAx1NA\n\n\n\n\n\n(ref:dumpster)\n\n\n\n\n\nDump all of your files into one place\nUse search tools to find what you want\nJust assume that things aren’t getting corrupted\nThe way many Gen Z students think about their files? (Chin 2021)"
  },
  {
    "objectID": "content/04-04-project-organization.html#project-organization",
    "href": "content/04-04-project-organization.html#project-organization",
    "title": "The life-changing magic of tidying your projects",
    "section": "Project organization",
    "text": "Project organization\n\nKeep your project self-contained\nLocate files quickly\nPlay nicely with version control\nSelf-document key relationships between project files\n\n\n\n\n\n\nFolder organization is your friend. I have top-level folders for teaching, coding, and writing projects. Then each project or collection of projects lives in its own folder. An ‘Archives’ folder is good for tucking old projects out of the way."
  },
  {
    "objectID": "content/04-04-project-organization.html#model-1-noblequickguideorganizing2009",
    "href": "content/04-04-project-organization.html#model-1-noblequickguideorganizing2009",
    "title": "The life-changing magic of tidying your projects",
    "section": "Model 1: Noble (2009)",
    "text": "Model 1: Noble (2009)\n\n\n\n\n\nNoble’s (2009) sample folder structure is designed for experimental biologists.\n\n\n\n\n\nIncludes code for\n\nrunning experiments\ncleaning and analyzing data\ntypesetting a LaTeX file into a PDF\n\nOther features\n\nnotebook file, updated regularly\nchronological folders for experimental runs\nbin for compiled code and source for corresponding source files"
  },
  {
    "objectID": "content/04-04-project-organization.html#model-2-some-of-dhs-projects",
    "href": "content/04-04-project-organization.html#model-2-some-of-dhs-projects",
    "title": "The life-changing magic of tidying your projects",
    "section": "Model 2: Some of DH’s projects",
    "text": "Model 2: Some of DH’s projects\nGitHub repo: https://github.com/dhicks/p_curve\n14 directories, 121 files\n.\n├── DESCRIPTION\n├── Makefile\n├── README.md\n├── _deploy.sh\n├── out\n│   ├── estimates_meta.png\n│   ├── estimates_study.png\n│   ├── fig_1_samples_young.png\n│   ├── fig_2_young_composite.png\n│   ├── fig_3_evidence_severity.png\n│   ├── fig_4_evidence_likelihood_zero.png\n│   ├── fig_5_evidence_likelihood_mix.png\n│   ├── likelihood.tex\n│   ├── linearity.png\n│   ├── linearity.tex\n│   ├── samples_schsp.png\n│   ├── samples_simonsohn.png\n│   ├── severity.tex\n│   ├── slopes.png\n│   ├── slopes.tex\n│   ├── slopes_scatter.png\n│   ├── test.png\n│   └── test_out.png\n├── p.curve\n│   ├── DESCRIPTION\n│   ├── NAMESPACE\n│   ├── R\n│   │   └── p-curve.R\n│   ├── man\n│   │   ├── draw_samples.Rd\n│   │   ├── draw_studies.Rd\n│   │   ├── flatten_to_chr.Rd\n│   │   ├── likelihood_ratio.Rd\n│   │   ├── many_metas.Rd\n│   │   ├── p_gap.Rd\n│   │   ├── p_value.Rd\n│   │   ├── qq_linear.Rd\n│   │   ├── qq_plot.Rd\n│   │   ├── qq_slope.Rd\n│   │   ├── schsp_curve.Rd\n│   │   ├── schsp_slope.Rd\n│   │   ├── simonsohn_curve.Rd\n│   │   ├── t_test.Rd\n│   │   ├── young_composite.Rd\n│   │   ├── young_curve.Rd\n│   │   └── young_slope.Rd\n│   └── p.curve.Rproj\n├── paper\n│   ├── *enviro\\ epi\n│   │   └── EE\\ Submission\\ Confirmation\\ for\\ Young's\\ p-value\\ plot\\ does\\ not\\ provide\\ evidence\\ against\\ air\\ pollution\\ hazards.eml\n│   ├── Young\\ papers.gsheet\n│   ├── Young.bib\n│   ├── cover\\ letter.pdf\n│   ├── diff.pdf\n│   ├── ehp\n│   │   ├── A\\ manuscript\\ number\\ has\\ been\\ assigned\\ to\\ Young's\\ p-value\\ plot\\ does\\ not\\ provide\\ evidence\\ against\\ air\\ pollution\\ hazards\\ -\\ [EMID_0ef854c3bb0b5cae].eml\n│   │   ├── Decision\\ on\\ EHP\\ Submission\\ EHP8013\\ -\\ [EMID_932e44ac2192c44f].eml\n│   │   ├── EHP-CFI-form.pdf\n│   │   ├── cover\\ letter.txt\n│   │   ├── paper_2020-07-31.docx\n│   │   └── title\\ page.md\n│   ├── example-refs.bib\n│   ├── fig_1_samples_young.png\n│   ├── fig_2_young_composite.png\n│   ├── fig_3_evidence_severity.png\n│   ├── fig_4_evidence_likelihood_zero.png\n│   ├── fig_5_evidence_likelihood_mix.png\n│   ├── header.yaml\n│   ├── paper.md\n│   ├── paper.pdf\n│   ├── paper.synctex.gz\n│   ├── paper.tex\n│   ├── paper.zip\n│   ├── paper_20201211.md\n│   ├── peerj\n│   │   ├── comments.md\n│   │   └── peerj.pdf\n│   ├── phil\\ med\n│   │   ├── [philmed]\\ Editor\\ Decision.eml\n│   │   └── [philmed]\\ Submission\\ Acknowledgement.eml\n│   ├── render.R\n│   ├── summary.md\n│   ├── summary.pdf\n│   ├── summary.tex\n│   ├── supplement.md\n│   ├── supplement.pdf\n│   ├── title.md\n│   ├── title.pdf\n│   ├── vancouver-superscript.csl\n│   └── wlpeerj.cls\n├── scripts\n│   ├── Makefile\n│   ├── run_metas.R\n│   ├── run_metas.html\n│   ├── run_metas_cache\n│   │   └── html\n│   │       ├── __packages\n│   │       ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.RData\n│   │       ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.rdb\n│   │       ├── power_sim_9c372ce79d0c5f5a133f461070cc735c.rdx\n│   │       ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.RData\n│   │       ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.rdb\n│   │       ├── run\\ simulations_b1dfebf278eb300e65b865f76b2893d2.rdx\n│   │       ├── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.RData\n│   │       ├── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.rdb\n│   │       └── vary_N_sim_7d1d09d59ab04fc75046799fcf7506f9.rdx\n│   ├── run_metas_files\n│   │   └── figure-html\n│   │       ├── QQ\\ linearity\\ tests-1.png\n│   │       ├── gaps-1.png\n│   │       ├── gaps-2.png\n│   │       ├── likelihood\\ analysis-1.png\n│   │       ├── likelihood\\ analysis-3.png\n│   │       ├── model\\ validation-1.png\n│   │       ├── model\\ validation-2.png\n│   │       ├── power_sim-1.png\n│   │       ├── power_sim-2.png\n│   │       ├── sample\\ plots-1.png\n│   │       ├── sample\\ plots-2.png\n│   │       ├── sample\\ plots-3.png\n│   │       ├── sample\\ plots-4.png\n│   │       ├── severity\\ analysis-1.png\n│   │       ├── severity\\ analysis-2.png\n│   │       ├── slopes-1.png\n│   │       ├── slopes-2.png\n│   │       ├── slopes-3.png\n│   │       ├── slopes-4.png\n│   │       ├── slopes-5.png\n│   │       ├── slopes-6.png\n│   │       ├── slopes-7.png\n│   │       ├── unnamed-chunk-2-1.png\n│   │       ├── unnamed-chunk-4-1.png\n│   │       └── unnamed-chunk-6-1.png\n│   └── scripts.Rproj\n└── tree.md"
  },
  {
    "objectID": "content/04-04-project-organization.html#just-the-directories",
    "href": "content/04-04-project-organization.html#just-the-directories",
    "title": "The life-changing magic of tidying your projects",
    "section": "Just the directories",
    "text": "Just the directories\n.\n├── out\n├── p.curve\n│   ├── R\n│   └── man\n├── paper\n│   ├── *enviro\\ epi\n│   ├── ehp\n│   ├── peerj\n│   └── phil\\ med\n└── scripts\n    ├── run_metas_cache\n    │   └── html\n    └── run_metas_files\n        └── figure-html\n\nscripts, paper, and out\np.curve, a little package containing the simulation code\nsimulation and analysis automatically reproduced: https://dhicks.github.io/p_curve/"
  },
  {
    "objectID": "content/04-04-project-organization.html#a-larger-text-mining-project",
    "href": "content/04-04-project-organization.html#a-larger-text-mining-project",
    "title": "The life-changing magic of tidying your projects",
    "section": "A larger text-mining project",
    "text": "A larger text-mining project\nPublished paper: https://doi.org/10.1162/qss_a_00150\nGitHub repo: https://github.com/dhicks/orus\n23 directories, 274 files (plus 160k data files)\n.\n├── Makefile\n├── ORU\\ faculty\n│   ├── ORU\\ Faculty.docx\n│   ├── ORU\\ Faculty.html\n│   ├── ORU\\ Publications.docx\n│   ├── ORU\\ Publications.fld\n│   │   ├── colorschememapping.xml\n│   │   ├── filelist.xml\n│   │   ├── header.html\n│   │   ├── image001.png\n│   │   ├── item0001.xml\n│   │   ├── props002.xml\n│   │   └── themedata.thmx\n│   ├── ORU\\ Publications.html\n│   └── auids.csv\n├── ORU\\ founding\\ dates.gsheet\n├── QSS\\ forms\n│   ├── QSS-Checklist-AcceptedManuscripts.docx\n│   ├── QSS_pub_agreement.pdf\n│   └── Quantitative\\ Science\\ Studies\\ -\\ Decision\\ on\\ Manuscript\\ ID\\ QSS-2021-0014.R2.eml\n├── R\n│   ├── api_keys.R\n│   └── hellinger.R\n├── auid\\ flow.txt\n├── data\n│   ├── *ORUs\\ -\\ DSL\\ -\\ Google\\ Drive.webloc\n│   ├── 00_UCD_2016.csv\n│   ├── 00_UCD_2017.csv\n│   ├── 00_UCD_2018.csv\n│   ├── 00_faculty_list.html\n│   ├── 00_manual_matches.csv\n│   ├── 00_publications_list.html\n│   ├── 01_departments.csv\n│   ├── 01_departments_canonical.csv\n│   ├── 01_faculty.Rds\n│   ├── 02_pubs.Rds\n│   ├── 03_codepartmentals.Rds\n│   ├── 03_dropout.Rds\n│   ├── 03_matched.Rds\n│   ├── 03_unmatched.Rds\n│   ├── 04_author_meta.Rds\n│   ├── 04_dropouts.Rds\n│   ├── 04_genderize\n│   ├── 04_namsor.Rds\n│   ├── 05_author_meta.Rds\n│   ├── 05_dept_dummies.Rds\n│   ├── 05_dropouts.Rds\n│   ├── 05_layout.Rds\n│   ├── 05_matched.Rds\n│   ├── 06_author_histories.Rds\n│   ├── 07_coauth_count.Rds\n│   ├── 07_parsed_histories.Rds\n│   ├── 08_phrases.Rds\n│   ├── 09_H.Rds\n│   ├── 09_atm.csv\n│   ├── 09_vocab.tex\n│   ├── 10_atm.csv\n│   ├── 10_atm_pc.Rds\n│   ├── 10_aytm.csv\n│   ├── 10_aytm_comp.csv\n│   ├── 10_aytm_did.csv\n│   ├── 10_model_stats.Rds\n│   ├── 10_models.Rds\n│   ├── 11_au_dept_xwalk.Rds\n│   ├── 11_departments.csv\n│   ├── 11_departments_canonical.csv\n│   ├── 11_dept_dummies.Rds\n│   ├── 11_dept_gamma.Rds\n│   ├── 11_dept_term_matrix.Rds\n│   ├── 11_oru_gamma.Rds\n│   ├── 11_oru_term_matrix.Rds\n│   ├── 11_test_train.Rds\n│   ├── 12_layout.Rds\n│   ├── author_histories [7665 entries exceeds filelimit, not opening dir]\n│   ├── authors_meta [6020 entries exceeds filelimit, not opening dir]\n│   ├── docs [145144 entries exceeds filelimit, not opening dir]\n│   ├── ldatuning_results\n│   │   ├── tuningResult_comp.Rds\n│   │   ├── tuningResult_comp.docx\n│   │   ├── tuningResult_comp.pdf\n│   │   ├── tuningResult_did.Rds\n│   │   └── tuningResult_did.pdf\n│   ├── ldatuning_results-20190415T164055Z-001.zip\n│   ├── parsed_blocks [430 entries exceeds filelimit, not opening dir]\n│   ├── pubs [282 entries exceeds filelimit, not opening dir]\n│   └── temp\n├── interdisciplinarity\\ project\\ notes.gdoc\n├── notes.txt\n├── paper\n│   ├── QSS_a_00150-Hicks_Proof1.pdf\n│   ├── apa-6th-edition.csl\n│   ├── cover\\ letter.txt\n│   ├── diff.pdf\n│   ├── header.yaml\n│   ├── img\n│   │   ├── ORU_DAG.png\n│   │   ├── cites_regression.png\n│   │   ├── coauths_regression.png\n│   │   ├── conceptual_model.png\n│   │   ├── dept_dist_fixed_reg.png\n│   │   ├── dept_dist_reg.png\n│   │   ├── dept_gamma.png\n│   │   ├── dept_hell_net.png\n│   │   ├── dept_hell_net_50.png\n│   │   ├── entropies.png\n│   │   ├── entropies_selected.png\n│   │   ├── entropy_regression.png\n│   │   ├── gender.png\n│   │   ├── mds.png\n│   │   ├── mds_dept.png\n│   │   ├── network.png\n│   │   ├── oru_dept_entropy.png\n│   │   ├── oru_dept_min_dist.png\n│   │   ├── oru_dept_min_dist_ridges.png\n│   │   ├── oru_dept_network.png\n│   │   ├── oru_dept_org_dist.png\n│   │   ├── oru_dept_org_dist_ridges.png\n│   │   ├── oru_gamma.png\n│   │   ├── pub_regression.png\n│   │   └── sample.png\n│   ├── lit\\ review\\ notes.txt\n│   ├── oru_paper.aux\n│   ├── oru_paper.log\n│   ├── oru_paper.md\n│   ├── oru_paper.out\n│   ├── oru_paper.pdf\n│   ├── oru_paper.synctex.gz\n│   ├── oru_paper.tex\n│   ├── oru_paper.zip\n│   ├── oru_paper_20200616.pdf\n│   ├── oru_paper_20210805.pdf\n│   ├── oru_project.bib\n│   ├── oru_project.yaml\n│   ├── response1.gdoc\n│   ├── response1.pdf\n│   ├── response2.gdoc\n│   ├── response2.pdf\n│   ├── scraps\n│   │   ├── Hellinger.md\n│   │   ├── Holbrook.md\n│   │   ├── table.md\n│   │   └── table.pdf\n│   ├── supplement.md\n│   └── supplement.pdf\n├── plots\n│   ├── 12_beta.tex\n│   ├── 12_cites_regression.png\n│   ├── 12_coauths_regression.png\n│   ├── 12_dept_dist_fixed_reg.png\n│   ├── 12_dept_dist_reg.png\n│   ├── 12_dept_gamma.png\n│   ├── 12_dept_hell_net.png\n│   ├── 12_dept_hell_net_50.png\n│   ├── 12_dept_topics.png\n│   ├── 12_entropies.png\n│   ├── 12_entropies_selected.png\n│   ├── 12_entropy_regression.png\n│   ├── 12_gender.png\n│   ├── 12_mds.png\n│   ├── 12_mds_dept.png\n│   ├── 12_mds_wide.png\n│   ├── 12_network.png\n│   ├── 12_oru_dept_entropy.png\n│   ├── 12_oru_dept_mean_dist.png\n│   ├── 12_oru_dept_mean_dist_ridges.png\n│   ├── 12_oru_dept_min_dist.png\n│   ├── 12_oru_dept_min_dist_ridges.png\n│   ├── 12_oru_dept_network.png\n│   ├── 12_oru_dept_org_dist.png\n│   ├── 12_oru_dept_org_dist_ridges.png\n│   ├── 12_oru_entropy.png\n│   ├── 12_oru_gamma.png\n│   ├── 12_pub_regression.png\n│   ├── 12_sample.png\n│   └── ORU_DAG.png\n├── presentations\n│   └── 2019-06-07\\ for\\ Paul\\ Dodd.gslides\n├── questions\\ for\\ jane.md\n├── scripts\n│   ├── 01_parse_faculty_list.R\n│   ├── 02_Scopus_search_results.R\n│   ├── 03_match.R\n│   ├── 03_matched.csv\n│   ├── 04_author_meta.R\n│   ├── 05_filtering.R\n│   ├── 06_author_histories.R\n│   ├── 07_complete_histories.R\n│   ├── 08_text_annotation.R\n│   ├── 09_build_vocab.R\n│   ├── 10_topic_modeling.R\n│   ├── 11_depts.R\n│   ├── 11_depts.html\n│   ├── 12_analysis\\ copy.html\n│   ├── 12_analysis-matched.html\n│   ├── 12_analysis.R\n│   ├── 12_analysis.html\n│   ├── 12_analysis_cache\n│   │   └── html\n│   │       ├── __packages\n│   │       ├── mds_viz_efd9009c794d667852b2549df2bccf96.RData\n│   │       ├── mds_viz_efd9009c794d667852b2549df2bccf96.rdb\n│   │       ├── mds_viz_efd9009c794d667852b2549df2bccf96.rdx\n│   │       ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.RData\n│   │       ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.rdb\n│   │       ├── network_c410cd78a4c339cdc4acd1d66c6c5e07.rdx\n│   │       ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.RData\n│   │       ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.rdb\n│   │       ├── silhouette_3170ef648aba325d2ce8c9be48c52e53.rdx\n│   │       ├── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.RData\n│   │       ├── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdb\n│   │       └── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdx\n│   ├── 12_analysis_files\n│   │   └── figure-html\n│   │       ├── author-dept\\ distance-1.png\n│   │       ├── author-dept\\ distance-2.png\n│   │       ├── author-dept\\ distance-3.png\n│   │       ├── author-dept\\ distance-4.png\n│   │       ├── author-dept\\ distance-5.png\n│   │       ├── desc_plots_tabs-1.png\n│   │       ├── desc_plots_tabs-2.png\n│   │       ├── desc_plots_tabs-3.png\n│   │       ├── desc_plots_tabs-4.png\n│   │       ├── h3-1.png\n│   │       ├── h3-2.png\n│   │       ├── h3-3.png\n│   │       ├── h3-4.png\n│   │       ├── h3-5.png\n│   │       ├── h3-6.png\n│   │       ├── mds_viz-1.png\n│   │       ├── mds_viz-10.png\n│   │       ├── mds_viz-11.png\n│   │       ├── mds_viz-12.png\n│   │       ├── mds_viz-13.png\n│   │       ├── mds_viz-14.png\n│   │       ├── mds_viz-2.png\n│   │       ├── mds_viz-3.png\n│   │       ├── mds_viz-4.png\n│   │       ├── mds_viz-5.png\n│   │       ├── mds_viz-6.png\n│   │       ├── mds_viz-7.png\n│   │       ├── mds_viz-8.png\n│   │       ├── mds_viz-9.png\n│   │       ├── network-1.png\n│   │       ├── network-2.png\n│   │       ├── productivity-1.png\n│   │       ├── productivity-2.png\n│   │       ├── productivity-3.png\n│   │       ├── productivity-4.png\n│   │       ├── productivity-5.png\n│   │       ├── productivity-6.png\n│   │       ├── productivity-7.png\n│   │       ├── productivity-8.png\n│   │       ├── productivity-9.png\n│   │       ├── silhouette-1.png\n│   │       ├── topic_models-1.png\n│   │       ├── topic_models-10.png\n│   │       ├── topic_models-11.png\n│   │       ├── topic_models-12.png\n│   │       ├── topic_models-13.png\n│   │       ├── topic_models-2.png\n│   │       ├── topic_models-3.png\n│   │       ├── topic_models-4.png\n│   │       ├── topic_models-5.png\n│   │       ├── topic_models-6.png\n│   │       ├── topic_models-7.png\n│   │       ├── topic_models-8.png\n│   │       ├── topic_models-9.png\n│   │       ├── topic_viz-1.png\n│   │       └── topic_viz-2.png\n│   ├── api_key.R\n│   └── scraps\n│       ├── 02_parse_pubs_list.R\n│       ├── 03_coe_pubs.R\n│       ├── 03_match_auids.R\n│       ├── 07.R\n│       ├── 12_regressions.R\n│       ├── BML-CMSI\\ deep\\ dive.R\n│       ├── Hellinger_low_memory.R\n│       ├── dept_hell_net.R\n│       ├── divergence\\ against\\ lagged\\ distributions.R\n│       ├── exploring\\ topics.R\n│       ├── fractional_authorship.R\n│       ├── hellinger.R\n│       ├── model_scratch.R\n│       ├── multicore.R\n│       ├── net_viz.R\n│       ├── prcomp.R\n│       ├── propensity.R\n│       ├── rs_diversity.R\n│       ├── spacyr.R\n│       ├── topic\\ counts\\ rather\\ than\\ entropies.R\n│       ├── topic_cosine_sim.R\n│       ├── unit-level.R\n│       ├── weighted\\ regression.R\n│       ├── word-topic_distance.R\n│       ├── xx_construct_samples.R\n│       └── xx_oru_complete_histories.R\n└── tree.md"
  },
  {
    "objectID": "content/04-04-project-organization.html#just-the-directories-1",
    "href": "content/04-04-project-organization.html#just-the-directories-1",
    "title": "The life-changing magic of tidying your projects",
    "section": "Just the directories",
    "text": "Just the directories\n.\n├── ORU\\ faculty\n│   └── ORU\\ Publications.fld\n├── QSS\\ forms\n├── R\n├── data\n│   ├── author_histories\n│   ├── authors_meta\n│   ├── docs\n│   ├── ldatuning_results\n│   ├── parsed_blocks\n│   ├── pubs\n│   └── temp\n├── paper\n│   ├── img\n│   └── scraps\n├── plots\n├── presentations\n└── scripts\n    ├── 12_analysis_cache\n    │   └── html\n    ├── 12_analysis_files\n    │   └── figure-html\n    └── scraps"
  },
  {
    "objectID": "content/04-04-project-organization.html#dhs-project-template",
    "href": "content/04-04-project-organization.html#dhs-project-template",
    "title": "The life-changing magic of tidying your projects",
    "section": "DH’s Project Template",
    "text": "DH’s Project Template\n\nhttps://github.com/dhicks/project_template\nConfigured as a GitHub “template,” making it easy to create new repositories for new projects\nDesignated folders for data, plots/outputs, and utility functions"
  },
  {
    "objectID": "content/04-04-project-organization.html#a-reminder-on-paths",
    "href": "content/04-04-project-organization.html#a-reminder-on-paths",
    "title": "The life-changing magic of tidying your projects",
    "section": "A reminder on paths",
    "text": "A reminder on paths\n\nWindows and Unix-based systems write paths differently\nUse file.path() or the here package to construct paths\n.. in a path means “go up to the parent folder”\n\nso ../data/00_raw_data.csv goes up one level (eg, from the scripts folder), then down to the data folder, then the file 00_raw_data.csv"
  },
  {
    "objectID": "content/04-04-project-organization.html#references",
    "href": "content/04-04-project-organization.html#references",
    "title": "The life-changing magic of tidying your projects",
    "section": "References",
    "text": "References\n\n\n\n\nBailey, David H., and Jonathan Borwein (Jon). n.d. “The Reinhart-Rogoff Error – or How Not to Excel at Economics.” The Conversation. Accessed May 16, 2020. http://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646.\n\n\nCassidy, John. n.d. “The Reinhart and Rogoff Controversy: A Summing Up.” The New Yorker. Accessed September 27, 2020. https://www.newyorker.com/news/john-cassidy/the-reinhart-and-rogoff-controversy-a-summing-up.\n\n\nChin, Monica. 2021. “Students Who Grew up with Search Engines Might Change STEM Education Forever.” The Verge. September 22, 2021. https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” Cambridge Journal of Economics 38 (2): 257–79. https://doi.org/10.1093/cje/bet075.\n\n\n“How Excel May Have Caused Loss of 16,000 Covid Tests in England.” 2020. The Guardian. October 5, 2020. http://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england.\n\n\nLaskowski, Kate. n.d. “What to Do When You Don’t Trust Your Data Anymore – Laskowski Lab at UC Davis.” Accessed January 29, 2020. https://laskowskilab.faculty.ucdavis.edu/2020/01/29/retractions/.\n\n\nNoble, William Stafford. 2009. “A Quick Guide to Organizing Computational Biology Projects.” PLOS Computational Biology 5 (7): e1000424. https://doi.org/10.1371/journal.pcbi.1000424.\n\n\nPennisi, Elizabeth. 2020. “Prominent Spider Biologist Spun a Web of Questionable Data.” Science 367 (6478): 613–14. https://doi.org/10.1126/science.367.6478.613.\n\n\nReinhart & Rogoff - Growth in a Time of Debt - EXERCISE! 2019. https://www.youtube.com/watch?v=ItGMz0ERvcw.\n\n\nThe Economist. 2011. “An Array of Errors,” September 10, 2011. https://www.economist.com/science-and-technology/2011/09/10/an-array-of-errors.\n\n\n“Video: Keith Baggerly, \"When Is Reproducibility an Ethical Issue? Genomics, Personalized Medicine, and Human Error\".” n.d. Accessed September 23, 2020. http://www.birs.ca/events/2013/5-day-workshops/13w5083/videos/watch/201308141121-Baggerly.html.\n\n\nViglione, Giuliana. 2020. “‘Avalanche’ of Spider-Paper Retractions Shakes Behavioural-Ecology Community.” Nature, February. https://doi.org/10.1038/d41586-020-00287-y."
  },
  {
    "objectID": "content/04-05-data-management.html#reading",
    "href": "content/04-05-data-management.html#reading",
    "title": "Managing and publishing data",
    "section": "Reading",
    "text": "Reading\n\nHudon (2018)\nWilkinson et al. (2016)"
  },
  {
    "objectID": "content/04-05-data-management.html#the-first-rule-of-data-management",
    "href": "content/04-05-data-management.html#the-first-rule-of-data-management",
    "title": "Managing and publishing data",
    "section": "The first rule of data management",
    "text": "The first rule of data management\nDo not edit your data."
  },
  {
    "objectID": "content/04-05-data-management.html#documentation",
    "href": "content/04-05-data-management.html#documentation",
    "title": "Managing and publishing data",
    "section": "Documentation",
    "text": "Documentation\n\nMany social science fields have a tradition of writing codebooks for their data\n\nStanford Open Policing codebook\n“Codebook-like summary” of the covdata package, automatically generated using skimr\nCaitlin Hudon’s approach (Hudon 2018)\n\nTable and field name, both verbatim\nField example value\nNotes for both table and field\n\n\n\n(ref:Hudson-tbl) Example of Caitlin Hudon’s approach to building a data dictionary. Source: https://caitlinhudon.com/2018/10/30/data-dictionaries/\n\n\n\n\n\n(ref:Hudson-tbl)"
  },
  {
    "objectID": "content/04-05-data-management.html#questions-a-codebook-should-answer",
    "href": "content/04-05-data-management.html#questions-a-codebook-should-answer",
    "title": "Managing and publishing data",
    "section": "Questions a codebook should answer",
    "text": "Questions a codebook should answer\n\nWhat does this field mean? How should I use it?\nWhat is the data [journey]?\n\nWhere does this data come from?\nHow exactly is it collected?\nHow often is it updated?\nWhere does it go next?\n\nWhat does the data in this field actually look like?\nAre there any caveats to keep in mind when using this data?\nWhere can I go for more information?\n\n(Hudon 2018)"
  },
  {
    "objectID": "content/04-05-data-management.html#major-codebook-elements",
    "href": "content/04-05-data-management.html#major-codebook-elements",
    "title": "Managing and publishing data",
    "section": "Major codebook elements",
    "text": "Major codebook elements\n(https://afit-r.github.io/codebook)\n\nOriginal source of the data\nSampling information\n\nWhere and how the data were generated\n\nVariable-level metadata and summaries\nStructure of the data"
  },
  {
    "objectID": "content/04-05-data-management.html#data-management-plans",
    "href": "content/04-05-data-management.html#data-management-plans",
    "title": "Managing and publishing data",
    "section": "Data management plans",
    "text": "Data management plans\n\nMuch like a research plan, data management plans provide an overview of the steps you’ll take to gather, publish, and maintain your data\n\nSince 2011, NSF has required a 2-page data management plan for most types of proposals\n\nExamples and resources\n\nUCM Library \nUCSD NSF examples\n\nSBE example 1\nSBE example 2\n\nNSF policy summary\n\nSBE-specific guidance"
  },
  {
    "objectID": "content/04-05-data-management.html#data-management-plan-common-elements",
    "href": "content/04-05-data-management.html#data-management-plan-common-elements",
    "title": "Managing and publishing data",
    "section": "Data management plan: Common elements",
    "text": "Data management plan: Common elements\n\nWho is responsible for data management\nWho else will have access to which data\nHow data will be collected\nData formatting standards\nWhether and how data will be archived and made available for reuse"
  },
  {
    "objectID": "content/04-05-data-management.html#fair-principles-for-published-data",
    "href": "content/04-05-data-management.html#fair-principles-for-published-data",
    "title": "Managing and publishing data",
    "section": "FAIR principles for published data",
    "text": "FAIR principles for published data\n\nFindable\n\nF1. (meta)data are assigned a globally unique and persistent identifier\nF2. data are described with rich metadata (defined by R1 below)\nF3. metadata clearly and explicitly include the identifier of the data it describes\nF4. (meta)data are registered or indexed in a searchable resource\n\nAccessible\n\nA1. (meta)data are retrievable by their identifier using a standardized communications protocol\n\nA1.1 the protocol is open, free, and universally implementable\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\n\nA2. metadata are accessible, even when the data are no longer available\n\nInteroperable\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nI2. (meta)data use vocabularies that follow FAIR principles\nI3. (meta)data include qualified references to other (meta)data\n\nReusable\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes\n\nR1.1. (meta)data are released with a clear and accessible data usage license\nR1.2. (meta)data are associated with detailed provenance\nR1.3. (meta)data meet domain-relevant community standards"
  },
  {
    "objectID": "content/04-05-data-management.html#dois-for-data",
    "href": "content/04-05-data-management.html#dois-for-data",
    "title": "Managing and publishing data",
    "section": "DOIs for data",
    "text": "DOIs for data\n\nInstructions for OSF\nNotes for Zenodo\n\nZenodo also plays nicely with GitHub for minting DOIs for code\n\nCitation models at Harvard Dataverse"
  },
  {
    "objectID": "content/04-05-data-management.html#references",
    "href": "content/04-05-data-management.html#references",
    "title": "Managing and publishing data",
    "section": "References",
    "text": "References\n\n\n\n\nHudon, Caitlin. 2018. “Field Notes: Building Data Dictionaries.” Haystacks. October 30, 2018. https://caitlinhudon.com/2018/10/30/data-dictionaries/.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1, 1): 1–9. https://doi.org/10.1038/sdata.2016.18."
  },
  {
    "objectID": "content/04-07-renv.html#renv",
    "href": "content/04-07-renv.html#renv",
    "title": "Tracking package versions with renv",
    "section": "renv",
    "text": "renv\n\nrenv homepage\nTwo date-based alternatives\n\ncheckpoint\ngroundhog"
  },
  {
    "objectID": "content/04-07-renv.html#the-problem-renv-tries-to-solve",
    "href": "content/04-07-renv.html#the-problem-renv-tries-to-solve",
    "title": "Tracking package versions with renv",
    "section": "The problem renv tries to solve",
    "text": "The problem renv tries to solve\n(ref:datacolada) dplyr 0.5.0 introduced a breaking change to distinct() in June 2016. Source: https://datacolada.org/95\n\n\n\n\n\n(ref:datacolada)"
  },
  {
    "objectID": "content/04-07-renv.html#how-r-locates-packages",
    "href": "content/04-07-renv.html#how-r-locates-packages",
    "title": "Tracking package versions with renv",
    "section": "How R locates packages",
    "text": "How R locates packages\n\n.libPaths()\n\n[1] \"/Library/Frameworks/R.framework/Versions/4.1/Resources/library\""
  },
  {
    "objectID": "content/04-07-renv.html#renv-workflow",
    "href": "content/04-07-renv.html#renv-workflow",
    "title": "Tracking package versions with renv",
    "section": "renv workflow",
    "text": "renv workflow\n\nInitialize renv for a project with renv::init()\nTrack renv.lock in version control\nrenv::snapshot() to update the lockfile\nrenv::restore() to install local copies of the packages to match the lockfile"
  },
  {
    "objectID": "content/04-07-renv.html#example-with-the-learning-make-project",
    "href": "content/04-07-renv.html#example-with-the-learning-make-project",
    "title": "Tracking package versions with renv",
    "section": "Example: with the learning-make project",
    "text": "Example: with the learning-make project\n\nCheck .libPaths()\nrenv::init() (and snapshot()?)\nWhat did this do?\n\n.libPaths()\ngit status\n\nDelete renv/library and renv::restore()"
  }
]