[
  {
    "objectID": "content/01-01-intro.html#land-acknowledgements",
    "href": "content/01-01-intro.html#land-acknowledgements",
    "title": "Introduction",
    "section": "Land acknowledgements",
    "text": "Land acknowledgements\n\nCampus land acknowledgment\nWe pause to acknowledge all local indigenous peoples, including the Yokuts and Miwuk, who inhabited this land. We embrace their continued connection to this region and thank them for allowing us to live, work, learn, and collaborate on their traditional homeland. Let us now take a moment of silence to pay respect to their elders and to all Yokuts and Miwuk people, past and present.\n\n\nInstructor’s land acknowledgment\nUC Merced and the City of Merced are on the traditional territory of the Yokut people. This land was stolen by Spanish, Mexican, and American settlers through acts of slavery and genocide. In addition, UC Merced is strongly associated with Ahwahne, known as Yosemite Valley. This valley was the traditional home of the Ahwahnechee people, who were the victims of some especially horrific, state-sponsored genocidal acts. For more on the history of Ahwahne, see https://tinyurl.com/y879jw8s. For more information on land acknowledgments, see https://native-land.ca."
  },
  {
    "objectID": "content/01-01-intro.html#about-the-instructor",
    "href": "content/01-01-intro.html#about-the-instructor",
    "title": "Introduction",
    "section": "About the instructor",
    "text": "About the instructor\nDan Hicks is a philosopher turned data scientist turned philosopher.\nI use they/them pronouns and identify as nonbinary. I grew up in Placerville, about two hours north of Merced in the Sierra Foothills. One branch of my family came to California during the Gold Rush, so I identify heavily as a Californian and have some complicated feelings about the genocide. I finished my PhD in philosophy of science at Notre Dame in 2012. After that I worked in a series of research positions in academia and the federal government. During 2015-2019 I was using data science methods at least half-time. I joined the faculty at UC Merced in Fall 2019.\n\nEmail: dhicks4@ucmerced.edu\nStudent hours: By appointment: https://doodle.com/mm/danhicks/office-hours\nWebsite: https://dhicks.github.io/"
  },
  {
    "objectID": "content/01-01-intro.html#what-this-course-isnt-and-is",
    "href": "content/01-01-intro.html#what-this-course-isnt-and-is",
    "title": "Introduction",
    "section": "What this course isn’t, and is",
    "text": "What this course isn’t, and is\nIs Not:\n\na statistics course (in the way you think)\na general introduction to software engineering\na basic introduction to R\nan introduction to machine learning or AI\n\nIs:\n\nan introduction to data science\nabout exploratory data analysis, data management, reproducibility — and a little philosophy of science\nhabituation to some good software engineering practices that are especially valuable for data science work"
  },
  {
    "objectID": "content/01-01-intro.html#learning-outcomes",
    "href": "content/01-01-intro.html#learning-outcomes",
    "title": "Introduction",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of the course, students will be able to\n\nApply concepts from software engineering and philosophy of science to methodological decisions in data science (CIS PLO 2 and 4; PSY PLO 2 and 6), such as\n\ndebugging techniques and functional programming\nexploratory data analysis and the data-phenomenon-theory distinction\nreproducibility vs. replicability\ndata justice\n\nUse exploratory data analysis techniques and tools to identify potential data errors and potential phenomena for further analysis (CIS PLO 2 and 4; PSY PLO 2 and 6)\nManage data, analysis, and outputs for reproducibility using practices such as data management, clear directory structure, self-documenting code, version control, and code review (CIS PLO 2, 3, 4; PSY PLO 2, 4, 6)\nIdentify ethical responsibilities to data providers and subjects and take these responsibilities into account during data collection, analysis, and communication (CIS PLO 2 and 3; PSY PLO 4 and 6)\nIdentify and justify key methodological decisions, analysis practices, exploratory data analysis findings, and ethical responsibilities to data subjects in written and oral media (CIS PLO 3; PSY PLO 4)"
  },
  {
    "objectID": "content/01-01-intro.html#prerequisites",
    "href": "content/01-01-intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course assumes basic competence with introductory R.\n\n“Introductory R”\n\nLessons 1-6 of the Carpentries “R for Social Scientists” curriculum\n\nInstalling R and packages\nWorking in the R Studio IDE\nCommon data types\nReading and writing CSV files\nTidyverse R: mutate(), filter(), select(); plotting with ggplot2\n\n\n“Basic competence”\n\nGiven time and a reference (cheatsheet, Stack Exchange, partner, mentor) you can figure out how to solve a problem"
  },
  {
    "objectID": "content/01-01-intro.html#course-materials",
    "href": "content/01-01-intro.html#course-materials",
    "title": "Introduction",
    "section": "Course materials",
    "text": "Course materials\n\nCourse website: https://data-science-methods.github.io/\nAll readings are linked on the schedule\nLecture notes takes you to a pretty version of my slides and notes for class\nCheatsheets might be useful, depending on how much R background you have"
  },
  {
    "objectID": "content/01-01-intro.html#requirements",
    "href": "content/01-01-intro.html#requirements",
    "title": "Introduction",
    "section": "Requirements",
    "text": "Requirements\n\nClass time\n\nMix of lecture/live coding, seminar-style discussion, and work time\n\nLabs\n\n6, done in pairs/small groups and submitted via GitHub\n\nSemester-long project\n\nPracticing ideas from the course on a data set of your choice"
  },
  {
    "objectID": "content/01-01-intro.html#references",
    "href": "content/01-01-intro.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/01-02-setup.html#checklist",
    "href": "content/01-02-setup.html#checklist",
    "title": "Software, hardware, and accounts",
    "section": "Checklist",
    "text": "Checklist\n\nR 4.1 or higher\nRStudio\nPackage manager for your OS\ngit and GitKraken\npandoc or Quarto\nGitHub account"
  },
  {
    "objectID": "content/01-02-setup.html#hardware",
    "href": "content/01-02-setup.html#hardware",
    "title": "Software, hardware, and accounts",
    "section": "Hardware",
    "text": "Hardware\n\nMany sessions will involve writing code in class\nTablets (iPad, Surface) will only work if you’re using them to access a cloud computing setup\nI recommend that your machine have at least\n\n200 GB free hard drive space\n8 GB RAM\nIntel i5 processor\nOperating system capable of running R 4.1 or higher\n\nMac: 10.13 (High Sierra) or more recent\nWindows: See here\nIf you’re running Linux I’m going to assume you can figure this out yourself"
  },
  {
    "objectID": "content/01-02-setup.html#accessibility",
    "href": "content/01-02-setup.html#accessibility",
    "title": "Software, hardware, and accounts",
    "section": "Accessibility",
    "text": "Accessibility\nI chose the tools and platforms for this course in part because they’re industry-standard. If you pursue a career as a data scientist in industry, you’ll be expected to use GitHub (or something similar) on a daily basis. And RStudio is by far the most commonly used IDE (“integrated development environment”) for R.\nHowever, like many other technologies, they were originally developed using ableist assumptions about “normal” computer users. In response to criticism, the developers of these systems and tools have gone back and made their technologies more accessible. But there may still be barriers to accessibility that I have not anticipated.\nIf you encounter a barrier to participating in this course — even a small inconvenience — please let me know. Similarly, if you have ideas for making the course more accessible, please share them with me."
  },
  {
    "objectID": "content/01-02-setup.html#r-and-rstudio",
    "href": "content/01-02-setup.html#r-and-rstudio",
    "title": "Software, hardware, and accounts",
    "section": "R and RStudio",
    "text": "R and RStudio\nR and RStudio\n\n\n\nR is updated regularly, and there’s a breaking change about once per year\nRStudio also regularly gets new features, though breaking changes are less frequent\nFor consistency in class, I encourage you to use the latest version of both R and RStudio\nI use rig to switch between different versions of R"
  },
  {
    "objectID": "content/01-02-setup.html#compilers",
    "href": "content/01-02-setup.html#compilers",
    "title": "Software, hardware, and accounts",
    "section": "Compilers",
    "text": "Compilers\nSometimes you’ll need to compile a package in order to install it.\n\nMac\n\nIn a terminal, simply enter\n\nsudo xcode-select --install\nYou’ll need to enter an administrator password\n\nIf you also need a Fortran compiler\n(personally I’ve never needed one)\n\nWindows\n\nRtools42 (R 4.2 and higher) or\nRtools4 (older versions of R)\n\nLinux\n\nI’m pretty sure Linux and the like come with the necessary compilers"
  },
  {
    "objectID": "content/01-02-setup.html#os",
    "href": "content/01-02-setup.html#os",
    "title": "Software, hardware, and accounts",
    "section": "OS",
    "text": "OS\n\nAll of the tools we’ll use are available on all major OSes\nI have much less experience debugging Windows and Linux, but will do what I can"
  },
  {
    "objectID": "content/01-02-setup.html#package-manager",
    "href": "content/01-02-setup.html#package-manager",
    "title": "Software, hardware, and accounts",
    "section": "Package manager",
    "text": "Package manager\n\nSoftware to facilitate installing, upgrading, and removing other software\nHighly recommended for installing git, pandoc, and some other tools we’ll use later\nMac OS: Homebrew or MacPorts\n\nPros and cons of these two main options\nI use Homebrew\n\nWindows: Chocolatey\n\nI guess there are others, but this is what previous students have recommended\n\nEither: Anaconda is popular among Python folks, and works as a package manager for multiple OSes\n\nBut sometimes packages aren’t up to date\nEx: Most recent version of R appears to be 3.6.0, released April 2019\n\n\n\nOnce your package manager is ready to go, install git and pandoc. (You might consider Quarto, which includes pandoc and possibly also LaTeX.)\n\nMac, Homebrew:\nbrew install git\nbrew install pandoc\nWindows, Chocolatey:\n\nNeeds to be in elevated/administrator mode\nIf prompted, Yes to running additional scripts\n\nchoco install git\nchoco install pandoc"
  },
  {
    "objectID": "content/01-02-setup.html#latex",
    "href": "content/01-02-setup.html#latex",
    "title": "Software, hardware, and accounts",
    "section": "LaTeX",
    "text": "LaTeX\n\nWe don’t use it directly, but it turns Rmarkdown into PDFs\nIf you have no idea what that meant, don’t worry about it for now\nI recommend the TinyTeX distribution, which you can install via R:\ninstall.packages('tinytex')\ntinytex::install_tinytex()"
  },
  {
    "objectID": "content/01-02-setup.html#github-and-gitkraken",
    "href": "content/01-02-setup.html#github-and-gitkraken",
    "title": "Software, hardware, and accounts",
    "section": "GitHub and GitKraken",
    "text": "GitHub and GitKraken\n\nYou’ll need\n\na GitHub account,\nthe GitHub Student Developer Pack, and\nGitKraken\n\nAfter you have access to the Student Developer Pack, activate GitHub integration in GitKraken. This should enable all of GitKraken’s Pro features.\n\n\n\n\nGitHub is owned by Microsoft\nAnd there are concerns about\n\nhow it mined public repositories to develop a code completion tool, and\na recent change to its privacy policy for Enterprise (business) customers\n\nBut it offers some key features that I want to use in this course\nYou can do everything this course requires in private repositories, so long as you give access to the people who need access"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#reading",
    "href": "content/01-03-data-sci-wtf.html#reading",
    "title": "Data Science: What and why",
    "section": "Reading",
    "text": "Reading\n\nWilson et al. (2017)\nMcElreath (2020)"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#discussion-question",
    "href": "content/01-03-data-sci-wtf.html#discussion-question",
    "title": "Data Science: What and why",
    "section": "Discussion question",
    "text": "Discussion question\nWhy did you decide to take a class called “data science”?"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#standard-definition",
    "href": "content/01-03-data-sci-wtf.html#standard-definition",
    "title": "Data Science: What and why",
    "section": "Standard definition",
    "text": "Standard definition\n\n\n\nData science, defined as the intersection of CS, stats, and “business knowledge.” Source: https://www.kdnuggets.com/2020/08/top-10-lists-data-science.html\n\n\n\nThe intersection of computer science/software engineering, statistics, and “business knowledge”\nBut this defines data science in terms of tools and techniques, not epistemic and practical goals. Compare:\n\nAn ecologist is someone who spends most of their time collecting specimens in the field and processing them in a lab, vs. \nAn ecologist is someone who studies interactions among organisms and their environment"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#discussion-questions",
    "href": "content/01-03-data-sci-wtf.html#discussion-questions",
    "title": "Data Science: What and why",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nWhat are the epistemic and practical goals of your scientific field?\nHow do you think “data science” will be useful for pursuing those goals?"
  },
  {
    "objectID": "content/01-03-data-sci-wtf.html#references",
    "href": "content/01-03-data-sci-wtf.html#references",
    "title": "Data Science: What and why",
    "section": "References",
    "text": "References\n\n\n\n\nMcElreath, Richard, dir. 2020. Science as Amateur Software Development. https://www.youtube.com/watch?v=zwRdO9_GGhY.\n\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510."
  },
  {
    "objectID": "content/02-02-git.html#some-motivating-examples",
    "href": "content/02-02-git.html#some-motivating-examples",
    "title": "Git for version control",
    "section": "Some motivating examples",
    "text": "Some motivating examples\n\n\nWhile working on your analysis code, you accidentally erase the first 35 lines of the script. You only discover this three days later, when you restart R and try to run the script from the top.\nThe code you wrote last week is suddenly giving you different results. A bunch of variables are missing, and the standard errors are huge. When you’re talking with your collaborator, he says that he was cleaning the data a few days ago and everything seemed fine. When you go to check, you discover that he’s replaced your 827 survey responses with 6 group summaries.\n\n\n\n\n\n\n\nPiled Higher and Deeper comic on file names. Source: https://phdcomics.com/comics.php?f=1531\n\n\n\nYou’re working on a paper with two coauthors. You prepare the final draft to send for submission: paper final.docx. But one of your coauthors discovers a typo. Now it’s paper final fixed typo.docx. Another realizes six references are missing. paper final fixed typo refs.docx. That’s getting confusing so you change it to paper 2 Aug 2021.docx. Once it comes back from review you need to make revisions. Now you have paper 30 Jan 2022.docx and, after your collaborators make their changes, paper 12 February 2022 DJH.docx and paper 12 February 20222 final.docx."
  },
  {
    "objectID": "content/02-02-git.html#version-control",
    "href": "content/02-02-git.html#version-control",
    "title": "Git for version control",
    "section": "Version control",
    "text": "Version control\n\nBasic idea: Tools for tracking and reversing changes to code over time\nUseful for identifying and reversing breaking changes\nImplementations upload to cloud, track who contributes code, control who can suggest vs. actually change code\nGood for collaboration, publishing code"
  },
  {
    "objectID": "content/02-02-git.html#git",
    "href": "content/02-02-git.html#git",
    "title": "Git for version control",
    "section": "Git",
    "text": "Git\n\nOne of many version control systems\nVery popular in part thanks to GitHub, which provides free hosting git repositories\n\nResources for students (and teachers): https://education.github.com/"
  },
  {
    "objectID": "content/02-02-git.html#gitting-started",
    "href": "content/02-02-git.html#gitting-started",
    "title": "Git for version control",
    "section": "Gitting started",
    "text": "Gitting started\n\ngit is very hard\nWe’re going to use the GitKraken GUI to ease into things"
  },
  {
    "objectID": "content/02-02-git.html#initial-commit",
    "href": "content/02-02-git.html#initial-commit",
    "title": "Git for version control",
    "section": "Initial commit",
    "text": "Initial commit\n\n\n\nLocation of the new local repository button in GitKraken\n\n\n\nInstall GitKraken and go through the configuration steps\n\nOptional: Preferences -&gt; UI Customization -&gt; Theme -&gt; GitKraken Light\n\nCreate a new local repository\n\nNew Tab -&gt; Repositories -&gt; Create a Repository -&gt; Laptop symbol\n\n\n\n\n\n\n\nCreating a new local repository in learning-git with GitKraken\n\n\n\n\nName the new repo learning-git\nOutside of GitKraken, create a folder for materials for this class, and then a subfolder for livecoding exercises. Use this livecoding folder for “Initialize in.”\n\n\n\n\n\n\nOpen your OS’s file browser to the new repo\n\n\n\n\n\nGitKraken creates a new folder, learning-git, with the file README.md\n\nmd is Markdown, a plain-text format that can be read by any text editor\n\nThe context (right-click) menu for a file has shortcuts to open the file or show it in your OS’s file browser\nFor now, open the file in GitKraken’s built-in text editor\n\nClick on the file to bring up the viewer\nThen click “Edit this file”\n\nAdd some text and save (command/control-S)\nGitKraken automatically opens up the file stage pane"
  },
  {
    "objectID": "content/02-02-git.html#tracking-changes",
    "href": "content/02-02-git.html#tracking-changes",
    "title": "Git for version control",
    "section": "Tracking changes",
    "text": "Tracking changes\n\n\n\n\nMaking your initial commit\n\n\n\n\n\nTracking changes to a file involves two steps: Staging and committing\nStage: You can stage files one at a time, or use the “Stage all changes” button\n\nThis tells git that we want to store these changes to the file in its archive\n\nCommit: Type a message in the comment field and click Commit\n\nThis tells git to go ahead and do the archiving process\n\nThe commit is now displayed in the history panel\n\nMake a few more changes to the file. Practice adding text, removing text, and committing them. Note how the changes accumulate in the history panel, and that you can view each change using Diff View."
  },
  {
    "objectID": "content/02-02-git.html#time-travel",
    "href": "content/02-02-git.html#time-travel",
    "title": "Git for version control",
    "section": "Time travel",
    "text": "Time travel\n\nWe can checkout previous commits to work with old versions of our files\nIn the example, suppose I made a commit with a mistake (my code stopped working or whatever)\nIn the history panel, right-click on a previous commit and select Checkout this commit\n\n\n\n\nChecking out an old commit to travel through time\n\n\n\n\n\n\nTrying (and failing) to change the past. My current HEAD commit will disappear as soon as I check out main.\n\n\n\nWe can make changes to the file in the same way as usual,\nBut when we go to stage, GitKraken warns us that we’re in an undetached head state\nTo see what this means, try making a change to the file, adding and committing it, then checking out the commit with the main or master tag"
  },
  {
    "objectID": "content/02-02-git.html#the-garden-of-forking-branches",
    "href": "content/02-02-git.html#the-garden-of-forking-branches",
    "title": "Git for version control",
    "section": "The garden of forking branches",
    "text": "The garden of forking branches\n\nTo actually change the past, we’ll use a branch\n\nBranches allow git to track multiple distinct “timelines” for files\nFor example, most major software projects will have separate “dev” (development) and “release” branches\nIndividual branches will also be created for work on specific areas of the project\nThis allows each area of active work to be isolated from work happening in other areas\n\n\n\n\n\n\n\nMerging fixing-mistake into main\n\n\n\n\n\nAfter checking out the previous commit, click on Branch in the toolbar\n\nName your new branch fixing-mistake (no spaces!)\n\nStart to work on fixing the mistake in the file, then add and commit as usual\nNow checkout main. Notice:\n\nYour commits on fixing-mistake don’t disappear\nThe state of your file changes to the main version\nThe History panel shows the split between the two branches\n\nAfter we’ve finished fixing the mistake, we want to merge these changes back into main\n\nMake sure you’re on main\nRight-click on fixing-mistake and select “Merge fixing-mistake into main”\n\n\n\n\n\nGitKraken will show the commit pane, with a warning about Merge Conflicts\n\nThis just means that the files you’re combining have conflicting histories, and git needs you to manually sort out what to keep and what to throw away\n\n\n\n\n\n\n\n\nImportant\n\n\n\ngit is now in a special conflict-resolution state.\nUntil you resolve the conflicts and finish the merge, a lot of standard git functionality either won’t work at all or will cause weird problems.\nIf git starts giving you a bunch of weird errors, check to see if you’re in the middle of a merge and need to resolve conflicts.\n\n\n\n\n\n\nGitKraken’s merge conflict resolution tool\n\n\n\nGitKraken will pull up a conflict resolution tool\nChoose which versions of conflicted lines to keep, and/or make edits directly in the bottom pane\nThe output file will be automatically staged\nHit “Commit and Merge” to complete the merge\nMain now points to the merge commit"
  },
  {
    "objectID": "content/02-02-git.html#working-with-github-remotes",
    "href": "content/02-02-git.html#working-with-github-remotes",
    "title": "Git for version control",
    "section": "Working with GitHub remotes",
    "text": "Working with GitHub remotes\n\n\n\nThe remote origin lives on GitHub. Source: https://happygitwithr.com/common-remote-setups.html#ours-you\n\n\nA remote is a copy of a repository that lives on a server somewhere else"
  },
  {
    "objectID": "content/02-02-git.html#working-with-your-own-repos",
    "href": "content/02-02-git.html#working-with-your-own-repos",
    "title": "Git for version control",
    "section": "Working with your own repos",
    "text": "Working with your own repos\n\nLook for the Remote section of left panel. Hover over the 0/0 and click to start creating a new remote\nSelect GitHub, your account, and then click “Create remote and push local refs”\nGitKraken will take a moment to push (upload) the repo, then display a notification\nClick to view the repo on GitHub\nThe URL should be https://github.com/[username]/learning-git\n\n\n\nGitHub also provides a simple text editor\nMake a change or two, then look for the green “Commit changes…” button\nGitKraken may check in GitHub, and pick up the new commits on the remote\n\nIf not: Confirm the changes show up on GitHub. Then, in GitKraken, bring up the command palette (control/command + P), type “fetch,” and select “Fetch All.”\n\nBut the remote commits aren’t automatically loaded locally\nClick “Pull” to download them to your local copy"
  },
  {
    "objectID": "content/02-02-git.html#lab-working-with-someone-elses-repos",
    "href": "content/02-02-git.html#lab-working-with-someone-elses-repos",
    "title": "Git for version control",
    "section": "Lab: Working with someone else’s repos",
    "text": "Lab: Working with someone else’s repos\n\nGitHub lets you download someone else’s repo (clone), and modify it locally, but not upload directly.\n\nYou can suggest a change to someone else’s code by submitting a pull request, which first requires forking the repository.\n\n\n\n\nForking copies a repository to your GitHub account. Then you clone the copy to your local machine. You can push to your remote copy as usual. You can suggest changes to the original using a pull request. Source: https://happygitwithr.com/fork-and-clone.html\n\n\n\n\n\n\nThe fork button is near the upper-right corner of a GitHub repository page. I wasn’t able to find a keyboard shortcut for this. :-(\n\n\n\nStart with the repo for this week’s lab: https://github.com/data-science-methods/lab-1-git\nFork: Look for the fork button in the upper-right\nAfter forking, notice you’re now in your copy of the lab: https://github.com/[username]/lab-1-git.\n\n\n\n\n\n\nCloning the lab repo. The screenshot shows two copies of the lab: the original course version, and the fork to my personal account.\n\n\n\n\n\nClone: After creating the fork, you need to download a copy to your machine.\nIn GitKraken, open a New Tab, then click “Clone a repo”\n\nSelect GitHub.com\nRecommended: Change “Where to clone to” to a specific folder for all the labs for this class\nType “lab-1-git” into the text box. (Or just use the dropdown.)\nIf GitKraken tells you that it can’t clone a private repo, this probably means you haven’t finished getting access to the GitHub Student Developer Pack. Back out of the cloning process, finish with GitHub, and then try cloning again.\n\nAfter opening the repo, click the “View All Files” checkbox or open the folder in your OS file browser\nOpen lab.html in your web browser and lab.Rmd in RStudio"
  },
  {
    "objectID": "content/02-03-functions.html#reading",
    "href": "content/02-03-functions.html#reading",
    "title": "Functions and functional programming",
    "section": "Reading",
    "text": "Reading\n\nSpringate (2013)"
  },
  {
    "objectID": "content/02-03-functions.html#programming-paradigms",
    "href": "content/02-03-functions.html#programming-paradigms",
    "title": "Functions and functional programming",
    "section": "Programming paradigms",
    "text": "Programming paradigms\n\nConceptual frameworks for software engineering\nWays of understanding how a piece of software works\nWays of writing a piece of software\n\n\n\nProcedural or imperative\n\nSoftware is a series of instructions (“procedures”), which the computer carries out in order. Special instructions (if-then, loops) are used to change the order based on inputs or other conditions.\n- Examples: FORTRAN, BASIC, C, a calculator\n\nObject-oriented\n\nSoftware is made up of objects, which have properties (“attributes,” including other objects) and do things (“methods”).\n- Examples: Python, Java\n\nFunctional\n\nSoftware is made up of functions, which are run sequentially on the inputs.\n- Examples: Lisp, Haskell"
  },
  {
    "objectID": "content/02-03-functions.html#r-is-both-object-oriented-and-functional",
    "href": "content/02-03-functions.html#r-is-both-object-oriented-and-functional",
    "title": "Functions and functional programming",
    "section": "R is both object-oriented and functional",
    "text": "R is both object-oriented and functional\n\nBut the object-oriented side is … idiosyncratic (Chambers 2014)"
  },
  {
    "objectID": "content/02-03-functions.html#functional-programming",
    "href": "content/02-03-functions.html#functional-programming",
    "title": "Functions and functional programming",
    "section": "Functional programming",
    "text": "Functional programming\n“Software is made up of functions, which are run sequentially on the inputs.”\n\n\n\n\nflowchart LR\n  pre1[\" \"] -- data --&gt; extract[extract\\nDV & IV]\n  pre2[\" \"] -- specification --&gt; extract\n  extract -- X --&gt; QR[QR\\ndecomposition] -- \"Q, R\" --&gt; combine\n  extract -- Y --&gt; combine\n  combine -- \"Rβ = Q&lt;sup&gt;T&lt;/sup&gt;Y\" --&gt; backsolve\n  backsolve -- β --&gt; post[\" \"]\n  style pre1 height:0px;\n  style pre2 height:0px;\n  style post height:0px;\n\n\nA regression model as a series of functions"
  },
  {
    "objectID": "content/02-03-functions.html#writing-functions-in-r",
    "href": "content/02-03-functions.html#writing-functions-in-r",
    "title": "Functions and functional programming",
    "section": "Writing functions in R",
    "text": "Writing functions in R\n\n## Standard syntax\nnormalize &lt;- function(x) {\n    z = (x - mean(x)) / sd(x)\n    return(z)\n}\n\n## Lambda syntax (R &gt;= 4.1.0)\nnormalize &lt;- \\(x){(x - mean(x)) / sd(x)}\n\n\n\n\n\nnormalize(mtcars$mpg)\n\n [1]  0.15088482  0.15088482  0.44954345  0.21725341 -0.23073453 -0.33028740\n [7] -0.96078893  0.71501778  0.44954345 -0.14777380 -0.38006384 -0.61235388\n[13] -0.46302456 -0.81145962 -1.60788262 -1.60788262 -0.89442035  2.04238943\n[19]  1.71054652  2.29127162  0.23384555 -0.76168319 -0.81145962 -1.12671039\n[25] -0.14777380  1.19619000  0.98049211  1.71054652 -0.71190675 -0.06481307\n[31] -0.84464392  0.21725341"
  },
  {
    "objectID": "content/02-03-functions.html#some-terminology",
    "href": "content/02-03-functions.html#some-terminology",
    "title": "Functions and functional programming",
    "section": "Some terminology",
    "text": "Some terminology\n\narguments\n\nThe inputs to the function, the things inside the parentheses\n\ncalling a function\n\nnormalize is the function itself, as an object. normalize(mtcars$mpg) calls or applies the function to the argument mtcars$mpg\n\npassing\n\nThe relationship between arguments in the call and the function’s internal variables. In normalize(mtcars$mpg), mtcars$mpg is passed to x.\n\nreturn\n\nThe output of the function. Either (a) the argument passed to return or (b) the value of the last line of the function."
  },
  {
    "objectID": "content/02-03-functions.html#features-of-functional-programming",
    "href": "content/02-03-functions.html#features-of-functional-programming",
    "title": "Functions and functional programming",
    "section": "Features of functional programming",
    "text": "Features of functional programming\n\nfirst-class functions\n\nFunctions can be used like any other data type, including as inputs to and outputs from other functions (functionals; function factories)\n\ndeterminism\n\nGiven the same input values, the function always returns the same output value\n\nno side effects\n\nThe function doesn’t have any effects other than returning its output\n\nimmutability\n\nOnce a variable is assigned a value, that value cannot be changed\n\n\n\n\n\nFunctional programming implements software as strict input-output flow\nR doesn’t enforce determinism, no side effects, or immutability\nBut writing your own code around them makes it easier to reason about how your code works"
  },
  {
    "objectID": "content/02-03-functions.html#references",
    "href": "content/02-03-functions.html#references",
    "title": "Functions and functional programming",
    "section": "References",
    "text": "References\n\n\n\n\nChambers, John M. 2014. “Object-Oriented Programming, Functional Programming and R.” Statistical Science 29 (2): 167–80. https://doi.org/10.1214/13-STS452.\n\n\nSpringate, David. 2013. “Functional Programming in R.” DataJujitsu. May 16, 2013. https://www.datajujitsu.co.uk/blog/2013/05/16/functional-programming-in-r/."
  },
  {
    "objectID": "content/02-04-getting-help.html#reading",
    "href": "content/02-04-getting-help.html#reading",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Reading",
    "text": "Reading\n\nBryan (2020)\n“R Faq - How to Make a Great R Reproducible Example” (n.d.)\nWickham (2019)"
  },
  {
    "objectID": "content/02-04-getting-help.html#dependencies",
    "href": "content/02-04-getting-help.html#dependencies",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Dependencies",
    "text": "Dependencies\n\ninstall.packages(\"lubridate\", \"reprex\")"
  },
  {
    "objectID": "content/02-04-getting-help.html#messages-warnings-and-errors",
    "href": "content/02-04-getting-help.html#messages-warnings-and-errors",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Messages, warnings, and errors",
    "text": "Messages, warnings, and errors\n\nMessage\n\nThings are fine, but here’s some information you should know\n\nWarning\n\nUhhhh I’m gonna keep going, but maybe this isn’t what you want\n\nError\n\nNope. I’m stopping here. You need to fix the thing.\n\n\n\nmessage('Hey, just FYI')\n\nHey, just FYI\n\nwarning('Uhhhh might want to check this out')\n\nWarning: Uhhhh might want to check this out\n\nstop('Noooooo')\n\nError in eval(expr, envir, enclos): Noooooo"
  },
  {
    "objectID": "content/02-04-getting-help.html#example-dates-are-often-problems",
    "href": "content/02-04-getting-help.html#example-dates-are-often-problems",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Example: Dates are often problems",
    "text": "Example: Dates are often problems\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nadd_six_months = function(date_str) {\n    parsed_dates = parse_date_time(date_str, orders = 'mdY')\n    parsed_dates %m+% months(6) \n}\n\nsome_data = c('June 2002', 'May 15, 2007', 'August 2007')\nadd_six_months(some_data)\n\nWarning: 2 failed to parse.\n\n\n[1] NA               \"2007-11-15 UTC\" NA"
  },
  {
    "objectID": "content/02-04-getting-help.html#warning-this-is-a-warning-not-an-error",
    "href": "content/02-04-getting-help.html#warning-this-is-a-warning-not-an-error",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Warning: This is a warning, not an error",
    "text": "Warning: This is a warning, not an error\n\nR won’t stop running here\nErrors might not show up until much later in our code, making it hard to identify the root cause\nOr they might cause invisible problems, eg, by default lm() silently drops observations with missing values"
  },
  {
    "objectID": "content/02-04-getting-help.html#a-debugging-mindset",
    "href": "content/02-04-getting-help.html#a-debugging-mindset",
    "title": "Warnings, Errors, and Getting Help",
    "section": "A debugging mindset",
    "text": "A debugging mindset\n\n\n\n\n\n“A debugging manifesto,” Julia Evans, https://jvns.ca/blog/2022/12/08/a-debugging-manifesto/\n\n\n\n\n\n\nInspect, don’t squash\nBeing stuck is temporary\nTrust nobody and nothing\nIt’s probably your code\nDon’t go it alone\nThere’s always a reason\nBuild your toolkit\nIt can be an adventure"
  },
  {
    "objectID": "content/02-04-getting-help.html#some-debugging-tactics",
    "href": "content/02-04-getting-help.html#some-debugging-tactics",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Some debugging tactics",
    "text": "Some debugging tactics\n\nRubber duck debugging\nRestart your session: Session \\(\\to\\) Restart R\nCheck the documentation: ?fun\nIsolate the problem by creating a MWE/reprex\nStackOverflow: https://stackoverflow.com/questions/tagged/r\nCRAN \\(\\to\\) BugReports (usually GitHub Issues)"
  },
  {
    "objectID": "content/02-04-getting-help.html#debugging",
    "href": "content/02-04-getting-help.html#debugging",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Debugging",
    "text": "Debugging\n\nsome_data = c('June 2002', 'May 15, 2007', 'August 2007')\nadd_six_months(some_data)\n\nWarning: 2 failed to parse.\n\n\n[1] NA               \"2007-11-15 UTC\" NA              \n\n\n\nLet’s start by using the RStudio debugger to isolate the problem\n\ndebugonce(add_six_months)\nadd_six_months(some_data)\n\ndebugging in: add_six_months(some_data)\ndebug at &lt;text&gt;#3: {\n    parsed_dates = parse_date_time(date_str, orders = \"mdY\")\n    parsed_dates %m+% months(6)\n}\ndebug at &lt;text&gt;#4: parsed_dates = parse_date_time(date_str, orders = \"mdY\")\n\n\nWarning: 2 failed to parse.\n\n\ndebug at &lt;text&gt;#5: parsed_dates %m+% months(6)\nexiting from: add_six_months(some_data)\n\n\n[1] NA               \"2007-11-15 UTC\" NA              \n\n\n\n\nThe problem is in lubridate::parse_date_time().\n\n\n?parse_date_time\n\n\nSpend a few minutes reading the documentation for this function and playing around with the call.\n\n\n\nWhat does the argument orders do?\n\n\nparse_date_time(some_data, orders = 'mdY')\n\nWarning: 2 failed to parse.\n\n\n[1] NA               \"2007-05-15 UTC\" NA              \n\n\n\n\n\nLet’s try SO: https://stackoverflow.com/search?q=%5BR%5D+lubridate+month-year\n\n\n\nparse_date_time(some_data, orders = c('mY', 'mdY'))\n\n[1] \"2002-06-01 UTC\" \"2007-05-15 UTC\" \"2007-08-01 UTC\"\n\n\n\n\nMake this change in add_six_months()\nConfirm it no longer trips the assertion\nExplain to your partner why this fixed things"
  },
  {
    "objectID": "content/02-04-getting-help.html#another-example-more-fun-with-dates",
    "href": "content/02-04-getting-help.html#another-example-more-fun-with-dates",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Another example: More fun with dates",
    "text": "Another example: More fun with dates\n\nmore_data = c('May 7, 2017', 'May 19, 2017', 'May Fifth, 2017')\nmdy(more_data)\n\n[1] \"2017-05-07\" \"2017-05-19\" \"2017-05-20\"\n\n\n\nSO doesn’t seem so helpful: https://stackoverflow.com/search?q=%5BR%5D+lubridate+written+days\n\n\n\nThe CRAN page for lubridate includes a link to report bugs: https://cran.r-project.org/web/packages/lubridate/index.html\n\n\n\n\nScreenshot of lubridate on CRAN, highlighting the BugReports field\n\n\n\n\nTrying a couple of searches gives us a promising result: https://github.com/tidyverse/lubridate/issues?q=is%3Aissue+is%3Aopen+mdy\n\n\n\n\nScreenshot of lubridate issues page, showing a relevant search result (August 2021)\n\n\n\nThis is a known bug; it looks like they’re thinking about doing something about it, but the only workaround is to create an NA: https://github.com/tidyverse/lubridate/issues/685"
  },
  {
    "objectID": "content/02-04-getting-help.html#writing-a-reproducible-example-reprex",
    "href": "content/02-04-getting-help.html#writing-a-reproducible-example-reprex",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Writing a reproducible example: reprex",
    "text": "Writing a reproducible example: reprex\n\nhttps://reprex.tidyverse.org/\nhttps://reprex.tidyverse.org/articles/articles/learn-reprex.html\nhttps://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\nPractice by writing a reprex for one of our two examples"
  },
  {
    "objectID": "content/02-04-getting-help.html#do-not-do-these-things-in-your-reprex-or-anywhere-else",
    "href": "content/02-04-getting-help.html#do-not-do-these-things-in-your-reprex-or-anywhere-else",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Do not do these things in your reprex (or anywhere else)",
    "text": "Do not do these things in your reprex (or anywhere else)\nOr Jenny Bryan will come to your office and set your computer on fire.\n\nsetwd('/users/danhicks/projects/catsaregreat/myscript/')\n\nUsed to ensure that R is running where your file is\nUnnecessary if you’re opening different projects in different RStudio sessions\nWill cause irrelevant errors on any other system\nInstead, use file.path() or here::here() to build paths\n\nrm(list=ls())\n\nUsed because people think it clears out the global environment\nDoesn’t actually clear out the global environment\n\neg, doesn’t unload packages or reset options()\n\nUnnecessary if you’re regularly using Session \\(\\to\\) Restart R\nAlso unnecessary at the top of a Rmd file, which is always knit in a new session\n\n\n\nNot on Bryan’s list, but also don’t do it:\n\nrequire(package)\n\nIf package is installed, will act just like library()\nIf not, will return FALSE\n\nThe script will keep going until there’s an error about a missing function 300 lines later\nProbably not the error you wanted help with\nAnnoying to debug because I have no idea where the function is supposed to come from\n\nIf library() can’t find the package, it immediately raises an error\n\nI can tell right away what package needs to be installed"
  },
  {
    "objectID": "content/02-04-getting-help.html#labs",
    "href": "content/02-04-getting-help.html#labs",
    "title": "Warnings, Errors, and Getting Help",
    "section": "Labs",
    "text": "Labs\nLab 2 introduces you to some of RStudio’s debugging tools.\nLab 3 explores functional programming in R:\n\nAssembling pipes using tidyverse functions\nWriting code that respects immutability\nUsing the map() functional to apply a function to each element of a list\nWriting a function factory to change the way a function behaves"
  },
  {
    "objectID": "content/02-04-getting-help.html#references",
    "href": "content/02-04-getting-help.html#references",
    "title": "Warnings, Errors, and Getting Help",
    "section": "References",
    "text": "References\n\n\n\n\nBryan, Jenny. 2020. “Object of Type ‘Closure’ Is Not Subsettable.” Presented at the RSTUDIO::CONF 2020, January 31. https://rstudio.com/resources/rstudioconf-2020/object-of-type-closure-is-not-subsettable/.\n\n\n“R Faq - How to Make a Great R Reproducible Example.” n.d. Stack Overflow. Accessed August 31, 2018. https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example.\n\n\nWickham, Hadley. 2019. “Debugging.” In Advanced R, Second Edition, ch. 22. CRC Press. https://adv-r.hadley.nz/debugging.html."
  },
  {
    "objectID": "content/03-02-models-of-eda.html",
    "href": "content/03-02-models-of-eda.html",
    "title": "Project: Data Journey Narrative",
    "section": "",
    "text": "Exploratory Data Analysis"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#section",
    "href": "content/03-02-models-of-eda.html#section",
    "title": "Project: Data Journey Narrative",
    "section": "",
    "text": "Characterize the journey that your data took to get to you. (BatesDataJourneysCapturing2016?; Madrigal and Meyer 2021) Some relevant questions to answer might include\n\nWho generated these data? Why? How?\nWhat measurement instruments were used to generate these data? What infrastructure has been used to store and transmit the data?\nWhat was the original intended use of these data? What are some other ways the data have been used?\nHow has the mutability/immutability of the data been important to its journey & use? How has its mutability/immutability been created and maintained?\nHow will you need to transform the data further for your project? How do the physical and material properties of the data facilitate this, and/or create friction?\nWhat values have been crystallized in the data? How have power relations and values shaped what is/isn’t included in the data?\nBased on these factors, do the data appear to be fit for purpose? That is, based on what you know so far, do the data appear to be appropriate for answering your research question?\n\nInclude references for the sources you use to answer these questions. You can work on this step of the project at the same time as the EDA step, but I will ask you to turn in the data journey narrative first. A good target length for your narrative is 500-1,000 words long, not counting any references."
  },
  {
    "objectID": "content/03-02-models-of-eda.html#section-1",
    "href": "content/03-02-models-of-eda.html#section-1",
    "title": "Project: Data Journey Narrative",
    "section": "",
    "text": "This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation. (John Wilder Tukey 1977)\n\n\nExploratory Data Analysis (EDA) is “an attitude, AND a flexibility, AND some graph paper (or transparencies, or both)” (John W. Tukey 1980)"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#exploratory-and-confirmatory-research",
    "href": "content/03-02-models-of-eda.html#exploratory-and-confirmatory-research",
    "title": "Project: Data Journey Narrative",
    "section": "Exploratory and Confirmatory Research",
    "text": "Exploratory and Confirmatory Research\nEspecially in the wake of the replication crisis, one common distinction is between exploratory and confirmatory research (Wagenmakers et al. 2012)\n\n\n\nWagenmakers et al. (2012) figure 1. The caption reads, in part: “A continuum of experimental exploration and the corresponding continuum of statistical wonkiness. On the far left of the continuum, researchers find their hypothesis in the data by post-hoc theorizing, and the corresponding statistics are ‘wonky,’ dramatically overestimating the evidence for the hypothesis.”\n\n\n\n\nExploratory vs. confirmatory research\n\n\n\n\n\n\nConfirmatory\nExploratory\n\n\n\n\nhypothesis testing\nhypothesis development\n\n\nspecified in advance\nadaptable\n\n\nalgorithmic\nfree, creative\n\n\nmechanical objectivity\n(Daston and Galison 2007)\npure subjectivity?\n\n\navoids inferential errors\nmakes errors?\n\n\nrigorous\nlacking rigor?\n\n\nreal science??\nh*cking around with data??\n\n\nassumes experimental methods\nrelevant to all methods\n\n\n\n\nI agree that it’s important to\n\nbe thoughtful about how much confidence we’re placing in our conclusions\ninterpret findings from one study in light of other studies\n\n\nBut the confirmatory/exploratory distinction can overhype the confirmatory side\n\nMaking us too rigid and narrow-minded about what counts as good science"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#better-models-for-eda-i-developing-phenomena",
    "href": "content/03-02-models-of-eda.html#better-models-for-eda-i-developing-phenomena",
    "title": "Project: Data Journey Narrative",
    "section": "Better Models for EDA I: Developing Phenomena",
    "text": "Better Models for EDA I: Developing Phenomena\nBogen and Woodward (1988) by way of Brown (2002)\n\nThe data/phenomena/theory distinction\n\n\n\n\n\n\n\nData\nPhenomena\nTheories/ Causal processes\n\n\n\n\nEx: Spreadsheet of numbers, downloaded from Qualtrics\nEx: Correlation between partisanship and sharing Covid misinformation\nEx: Conservative susceptibility to anxiety hypothesis\n\n\n\n\n\n\n\n\ncollected\nabstracted or extracted from data\npostulated\n\n\nnot explained\nexplained by theories\nexplain phenomena\n\n\nhighly local to time, place, sample, procedure\nvarying scope\nuniversal?\n\n\n“raw,” messy, unwieldy\n“processed,” clean, stylized\n“laws of nature”?"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#eda-as-phenomena-development",
    "href": "content/03-02-models-of-eda.html#eda-as-phenomena-development",
    "title": "Project: Data Journey Narrative",
    "section": "EDA as phenomena development",
    "text": "EDA as phenomena development\n\ncleaning messy data\n\nidentifying and mitigating (where possible) errors and idiosyncracies\n\nidentifying local patterns (“local phenomena”)\n\n\n\n\nnot yet claiming these will be stable and appear elsewhere\nnot yet worrying (much) about explanations"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#better-models-for-eda-ii-epicycle-of-analysis",
    "href": "content/03-02-models-of-eda.html#better-models-for-eda-ii-epicycle-of-analysis",
    "title": "Project: Data Journey Narrative",
    "section": "Better Models for EDA II: Epicycle of Analysis",
    "text": "Better Models for EDA II: Epicycle of Analysis\n\n\n\n\n\nEpicycle of analysis model, Peng and Matsui (2016), 5"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#pengartdatascience2016",
    "href": "content/03-02-models-of-eda.html#pengartdatascience2016",
    "title": "Project: Data Journey Narrative",
    "section": "Peng and Matsui (2016)",
    "text": "Peng and Matsui (2016)\n\nData analysis is organized into 5 activities\nEach activity involves the same 3-step “epicycle” process\n\nDevelop expectations\nCollect information\nCompare and revise expectations\n\nNot “the scientific method”! (Peng and Matsui 2016, 4)\n\n“Highly iterative and non-linear”\n“information is learned at each step, which then informs\n\nwhether (and how) to refine, and redo, the [previous] step …, or\nwhether (and how) to proceed to the next step.”"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#section-2",
    "href": "content/03-02-models-of-eda.html#section-2",
    "title": "Project: Data Journey Narrative",
    "section": "",
    "text": "Aligning the goals of EDA with steps in the “epicycle of analysis” (Peng and Matsui 2016)\n\n\n\n\n\n\nGoals of EDA\nEpicycle step\n\n\n\n\nDetermine if there are problems with the data\n2. Collecting information\n\n\nDetermine whether our question can be answered with these data\n3. Comparing and revising expectations\n\n\nDevelop sketch of an answer\n1. Developing expectations"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#discussion",
    "href": "content/03-02-models-of-eda.html#discussion",
    "title": "Project: Data Journey Narrative",
    "section": "Discussion",
    "text": "Discussion\n\nFor each of these models, how well do they fit the way you’ve been taught to do science?\nHow do they challenge the way you’ve been taught to do science?"
  },
  {
    "objectID": "content/03-02-models-of-eda.html#references",
    "href": "content/03-02-models-of-eda.html#references",
    "title": "Project: Data Journey Narrative",
    "section": "References",
    "text": "References\n\n\n\n\nBogen, James, and James Woodward. 1988. “Saving the Phenomena.” The Philosophical Review 97 (3): 303–52. http://www.jstor.org/stable/2185445.\n\n\nBrown, James Robert. 2002. Smoke and Mirrors: How Science Reflects Reality. Routledge.\n\n\nDaston, Lorraine, and Peter Galison. 2007. Objectivity. New York : Cambridge, Mass: Zone Books ; Distributed by the MIT Press.\n\n\nMadrigal, Alexis C., and Robinson Meyer. 2021. “Why the Pandemic Experts Failed.” The Atlantic. March 15, 2021. https://www.theatlantic.com/science/archive/2021/03/americas-coronavirus-catastrophe-began-with-data/618287/.\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience.\n\n\nTukey, John W. 1980. “We Need Both Exploratory and Confirmatory.” The American Statistician 34 (1): 23–25. https://doi.org/10.1080/00031305.1980.10482706.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley Series in Behavioral Science. Reading, Mass: Addison-Wesley Pub. Co. https://archive.org/details/exploratorydataa00tuke_0.\n\n\nWagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der Maas, and Rogier A. Kievit. 2012. “An Agenda for Purely Confirmatory Research.” Perspectives on Psychological Science 7 (6): 632–38. https://doi.org/10.1177/1745691612463078."
  },
  {
    "objectID": "content/03-04-oakland-eda.html#section",
    "href": "content/03-04-oakland-eda.html#section",
    "title": "Case Study: Police Stops in Oakland",
    "section": "",
    "text": "For this EDA, we’ll work with data on police stops in Oakland, California, that have been pre-cleaned and released by the Stanford Open Policing Project (Pierson et al. 2020).\nWe’ll be following the checklist from Peng and Matsui (2016).\nWe’ll also be learning to use the skimr and visdat packages"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#pengartdatascience2016",
    "href": "content/03-04-oakland-eda.html#pengartdatascience2016",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Peng and Matsui (2016)",
    "text": "Peng and Matsui (2016)\n\nFormulate your question\nRead in your data\nCheck the packaging\nLook at the top and the bottom of your data\nCheck your “n”s\nValidate with at least one external data source\nMake a plot\nTry the easy solution first\nFollow up"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#formulate-your-question",
    "href": "content/03-04-oakland-eda.html#formulate-your-question",
    "title": "Case Study: Police Stops in Oakland",
    "section": "1. Formulate your question",
    "text": "1. Formulate your question\n\nThe Black Lives Matter protests over the last several years have made us aware of the racial aspects of policing.\nHere we’re specifically interested in\n\nWhether Black people in Oakland might be more likely to be stopped than White people\nWhether Black people who are stopped might be more likely to have contraband\n\nThese aren’t very precise, but that’s okay: Part of the goal of EDA is to clarify and refine our research questions"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#set-up-our-workspace",
    "href": "content/03-04-oakland-eda.html#set-up-our-workspace",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Set up our workspace",
    "text": "Set up our workspace\n\nDedicated project folder\nOptional: Create an RStudio Project\nClean R session\nMore on project management and organization later in the semester"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#packages",
    "href": "content/03-04-oakland-eda.html#packages",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)   # for working with the data\nlibrary(lubridate)   # for working with datetime data\n\nlibrary(skimr)       # generate a text-based overview of the data\nlibrary(visdat)      # generate plots visualizing data types and missingness\nlibrary(plotly)      # quickly create interactive plots"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#get-the-data",
    "href": "content/03-04-oakland-eda.html#get-the-data",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Get the Data",
    "text": "Get the Data\n\nWe’ll be using data on police stops in Oakland, California, collected and published by the Stanford Open Policing Project.\nFor reproducibility, we’ll write a bit of code that automatically downloads the data\nTo get the download URL:\n\nhttps://openpolicing.stanford.edu/data/\nScroll down to Oakland\nRight-click on the file symbol to copy the URL\n\nREADME: https://github.com/stanford-policylab/opp/blob/master/data_readme.md.\n\n\ndata_dir = 'data'\ntarget_file = file.path(data_dir, 'oakland.zip')\n\nif (!dir.exists(data_dir)) {\n    dir.create(data_dir)\n}\nif (!file.exists(target_file)) {\n    download.file('https://stacks.stanford.edu/file/druid:yg821jf8611/yg821jf8611_ca_oakland_2020_04_01.csv.zip', \n                  target_file)\n}"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#read-in-your-data",
    "href": "content/03-04-oakland-eda.html#read-in-your-data",
    "title": "Case Study: Police Stops in Oakland",
    "section": "2. Read in your data",
    "text": "2. Read in your data\nThe dataset is a zipped csv or comma-separated value file. CSVs are structured like Excel spreadsheets, but are stored in plain text rather than Excel’s format.\n\ndataf = read_csv(target_file)\n\nRows: 133407 Columns: 28\n── Column specification ────────────────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (16): raw_row_number, location, beat, subject_race, subject_sex, officer_assignment, type, ...\ndbl   (3): lat, lng, subject_age\nlgl   (7): arrest_made, citation_issued, warning_issued, contraband_found, contraband_drugs, con...\ndate  (1): date\ntime  (1): time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "content/03-04-oakland-eda.html#check-the-packaging",
    "href": "content/03-04-oakland-eda.html#check-the-packaging",
    "title": "Case Study: Police Stops in Oakland",
    "section": "3. Check the packaging",
    "text": "3. Check the packaging\nPeng and Matsui (2016) use some base R functions to look at dimensions of the dataframe and column (variable) types. skimr is more powerful.\n\n## May take a couple of seconds\nskim(dataf)\n\n── Data Summary ────────────────────────\n                           Values\nName                       dataf \nNumber of rows             133407\nNumber of columns          28    \n_______________________          \nColumn type frequency:           \n  character                16    \n  Date                     1     \n  difftime                 1     \n  logical                  7     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────────────────────────\n   skim_variable                 n_missing complete_rate min max empty n_unique whitespace\n 1 raw_row_number                        0        1        1  71     0   133407          0\n 2 location                             51        1.00     1  78     0    60723          0\n 3 beat                              72424        0.457    3  19     0      129          0\n 4 subject_race                          0        1        5  22     0        5          0\n 5 subject_sex                          90        0.999    4   6     0        2          0\n 6 officer_assignment               121431        0.0898   5  97     0       20          0\n 7 type                              20066        0.850    9  10     0        2          0\n 8 outcome                           34107        0.744    6   8     0        3          0\n 9 search_basis                      92250        0.309    5  14     0        3          0\n10 reason_for_stop                       0        1       14 197     0      113          0\n11 use_of_force_description         116734        0.125   10  10     0        1          0\n12 raw_subject_sdrace                    0        1        1   1     0        7          0\n13 raw_subject_resultofencounter         0        1        7 213     0      315          0\n14 raw_subject_searchconducted           0        1        2  24     0       34          0\n15 raw_subject_typeofsearch          52186        0.609    2 112     0      417          0\n16 raw_subject_resultofsearch       111633        0.163    5  95     0      298          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min        max        median     n_unique\n1 date                  2          1.00 2013-04-01 2017-12-31 2015-07-19     1638\n\n── Variable type: difftime ─────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min    max        median n_unique\n1 time                  2          1.00 0 secs 86340 secs 16:12      1439\n\n── Variable type: logical ──────────────────────────────────────────────────────────────────────────\n  skim_variable      n_missing complete_rate   mean count                  \n1 arrest_made                0         1     0.121  FAL: 117217, TRU: 16190\n2 citation_issued            0         1     0.394  FAL: 80836, TRU: 52571 \n3 warning_issued             0         1     0.231  FAL: 102545, TRU: 30862\n4 contraband_found       92250         0.309 0.149  FAL: 35005, TRU: 6152  \n5 contraband_drugs       92250         0.309 0.0844 FAL: 37684, TRU: 3473  \n6 contraband_weapons     92250         0.309 0.0299 FAL: 39928, TRU: 1229  \n7 search_conducted           0         1     0.309  FAL: 92250, TRU: 41157 \n\n── Variable type: numeric ──────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   mean      sd     p0    p25    p50    p75   p100 hist \n1 lat                 114         0.999   37.8  0.0284   37.4   37.8   37.8   37.8   38.1 ▁▁▇▁▁\n2 lng                 114         0.999 -122.   0.0432 -122.  -122.  -122.  -122.  -119.  ▇▁▁▁▁\n3 subject_age      102724         0.230   33.2 13.3      10     23     29     41     97   ▇▆▃▁▁\n\n\n\nHow many rows and columns?\nWhat variables are represented as different variable types?\nAre there any types that might indicate parsing problems?\n\n\n\n133k rows (observations); 28 columns (variables)\n16 variables are handled as characters\n\nraw_row_number has 1 unique value per row\n\nSo it’s probably some kind of identifier\n\nsubject_race and subject_sex have just 5 and 2 unique values\n\nThese are probably categorical variables represented as characters\n\nSimilarly with type, outcome, and search_basis\n\nThough these have lots of missing values (high n_missing, low complete_rate)\n\n\n1 variable represents the date, and another is difftime\n\n?difftime tells us that difftime is used to represent intervals or “time differences”\n\n7 logical variables\n\nA lot of these look like coded outcomes that we might be interested in, eg, search_conducted and contraband_found\nsearch_conducted has no missing values, but contraband_found has a lot of missing values"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#for-our-motivating-questions",
    "href": "content/03-04-oakland-eda.html#for-our-motivating-questions",
    "title": "Case Study: Police Stops in Oakland",
    "section": "For our motivating questions",
    "text": "For our motivating questions\n\nGood: subject_race is 100% complete\nAlso good: search_conducted is also 100% complete\nPotentially worrisome: contraband_found is only 31% complete"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#missing-values",
    "href": "content/03-04-oakland-eda.html#missing-values",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Missing values",
    "text": "Missing values\n\nLet’s use visdat::vis_miss()to\n\nvisualize missing values and\ncheck what’s up with contraband_found.\n\n\n\nBut this raises an error about large data\n\nvis_miss(dataf)\n\nError in `test_if_large_data()`:\n! Data exceeds recommended size for visualisation\nConsider downsampling your data with `dplyr::slice_sample()`\nOr set argument, `warn_large_data` = `FALSE`\n\n\n\nSo we’ll use sample_n() to draw a subset\n\nset.seed(2021-09-28)\ndataf_smol = sample_n(dataf, 1000)\n\nvis_miss(dataf_smol)\n\n\n\n\n\nArguments in vis_miss() are useful for picking up patterns in missing values\n\n## cluster = TRUE uses hierarchical clustering to order the rows\nvis_miss(dataf_smol, cluster = TRUE) +\n    coord_flip()\n\n\n\n\n\nSeveral variables related to search outcomes are missing together\n\ncontraband_found, contraband_drugs, contraband_weapons, search_basis, use_of_force_description, raw_subject_typeofsearch, and raw_subject_resultofsearch\n\nHowever, search_conducted is complete"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#a-critical-question",
    "href": "content/03-04-oakland-eda.html#a-critical-question",
    "title": "Case Study: Police Stops in Oakland",
    "section": "A critical question",
    "text": "A critical question\nWhen a search has been conducted, do we know whether contraband was found?\n\nOr: are there cases where a search was conducted, but contraband_found is missing?\n\n\ndataf  |&gt;  \n    filter(search_conducted) |&gt;\n    count(search_conducted, is.na(contraband_found))\n\n# A tibble: 1 × 3\n  search_conducted `is.na(contraband_found)`     n\n  &lt;lgl&gt;            &lt;lgl&gt;                     &lt;int&gt;\n1 TRUE             FALSE                     41157"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#look-at-the-top-and-the-bottom-of-your-data",
    "href": "content/03-04-oakland-eda.html#look-at-the-top-and-the-bottom-of-your-data",
    "title": "Case Study: Police Stops in Oakland",
    "section": "4. Look at the top and the bottom of your data",
    "text": "4. Look at the top and the bottom of your data\n\nWith 28 columns, the dataframe is too wide to print in a readable way.\nInstead we’ll use the base R function View() in an interactive session\n\nAn Excel-like spreadsheet presentation\n\nView() can cause significant problems if you use it with a large dataframe on a slower machine\n\nWe’ll use a pipe:\n\nFirst extract the head() or tail() of the dataset\nThen View() it\n\nWe’ll also use dataf_smol\n\n\n\nif (interactive()) {\n    dataf |&gt; \n        head() |&gt; \n        View()\n    \n    dataf |&gt; \n        tail() |&gt; \n        View()\n    \n    View(dataf_smol)\n}\n\n\nThis is interactive code\n\nDon’t want it to run when we run the script automatically or render it to HTML\nUse if with interactive() or rlang::is_interactive()\n\n\n\nSome of my observations:\n\nThe ID variable raw_row_number can’t be turned into a numeric value\nlocation is a mix of addresses and intersections (“Bond St @ 48TH AVE”)\n\nIf we were going to generate a map using this column, geocoding might be tricky\nFortunately we also get latitude and longitude columns\n\nuse_of_force_description doesn’t seem to be a descriptive text field; instead it seems to be mostly missing or “handcuffed”\n\n\nWe can also use skimr to check data quality by looking at the minimum and maximum values. Do these ranges make sense for what we expect the variable to be?\n\nskim(dataf)\n\n── Data Summary ────────────────────────\n                           Values\nName                       dataf \nNumber of rows             133407\nNumber of columns          28    \n_______________________          \nColumn type frequency:           \n  character                16    \n  Date                     1     \n  difftime                 1     \n  logical                  7     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────────────────────────\n   skim_variable                 n_missing complete_rate min max empty n_unique whitespace\n 1 raw_row_number                        0        1        1  71     0   133407          0\n 2 location                             51        1.00     1  78     0    60723          0\n 3 beat                              72424        0.457    3  19     0      129          0\n 4 subject_race                          0        1        5  22     0        5          0\n 5 subject_sex                          90        0.999    4   6     0        2          0\n 6 officer_assignment               121431        0.0898   5  97     0       20          0\n 7 type                              20066        0.850    9  10     0        2          0\n 8 outcome                           34107        0.744    6   8     0        3          0\n 9 search_basis                      92250        0.309    5  14     0        3          0\n10 reason_for_stop                       0        1       14 197     0      113          0\n11 use_of_force_description         116734        0.125   10  10     0        1          0\n12 raw_subject_sdrace                    0        1        1   1     0        7          0\n13 raw_subject_resultofencounter         0        1        7 213     0      315          0\n14 raw_subject_searchconducted           0        1        2  24     0       34          0\n15 raw_subject_typeofsearch          52186        0.609    2 112     0      417          0\n16 raw_subject_resultofsearch       111633        0.163    5  95     0      298          0\n\n── Variable type: Date ─────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min        max        median     n_unique\n1 date                  2          1.00 2013-04-01 2017-12-31 2015-07-19     1638\n\n── Variable type: difftime ─────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min    max        median n_unique\n1 time                  2          1.00 0 secs 86340 secs 16:12      1439\n\n── Variable type: logical ──────────────────────────────────────────────────────────────────────────\n  skim_variable      n_missing complete_rate   mean count                  \n1 arrest_made                0         1     0.121  FAL: 117217, TRU: 16190\n2 citation_issued            0         1     0.394  FAL: 80836, TRU: 52571 \n3 warning_issued             0         1     0.231  FAL: 102545, TRU: 30862\n4 contraband_found       92250         0.309 0.149  FAL: 35005, TRU: 6152  \n5 contraband_drugs       92250         0.309 0.0844 FAL: 37684, TRU: 3473  \n6 contraband_weapons     92250         0.309 0.0299 FAL: 39928, TRU: 1229  \n7 search_conducted           0         1     0.309  FAL: 92250, TRU: 41157 \n\n── Variable type: numeric ──────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate   mean      sd     p0    p25    p50    p75   p100 hist \n1 lat                 114         0.999   37.8  0.0284   37.4   37.8   37.8   37.8   38.1 ▁▁▇▁▁\n2 lng                 114         0.999 -122.   0.0432 -122.  -122.  -122.  -122.  -119.  ▇▁▁▁▁\n3 subject_age      102724         0.230   33.2 13.3      10     23     29     41     97   ▇▆▃▁▁\n\n\n\nMore observations:\n\nDate range is April 1, 2013 to December 31, 2017\n\nIf we break things down by year, we should expect 2013 to have fewer cases\nFor some purposes, we might need to exclude 2013 data\nfilter(dataf, date &gt;= '2014-01-01')\n\nAge range is from 10 years old (!) to 97 (!)\n\nMedian (p50) is 29; 50% of values are between 23 and 41\nFor some purposes, we might need to restrict the analysis to working-age adults\nfilter(dataf, subject_age &gt;= 18, subject_age &lt; 65)"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#check-your-ns-and-6.-validate-with-at-least-one-external-data-source",
    "href": "content/03-04-oakland-eda.html#check-your-ns-and-6.-validate-with-at-least-one-external-data-source",
    "title": "Case Study: Police Stops in Oakland",
    "section": "5. Check your Ns (and) 6. Validate with at least one external data source",
    "text": "5. Check your Ns (and) 6. Validate with at least one external data source\n\nPeng and Matsui (2016) use an air quality example with a regular sampling rate\n\nso they can calculate exactly how many observations they should have\n\nWe can’t do that here\n\nSo we’ll combine steps 5 and 6 together\n\n\n\n\nA web search leads us to this City of Oakland page on police stop data: https://www.oaklandca.gov/resources/stop-data\n\nThe page mentions a Stanford study that was released in June 2016\nRecall we got our data from the Stanford Open Policing Project\nOur data run April 2013 through December 2017\nSo there’s a good chance we’re using a superset of the “Stanford study” data\nThough the links under that section don’t have nice descriptive tables\n\n\n\n\nLet’s try Historical Stop Data and Reports\nThen OPD Racial Impact Report: 2016-2018\n\nPage 8 has two summary tables that we can compare to our data\n\n\n\n\n\nScreenshot of the two summary tables from the Oakland report. Source: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf, page 8"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#from-dates-to-years",
    "href": "content/03-04-oakland-eda.html#from-dates-to-years",
    "title": "Case Study: Police Stops in Oakland",
    "section": "From dates to years",
    "text": "From dates to years\n\nOur data has the particular date of each stop\n\nWe need to extract the year of each stop\n\nlubridate::year() does exactly this\n\nFilter to the years in our data that overlap with the tables\nAnd then aggregate by year and gender using count\n\n\n\n\n\n\ndataf |&gt; \n    mutate(year = year(date)) |&gt; \n    filter(year %in% c(2016, 2017)) |&gt; \n    count(year)\n\n# A tibble: 2 × 2\n   year     n\n  &lt;dbl&gt; &lt;int&gt;\n1  2016 30268\n2  2017 30715\n\ndataf |&gt; \n    mutate(year = year(date)) |&gt; \n    filter(year %in% c(2016, 2017)) |&gt; \n    count(year, subject_sex)\n\n# A tibble: 6 × 3\n   year subject_sex     n\n  &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n1  2016 female       7677\n2  2016 male        22563\n3  2016 &lt;NA&gt;           28\n4  2017 female       7879\n5  2017 male        22818\n6  2017 &lt;NA&gt;           18\n\n\n\n\n\n\nScreenshot of the two summary tables from the Oakland report. Source: https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf, page 8\n\n\n\n\n\n\nFor both years, we have fewer observations than the report table indicates\n\nCould our data have been pre-filtered?\nLet’s check the documentation for our data: https://github.com/stanford-policylab/opp/blob/master/data_readme.md#oakland-ca\n\n\n\n“Data is deduplicated on raw columns contactdate, contacttime, streetname, subject_sdrace, subject_sex, and subject_age, reducing the number of records by ~5.2%”\n\nThe difference with the report is on this order of magnitude,\nBut varies within groups by several percentage points\nSo deduplication might explain the difference\nBut in a more serious analysis we might want to check, eg, with the Stanford Open Policing folks\n\n\n## Men in 2016 in the report vs. our data: 8.2%\n(24576 - 22563) / 24576\n\n[1] 0.08190918\n\n## Women in 2016 in the report vs. our data: 3.6%\n(7965 - 7677) / 7965\n\n[1] 0.03615819\n\n## All of 2016 in the report vs. our data: 7.1%\n(32569 - 30268) / 32569\n\n[1] 0.07065"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#make-a-plot",
    "href": "content/03-04-oakland-eda.html#make-a-plot",
    "title": "Case Study: Police Stops in Oakland",
    "section": "7. Make a plot",
    "text": "7. Make a plot\nPeng and Matsui note that plots are useful for both checking and setting expectations\n\nAn expectation we formed earlier: fewer stops in 2013\nWe can combine pipes with ggplot() to get the year\ngeom_bar() gives us counts\nWhat’s up with the warnings?\n\n\ndataf |&gt; \n    mutate(year = year(date)) |&gt; \n    ggplot(aes(year)) +\n    geom_bar()\n\nWarning: Removed 2 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\n\nHow about counts per year by race/ethnicity?\n\nThis version is too hard to read\n\n\ndataf |&gt; \n    mutate(year = year(date)) |&gt; \n    ggplot(aes(year, fill = subject_race)) +\n    geom_bar()\n\nWarning: Removed 2 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\n\nLet’s switch from bars to points and lines and change up the color palette\n\nWhy does this produce 2 warnings?\n\n\ndataf |&gt; \n    mutate(year = year(date)) |&gt; \n    ggplot(aes(year, color = subject_race)) +\n    geom_point(stat = 'count') +\n    geom_line(stat = 'count') +\n    scale_color_brewer(palette = 'Set1')\n\nWarning: Removed 2 rows containing non-finite values (`stat_count()`).\nRemoved 2 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\n\nplotly::ggplotly() creates an interactive version of a ggplot object\n\nWhy don’t I need to give it the plot we just created?\n\n\nggplotly()\n\nWarning: Removed 2 rows containing non-finite values (`stat_count()`).\nRemoved 2 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\n\n\n\nThese plots confirm our expectation of lower counts in 2013\nDo they help us develop any new expectations as we move on to addressing our research questions?"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#try-the-easy-solution-first",
    "href": "content/03-04-oakland-eda.html#try-the-easy-solution-first",
    "title": "Case Study: Police Stops in Oakland",
    "section": "8. Try the easy solution first",
    "text": "8. Try the easy solution first\nLet’s translate our natural-language research questions into statistical questions:\n\nWhether Black people in Oakland might be more likely to be stopped than White people\n\n\\[ \\Pr(stopped | Black) \\textrm{ vs } \\Pr(stopped | White) \\]\n\nWhether Black people who are stopped might be more likely to have contraband\n\n\\[ \\Pr(contraband | stopped, searched, Black) \\textrm{ vs } \\Pr(contraband | stopped, searched, White) \\] - The easy solution is to estimate probabilities by calculating rates within groups"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#mathematical-aside",
    "href": "content/03-04-oakland-eda.html#mathematical-aside",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Mathematical aside",
    "text": "Mathematical aside\n\nFor Q1, we can’t calculate \\(\\Pr(stopped | Black)\\) directly:\neveryone in our data was stopped\nBut Bayes’ theorem lets us get at the comparison indirectly\n\n\\[ \\Pr(stopped | Black) = \\frac{\\Pr(Black | stopped) \\times \\Pr(stopped)}{\\Pr(Black)} \\]\n\\[ \\Pr(stopped | Black) : \\Pr(stopped | White) = \\frac{\\Pr(Black|stopped)}{\\Pr(Black)} : \\frac{\\Pr(White|stopped)}{\\Pr(White)} \\]\n\n\\(\\Pr(Black)\\) is the share of the target population that’s Black\nIf we assume the target population = residents of Oakland, can use Census data\nWhy is this assumption probably false? Is it okay to use it anyways?"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#stops-by-race",
    "href": "content/03-04-oakland-eda.html#stops-by-race",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Stops, by race",
    "text": "Stops, by race\n\\[ \\Pr(Black|stopped) \\]\n\ndataf |&gt; \n    count(subject_race) |&gt; \n    mutate(share = n / sum(n)) |&gt; \n    arrange(desc(share)) |&gt; \n    mutate(share = scales::percent(share, accuracy = 1))\n\n# A tibble: 5 × 3\n  subject_race               n share\n  &lt;chr&gt;                  &lt;int&gt; &lt;chr&gt;\n1 black                  78925 59%  \n2 hispanic               26257 20%  \n3 white                  15628 12%  \n4 asian/pacific islander  8099 6%   \n5 other                   4498 3%   \n\n\n\n\nPolice stop demographics\n\n59% of subjects stopped by police are Black\n20% are Hispanic\n12% are White\n6% are API\n\nOakland demographics in 2020: https://en.wikipedia.org/wiki/Oakland,_California#Race_and_ethnicity\n\n24% of residents are Black\n27% are Hispanic\n27% are non-Hispanic White\n16% are Asian\n\n\n\n\n\n\nr/e\nresidents\nstops\nratio\n\n\n\n\nBlack\n24%\n59%\n2.5\n\n\nHispanic\n27%\n20%\n0.7\n\n\nWhite\n27%\n12%\n0.4\n\n\nAPI\n16%\n6%\n0.4\n\n\n\n\nBlacks are severely overrepresented in police stops\nHispanics are slightly underrepresented\nWhite and API folks are significantly underrepresented"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#searches-by-race",
    "href": "content/03-04-oakland-eda.html#searches-by-race",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Searches, by race",
    "text": "Searches, by race\n\nWhat fraction of stops had a search? \\[ \\Pr(searched | stopped) \\]\nAre there disparities by race there? \\[ \\Pr(searched | stopped, Black) \\textrm{ vs } \\Pr(searched | stopped, White) \\]\n\n\n## What fraction of stops had a search? \ndataf |&gt; \n    count(search_conducted) |&gt; \n    mutate(share = n / sum(n))\n\n# A tibble: 2 × 3\n  search_conducted     n share\n  &lt;lgl&gt;            &lt;int&gt; &lt;dbl&gt;\n1 FALSE            92250 0.691\n2 TRUE             41157 0.309\n\n\nAcross all subjects, 31% of stops involved a search.\n\nNow we break down the search rate by race-ethnicity\n\nggplot(dataf, aes(subject_race, fill = search_conducted)) +\n    geom_bar(position = position_fill()) +\n    scale_fill_manual(values = c('transparent', 'red'))\n\n\n\n\n\n\ndataf |&gt; \n    count(subject_race, search_conducted) |&gt; \n    group_by(subject_race) |&gt; \n    mutate(rate = n / sum(n)) |&gt; \n    ungroup() |&gt; \n    filter(search_conducted) |&gt; \n    mutate(rate = scales::percent(rate, accuracy = 1))\n\n# A tibble: 5 × 4\n  subject_race           search_conducted     n rate \n  &lt;chr&gt;                  &lt;lgl&gt;            &lt;int&gt; &lt;chr&gt;\n1 asian/pacific islander TRUE              1304 16%  \n2 black                  TRUE             30025 38%  \n3 hispanic               TRUE              6722 26%  \n4 other                  TRUE               706 16%  \n5 white                  TRUE              2400 15%  \n\n\n\nFor all groups, most stops didn’t involve a search\nFor Black subjects, 38% of stops involved a search\nFor White subjects, 15% of stops involved a search\nBlack subjects were about 2.5x more likely to be searched than White subjects"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#contraband-finds-by-race",
    "href": "content/03-04-oakland-eda.html#contraband-finds-by-race",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Contraband finds, by race",
    "text": "Contraband finds, by race\n\\[ \\Pr(contraband | searched, stopped, Black) \\] We want to filter() down to only stops where a search was conducted\n\ndataf |&gt; \n    filter(search_conducted) |&gt; \n    ggplot(aes(subject_race, fill = contraband_found)) +\n    geom_bar(position = position_fill()) +\n    scale_fill_manual(values = c('transparent', 'blue')) +\n    ylim(0, .2)\n\nWarning: Removed 5 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\n\ndataf |&gt; \n    filter(search_conducted) |&gt; \n    count(subject_race, contraband_found) |&gt; \n    group_by(subject_race) |&gt; \n    mutate(rate = n / sum(n)) |&gt; \n    ungroup() |&gt; \n    filter(contraband_found) |&gt; \n    mutate(rate = scales::percent(rate, accuracy = 1))\n\n# A tibble: 5 × 4\n  subject_race           contraband_found     n rate \n  &lt;chr&gt;                  &lt;lgl&gt;            &lt;int&gt; &lt;chr&gt;\n1 asian/pacific islander TRUE               176 13%  \n2 black                  TRUE              4364 15%  \n3 hispanic               TRUE              1116 17%  \n4 other                  TRUE                85 12%  \n5 white                  TRUE               411 17%  \n\n\n\nFor Black subjects who were searched, contraband was found 15% of the time\nFor White subjects, 17% of the time"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#results",
    "href": "content/03-04-oakland-eda.html#results",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Results",
    "text": "Results\nThis preliminary analysis indicates that\n\nBlack subjects were more likely to be stopped and searched than White subjects; but,\nwhen they were searched, White subjects were more likely to have contraband."
  },
  {
    "objectID": "content/03-04-oakland-eda.html#follow-up",
    "href": "content/03-04-oakland-eda.html#follow-up",
    "title": "Case Study: Police Stops in Oakland",
    "section": "9. Follow up",
    "text": "9. Follow up\nWhat are some further directions we could take this analysis?\n\n\nInvestigate outstanding questions about quality and reliability of the data\n\neg, follow up with Stanford Open Policing Project about the difference in row counts\nFits with epicycle of analysis: checking expectations\n\nBreak down our question into more fine-grained analyses\n\neg, the Oakland web site and report talk about policy changes; do we see changes by year in the data?\nFits with epicycle of analysis: refine and specify research questions\n\nApply more sophisticated statistical analysis\n\neg, a regression model to control for age, gender, and other covariates\nFits with phenomena development: reducing data, eliminating noise, in order to identity local phenomena"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#discussion-questions",
    "href": "content/03-04-oakland-eda.html#discussion-questions",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nSuppose you’ve conducted this EDA because you’re working with an activist organization that promotes defunding the police and prison abolition. Should you share the preliminary findings above with your organization contacts?\nWhat influence would the following factors make to your answer?\n\nFunding: Whether you’re being paid as a consultant vs. volunteering your expertise\nValues: Your own views about policing and prisons\nRelationships: Whether you are friends with members of the activist organization and/or police\nCommunications: The degree to which you can control whether and how the organization will publish your preliminary findings\nTimeliness: Whether these findings are relevant to a pending law or policy change\n\nWhat other factors should be taken into account as you decide whether to share your findings? Or not taken into account?\nHow has this “raw data” been shaped by the journey of the data to get to us?"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#lab",
    "href": "content/03-04-oakland-eda.html#lab",
    "title": "Case Study: Police Stops in Oakland",
    "section": "Lab",
    "text": "Lab\nThe EDA lab walks you through an EDA of Covid-19 vaccine hesitancy data, looking at changes over time across racial-ethnic groups.\nhttps://github.com/data-science-methods/lab-4-eda"
  },
  {
    "objectID": "content/03-04-oakland-eda.html#references",
    "href": "content/03-04-oakland-eda.html#references",
    "title": "Case Study: Police Stops in Oakland",
    "section": "References",
    "text": "References\n\n\n\n\nPeng, Roger D., and Elizabeth Matsui. 2016. The Art of Data Science: A Guide for Anyone Who Works with Data. Leanpub. http://leanpub.com/artofdatascience.\n\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, May, 1–10. https://doi.org/10.1038/s41562-020-0858-1."
  },
  {
    "objectID": "content/04-02-readability.html#reading",
    "href": "content/04-02-readability.html#reading",
    "title": "Readable code is reliable code",
    "section": "Reading",
    "text": "Reading\n\nBryan (2018)\n“What to Look for in a Code Review” (n.d.)\nLamb (2018)\nWickham (n.d.)"
  },
  {
    "objectID": "content/04-02-readability.html#replicability-reproducibility-reliability",
    "href": "content/04-02-readability.html#replicability-reproducibility-reliability",
    "title": "Readable code is reliable code",
    "section": "Replicability, reproducibility, reliability",
    "text": "Replicability, reproducibility, reliability\n\nreplicability\n\nQualitatively similar results when we repeat the experiment\n(gather new data; may or may not use the same code)\n\nreproducibility\n\nQuantitatively identical outputs when we run the same code on the same data\n(same data, same code)\n\nreliability\n\nThe code performs the analysis that we think it’s performing\n(compare construct validity)"
  },
  {
    "objectID": "content/04-02-readability.html#checking-reliability",
    "href": "content/04-02-readability.html#checking-reliability",
    "title": "Readable code is reliable code",
    "section": "Checking reliability",
    "text": "Checking reliability\n\nreliability\n\nThe code performs the analysis that we think it’s performing\n\n\nCan reliability be checked computationally?"
  },
  {
    "objectID": "content/04-02-readability.html#writing-code-is-writing",
    "href": "content/04-02-readability.html#writing-code-is-writing",
    "title": "Readable code is reliable code",
    "section": "Writing code is writing",
    "text": "Writing code is writing\n\nMultiple audiences\n\nCollaborators\n(Some) reviewers and readers of the paper\nPeers who want to analyze and extend your methods\nYourself in six months\n\n\n\nAny code of your own that you haven’t looked at for six or more months might as well have been written by someone else. (“Eagleson’s Law”)\n\n\nYour code is readable to the extent that people can easily assess reliability:\n\npredict,\ndiagnose, and\nextend your code"
  },
  {
    "objectID": "content/04-02-readability.html#code-style",
    "href": "content/04-02-readability.html#code-style",
    "title": "Readable code is reliable code",
    "section": "Code style",
    "text": "Code style\niNéwritteNélanguagEáconventionSéfoRîpunctuatioNøcapitalizatioNîaiDéco\nmprehensioNébYéindicatinGéstructurE\n\n\niNéwritteNélanguagEáconventionSéfoRîpunctuatioNøcapitalizatioNîaiDéco\nmprehensioNébYéindicatinGéstructurE\n\nthis is what it’s like to read poorly-styled code\nconventions only work if they’re shared conventions\n\nStyle guides provide shared conventions for readable code\n\nIn-line spacing makes it easier to pick out distinguish functions, operators, and variables in a line\nReturns distinguish arguments in a function call\nIndentation corresponds to structure of complex expressions\nCommon conventions for naming, assignment reduce cognitive load\n\nTidyverse style guide: https://style.tidyverse.org/ (Wickham n.d.)"
  },
  {
    "objectID": "content/04-02-readability.html#highlights-from-the-tidyverse-style-guide",
    "href": "content/04-02-readability.html#highlights-from-the-tidyverse-style-guide",
    "title": "Readable code is reliable code",
    "section": "Highlights from the Tidyverse style guide",
    "text": "Highlights from the Tidyverse style guide\n\nPlace all package() calls at the top of the script\nLimit your code to 80 characters per line\nUse at least 4 spaces for indenting multiline expression\n\nControl-I in RStudio will do automagic indenting\n\nIn multiline function calls, 1 argument = 1 line\n\n\nlong_function_name &lt;- function(a = \"a long argument\",\n                               b = \"another argument\",\n                               c = \"another long argument\") {\n    # As usual code is indented by four spaces.\n}"
  },
  {
    "objectID": "content/04-02-readability.html#spaces-let-your-code-b-r-e-a-t-h-e",
    "href": "content/04-02-readability.html#spaces-let-your-code-b-r-e-a-t-h-e",
    "title": "Readable code is reliable code",
    "section": "Spaces: Let your code b r e a t h e",
    "text": "Spaces: Let your code b r e a t h e\n\nAlways put spaces after commas, and never before (like English)\nNo space between a function name and the parentheses (like math)\nSpaces on both sides of infix operators (==, +, -, &lt;-, =)\nPipes %&gt;% |&gt; should have a space before and be at the end of the line"
  },
  {
    "objectID": "content/04-02-readability.html#code-blocks",
    "href": "content/04-02-readability.html#code-blocks",
    "title": "Readable code is reliable code",
    "section": "Code blocks",
    "text": "Code blocks\nWhen you put a block of code in curly braces {}:\n\n{ should be the last character on a line\n} should be the first character on the line\n\n\nif (y == 0) {\n  if (x &gt; 0) {\n    log(x)\n  } else {\n    message(\"x is negative or zero\")\n  }\n} else {\n  y^x\n}"
  },
  {
    "objectID": "content/04-02-readability.html#boolean-variables-vs.-control-flow",
    "href": "content/04-02-readability.html#boolean-variables-vs.-control-flow",
    "title": "Readable code is reliable code",
    "section": "Boolean variables vs. control flow",
    "text": "Boolean variables vs. control flow\n\nFunctions that return vectors:\n\n&, |, ==, ifelse(), dplyr::if_else()\n\nFunctions that return a single value:\n\n&&, ||, identical\n\nif (x) a else b only looks at the first (hopefully single) value of x"
  },
  {
    "objectID": "content/04-02-readability.html#code-review-is-peer-feedback-on-your-writing",
    "href": "content/04-02-readability.html#code-review-is-peer-feedback-on-your-writing",
    "title": "Readable code is reliable code",
    "section": "Code review is peer feedback on your writing",
    "text": "Code review is peer feedback on your writing\n\nMay be done formally as part of journal review\n\nBut this is super rare\n\nInformally, with collaborators\nInformally, as a stage of writing"
  },
  {
    "objectID": "content/04-02-readability.html#goals-of-code-review",
    "href": "content/04-02-readability.html#goals-of-code-review",
    "title": "Readable code is reliable code",
    "section": "Goals of code review",
    "text": "Goals of code review\n\nMake the code more reliable\nIntegrate your code into the collaborative code base\nLearning and growth for both author and reviewer\n\nExamples of things that will promote/frustrate these goals?\n(Lamb 2018)"
  },
  {
    "objectID": "content/04-02-readability.html#what-to-look-for-in-code-review",
    "href": "content/04-02-readability.html#what-to-look-for-in-code-review",
    "title": "Readable code is reliable code",
    "section": "What to look for in code review",
    "text": "What to look for in code review\n\nGood things: What does the code do well?\nDesign: The code communicates the author’s intentions.\nFunctionality: The code does what the author intended.\nComplexity: The code isn’t more complex than it needs to be.\nOver-engineering: The developer isn’t implementing things they might need in the future but don’t know they need now.\nTests: Code has appropriate, well-designed unit tests [or uses assertions].\nNaming: The developer used clear names for everything.\nComments: Comments are clear and useful, and mostly explain why instead of what.\nStyle: The code follows the group’s / a standard style guide.\nDocumentation: Does the code explain what functions or long pipes do and how they’re used?\n\n(adapted from “What to Look for in a Code Review” n.d.)\n\nSetup: The documentation/README explains where to get the data and any special software installation.\n\nDependencies: All of the required packages are listed at the top of the script.\nReproducibility: The code runs, and produces the output identified elsewhere (comments, paper text)"
  },
  {
    "objectID": "content/04-02-readability.html#labs-5-6",
    "href": "content/04-02-readability.html#labs-5-6",
    "title": "Readable code is reliable code",
    "section": "Labs 5-6",
    "text": "Labs 5-6\n\nLab 5: Code review of Zhou et al. (2021)\n\nWork in pairs\nWrite a rubric first\nDon’t worry about reproducibility\nStart working on this Thursday, even if you’re not finished with lab 4\n\nLab 6: Reproducibility check of Zhou et al. (2021)\n\nSpecifically, the values reported in Table 1"
  },
  {
    "objectID": "content/04-02-readability.html#project-code-review",
    "href": "content/04-02-readability.html#project-code-review",
    "title": "Readable code is reliable code",
    "section": "Project: Code Review",
    "text": "Project: Code Review\nhttps://data-science-methods.github.io/project.html#code-review-and-reproducibility-check\n\nI’ll assign you to review someone else’s EDA code\nFork and clone the code\nSubmit your feedback using a PR"
  },
  {
    "objectID": "content/04-02-readability.html#references",
    "href": "content/04-02-readability.html#references",
    "title": "Readable code is reliable code",
    "section": "References",
    "text": "References\n\n\n\n\nBryan, Jenny. 2018. “Code Smells and Feels.” Presented at the useR 2018, July 21. https://www.youtube.com/watch?v=7oyiPBjLAWY.\n\n\nLamb, Julianna. 2018. “Building an Inclusive Code Review Culture.” Plaid. July 30, 2018. https://plaid.com/blog/building-an-inclusive-code-review-culture/.\n\n\n“What to Look for in a Code Review.” n.d. eng-practices. Accessed October 23, 2023. https://google.github.io/eng-practices/review/reviewer/looking-for.html.\n\n\nWickham, Hadley. n.d. The Tidyverse Style Guide. Accessed November 6, 2023. https://style.tidyverse.org/.\n\n\nZhou, Xiaodan, Kevin Josey, Leila Kamareddine, Miah C. Caine, Tianjia Liu, Loretta J. Mickley, Matthew Cooper, and Francesca Dominici. 2021. “Excess of COVID-19 Cases and Deaths Due to Fine Particulate Matter Exposure During the 2020 Wildfires in the United States.” Science Advances 7 (33): eabi8789. https://doi.org/10.1126/sciadv.abi8789."
  },
  {
    "objectID": "content/04-05-data-management.html#reading",
    "href": "content/04-05-data-management.html#reading",
    "title": "Project and data management",
    "section": "Reading",
    "text": "Reading\n\nLaskowski (n.d.)\nNoble (2009)\nWilkinson et al. (2016)\nJennings et al. (2023)"
  },
  {
    "objectID": "content/04-05-data-management.html#some-data-management-disasters",
    "href": "content/04-05-data-management.html#some-data-management-disasters",
    "title": "Project and data management",
    "section": "Some data management disasters",
    "text": "Some data management disasters\n\nThe Economist (2011), “Video: Keith Baggerly, \"When Is Reproducibility an Ethical Issue? Genomics, Personalized Medicine, and Human Error\"” (n.d.)\nHerndon, Ash, and Pollin (2014), but you can just read Bailey and Borwein (Jon) (n.d.), Cassidy (n.d.), and/or watch Reinhart & Rogoff - Growth in a Time of Debt - EXERCISE! (2019)\nLaskowski (n.d.), Viglione (2020), Pennisi (2020)\n“How Excel May Have Caused Loss of 16,000 Covid Tests in England” (2020)"
  },
  {
    "objectID": "content/04-05-data-management.html#dumpster-organization",
    "href": "content/04-05-data-management.html#dumpster-organization",
    "title": "Project and data management",
    "section": "Dumpster organization",
    "text": "Dumpster organization\n\n\n\n😱 Source: https://pbs.twimg.com/media/DFca5SRXsAAx1NA\n\n\n\nDump all of your files into one place\nUse search tools to find what you want\nJust assume that things aren’t getting corrupted\nThe way many Gen Z students think about their files? (Chin 2021)"
  },
  {
    "objectID": "content/04-05-data-management.html#the-first-rule-of-data-management-do-not-edit-your-data",
    "href": "content/04-05-data-management.html#the-first-rule-of-data-management-do-not-edit-your-data",
    "title": "Project and data management",
    "section": "The first rule of data management Do not edit your data",
    "text": "The first rule of data management Do not edit your data"
  },
  {
    "objectID": "content/04-05-data-management.html#project-organization",
    "href": "content/04-05-data-management.html#project-organization",
    "title": "Project and data management",
    "section": "Project organization",
    "text": "Project organization\n\n\n\nNoble’s (2009) sample folder structure is designed for experimental biologists\n\n\n\nKeep your project self-contained\nLocate files quickly\nPlay nicely with version control\nSelf-document key relationships between project files\nWork with your data without editing it"
  },
  {
    "objectID": "content/04-05-data-management.html#a-model-for-computational-social-science-projects",
    "href": "content/04-05-data-management.html#a-model-for-computational-social-science-projects",
    "title": "Project and data management",
    "section": "A model for computational social science projects",
    "text": "A model for computational social science projects\n.\n├── R\n├── data\n├── paper\n│   ├── _quarto.yml\n│   ├── paper.qmd\n├── plots\n├── readme.md\n├── scripts\n└── talk\n\nhttps://github.com/dhicks/project_template\nConfigured as a GitHub “template,” making it easy to create new repositories for new projects\nDesignated folders for data, plots/outputs, and utility functions"
  },
  {
    "objectID": "content/04-05-data-management.html#file-naming-convention",
    "href": "content/04-05-data-management.html#file-naming-convention",
    "title": "Project and data management",
    "section": "File naming convention",
    "text": "File naming convention\nFiles in scripts, data, and plots should generally use a sequential naming convention:\n\nScripts in scripts should have filenames starting with 01_:\n\n01_scrape.R\n02_parse.R\n03_eda.R, and so on\n\nData and plot files (data and plots) should use a parallel naming convention:\n\n00_ indicates raw data (produced or gathered outside of the pipeline in scripts)\n01_ indicates plots and intermediate data files produced by script number 01, and so on"
  },
  {
    "objectID": "content/04-05-data-management.html#the-model-in-action-a-mid-sized-text-mining-project",
    "href": "content/04-05-data-management.html#the-model-in-action-a-mid-sized-text-mining-project",
    "title": "Project and data management",
    "section": "The model in action: A mid-sized text-mining project",
    "text": "The model in action: A mid-sized text-mining project\nPublished paper: https://doi.org/10.1162/qss_a_00150\nGitHub repo: https://github.com/dhicks/orus\n23 directories, 274 files (plus 160k data files)\n.\n├── ORU\\ faculty\n│   └── ORU\\ Publications.fld\n├── QSS\\ forms\n├── R\n├── data\n│   ├── author_histories\n│   ├── authors_meta\n│   ├── docs\n│   ├── ldatuning_results\n│   ├── parsed_blocks\n│   ├── pubs\n│   └── temp\n├── paper\n│   ├── img\n│   └── scraps\n├── plots\n├── presentations\n└── scripts\n    ├── 12_analysis_cache\n    │   └── html\n    ├── 12_analysis_files\n    │   └── figure-html\n    └── scraps"
  },
  {
    "objectID": "content/04-05-data-management.html#the-analysis-pipeline",
    "href": "content/04-05-data-management.html#the-analysis-pipeline",
    "title": "Project and data management",
    "section": "The analysis pipeline",
    "text": "The analysis pipeline\n├── scripts\n│   ├── 01_parse_faculty_list.R\n│   ├── 02_Scopus_search_results.R\n│   ├── 03_match.R\n│   ├── 03_matched.csv\n│   ├── 04_author_meta.R\n│   ├── 05_filtering.R\n│   ├── 06_author_histories.R\n│   ├── 07_complete_histories.R\n│   ├── 08_text_annotation.R\n│   ├── 09_build_vocab.R\n│   ├── 10_topic_modeling.R\n│   ├── 11_depts.R\n│   ├── 11_depts.html\n│   ├── 12_analysis\\ copy.html\n│   ├── 12_analysis-matched.html\n│   ├── 12_analysis.R\n│   ├── 12_analysis.html\n│   ├── 12_analysis_cache\n│   │   └── html\n│   │       ├── __packages\n│   │       ...\n│   │       └── topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdx\n│   ├── 12_analysis_files\n│   │   └── figure-html\n│   │       ├── author-dept\\ distance-1.png\n│   │       ...\n│   │       └── topic_viz-2.png\n│   ├── api_key.R\n│   └── scraps\n│       ├── 02_parse_pubs_list.R\n│       ├── 03_coe_pubs.R\n│       ├── 03_match_auids.R\n│       ├── 07.R\n│       ├── 12_regressions.R\n│       ├── BML-CMSI\\ deep\\ dive.R\n│       ├── Hellinger_low_memory.R\n│       ├── dept_hell_net.R\n│       ├── divergence\\ against\\ lagged\\ distributions.R\n│       ├── exploring\\ topics.R\n│       ├── fractional_authorship.R\n│       ├── hellinger.R\n│       ├── model_scratch.R\n│       ├── multicore.R\n│       ├── net_viz.R\n│       ├── prcomp.R\n│       ├── propensity.R\n│       ├── rs_diversity.R\n│       ├── spacyr.R\n│       ├── topic\\ counts\\ rather\\ than\\ entropies.R\n│       ├── topic_cosine_sim.R\n│       ├── unit-level.R\n│       ├── weighted\\ regression.R\n│       ├── word-topic_distance.R\n│       ├── xx_construct_samples.R\n│       └── xx_oru_complete_histories.R"
  },
  {
    "objectID": "content/04-05-data-management.html#intermediate-data-files",
    "href": "content/04-05-data-management.html#intermediate-data-files",
    "title": "Project and data management",
    "section": "Intermediate data files",
    "text": "Intermediate data files\ndata\n├── *ORUs\\ -\\ DSL\\ -\\ Google\\ Drive.webloc\n├── 00_UCD_2016.csv\n├── 00_UCD_2017.csv\n├── 00_UCD_2018.csv\n├── 00_faculty_list.html\n├── 00_manual_matches.csv\n├── 00_publications_list.html\n├── 01_departments.csv\n├── 01_departments_canonical.csv\n├── 01_faculty.Rds\n├── 02_pubs.Rds\n├── 03_codepartmentals.Rds\n├── 03_dropout.Rds\n├── 03_matched.Rds\n├── 03_unmatched.Rds\n├── 04_author_meta.Rds\n├── 04_dropouts.Rds\n├── 04_genderize\n├── 04_namsor.Rds\n├── 05_author_meta.Rds\n├── 05_dept_dummies.Rds\n├── 05_dropouts.Rds\n├── 05_layout.Rds\n├── 05_matched.Rds\n├── 06_author_histories.Rds\n├── 07_coauth_count.Rds\n├── 07_parsed_histories.Rds\n├── 08_phrases.Rds\n├── 09_H.Rds\n├── 09_atm.csv\n├── 09_vocab.tex\n├── 10_atm.csv\n├── 10_atm_pc.Rds\n├── 10_aytm.csv\n├── 10_aytm_comp.csv\n├── 10_aytm_did.csv\n├── 10_model_stats.Rds\n├── 10_models.Rds\n├── 11_au_dept_xwalk.Rds\n├── 11_departments.csv\n├── 11_departments_canonical.csv\n├── 11_dept_dummies.Rds\n├── 11_dept_gamma.Rds\n├── 11_dept_term_matrix.Rds\n├── 11_oru_gamma.Rds\n├── 11_oru_term_matrix.Rds\n├── 11_test_train.Rds\n├── 12_layout.Rds\n├── author_histories [7665 entries exceeds filelimit, not opening dir]\n├── authors_meta [6020 entries exceeds filelimit, not opening dir]\n├── docs [145144 entries exceeds filelimit, not opening dir]\n├── ldatuning_results\n│   ├── tuningResult_comp.Rds\n│   ├── tuningResult_comp.docx\n│   ├── tuningResult_comp.pdf\n│   ├── tuningResult_did.Rds\n│   └── tuningResult_did.pdf\n├── ldatuning_results-20190415T164055Z-001.zip\n├── parsed_blocks [430 entries exceeds filelimit, not opening dir]\n├── pubs [282 entries exceeds filelimit, not opening dir]\n└── temp"
  },
  {
    "objectID": "content/04-05-data-management.html#a-reminder-on-paths",
    "href": "content/04-05-data-management.html#a-reminder-on-paths",
    "title": "Project and data management",
    "section": "A reminder on paths",
    "text": "A reminder on paths\n\nWindows and Unix-based systems write paths differently\nUse file.path() or (even better) the here package to construct paths"
  },
  {
    "objectID": "content/04-05-data-management.html#exercise-organize-your-eda",
    "href": "content/04-05-data-management.html#exercise-organize-your-eda",
    "title": "Project and data management",
    "section": "Exercise: Organize your EDA",
    "text": "Exercise: Organize your EDA\n.\n├── R\n├── data\n├── paper\n├── plots\n├── readme.md\n├── scripts\n└── talk"
  },
  {
    "objectID": "content/04-05-data-management.html#data-management-plans",
    "href": "content/04-05-data-management.html#data-management-plans",
    "title": "Project and data management",
    "section": "Data management plans",
    "text": "Data management plans\n\nMuch like a research plan, data management plans provide an overview of the steps you’ll take to gather, publish, and maintain your data\n\nSince 2011, NSF has required a 2-page data management plan for most types of proposals\n\n\n\nCommon elements\n\nWho is responsible for data management\nWho else will have access to which data\nHow data will be collected\nData formatting standards\nWhether and how data will be archived and made available for reuse"
  },
  {
    "objectID": "content/04-05-data-management.html#data-management-plan-examples-and-resources",
    "href": "content/04-05-data-management.html#data-management-plan-examples-and-resources",
    "title": "Project and data management",
    "section": "Data management plan examples and resources",
    "text": "Data management plan examples and resources\n\nUCM Library\nUCSD NSF examples\n\nSBE example 1\nSBE example 2\n\nNSF policy summary\n\nSBE-specific guidance"
  },
  {
    "objectID": "content/04-05-data-management.html#fair-principles-for-published-data",
    "href": "content/04-05-data-management.html#fair-principles-for-published-data",
    "title": "Project and data management",
    "section": "FAIR principles for published data",
    "text": "FAIR principles for published data\n\nFindable\n\nF1. (meta)data are assigned a globally unique and persistent identifier\nF2. data are described with rich metadata (defined by R1 below)\nF3. metadata clearly and explicitly include the identifier of the data it describes\nF4. (meta)data are registered or indexed in a searchable resource\n\nAccessible\n\nA1. (meta)data are retrievable by their identifier using a standardized communications protocol\n\nA1.1 the protocol is open, free, and universally implementable\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\n\nA2. metadata are accessible, even when the data are no longer available\n\nInteroperable\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nI2. (meta)data use vocabularies that follow FAIR principles\nI3. (meta)data include qualified references to other (meta)data\n\nReusable\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes\n\nR1.1. (meta)data are released with a clear and accessible data usage license\nR1.2. (meta)data are associated with detailed provenance\nR1.3. (meta)data meet domain-relevant community standards"
  },
  {
    "objectID": "content/04-05-data-management.html#care-principles",
    "href": "content/04-05-data-management.html#care-principles",
    "title": "Project and data management",
    "section": "CARE principles",
    "text": "CARE principles\n\nApplications of FAIR Principles have the potential to neglect the rights of Indigenous Peoples and their protocols for cultural, spiritual and ecological information. (Jennings et al. 2023)\n\n\nCollective benefit\n\nC1. For inclusive development and innovation\nC2. For improved governance and citizen engagement\nC3. For equitable outcomes\n\nAuthority to control\n\nA1. Recognizing rights and interests\nA2. Data for governance [self-governance and self-determination]\nA3. Governance of data\n\nResponsibility\n\nR1. For positive relationships\nR2. For expanding capability and capacity\nR3. For Indigenous languages and worldviews\n\nEthics\n\nE1. For minimizing harm and maximizing benefit\nE2. For justice\nE3. For future use\n\n\nhttps://www.gida-global.org/care"
  },
  {
    "objectID": "content/04-05-data-management.html#references",
    "href": "content/04-05-data-management.html#references",
    "title": "Project and data management",
    "section": "References",
    "text": "References\n\n\n\n\nBailey, David H., and Jonathan Borwein (Jon). n.d. “The Reinhart-Rogoff Error – or How Not to Excel at Economics.” The Conversation. Accessed May 16, 2020. http://theconversation.com/the-reinhart-rogoff-error-or-how-not-to-excel-at-economics-13646.\n\n\nCassidy, John. n.d. “The Reinhart and Rogoff Controversy: A Summing Up.” The New Yorker. Accessed September 27, 2020. https://www.newyorker.com/news/john-cassidy/the-reinhart-and-rogoff-controversy-a-summing-up.\n\n\nChin, Monica. 2021. “Students Who Grew up with Search Engines Might Change STEM Education Forever.” The Verge. September 22, 2021. https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z.\n\n\nHerndon, Thomas, Michael Ash, and Robert Pollin. 2014. “Does High Public Debt Consistently Stifle Economic Growth? A Critique of Reinhart and Rogoff.” Cambridge Journal of Economics 38 (2): 257–79. https://doi.org/10.1093/cje/bet075.\n\n\n“How Excel May Have Caused Loss of 16,000 Covid Tests in England.” 2020. The Guardian. October 5, 2020. http://www.theguardian.com/politics/2020/oct/05/how-excel-may-have-caused-loss-of-16000-covid-tests-in-england.\n\n\nJennings, Lydia, Talia Anderson, Andrew Martinez, Rogena Sterling, Dominique David Chavez, Ibrahim Garba, Maui Hudson, Nanibaa’ A. Garrison, and Stephanie Russo Carroll. 2023. “Applying the ‘CARE Principles for Indigenous Data Governance’ to Ecology and Biodiversity Research.” Nature Ecology & Evolution, August, 1–5. https://doi.org/10.1038/s41559-023-02161-2.\n\n\nLaskowski, Kate. n.d. “What to Do When You Don’t Trust Your Data Anymore – Laskowski Lab at UC Davis.” Accessed January 29, 2020. https://laskowskilab.faculty.ucdavis.edu/2020/01/29/retractions/.\n\n\nNoble, William Stafford. 2009. “A Quick Guide to Organizing Computational Biology Projects.” PLOS Computational Biology 5 (7): e1000424. https://doi.org/10.1371/journal.pcbi.1000424.\n\n\nPennisi, Elizabeth. 2020. “Prominent Spider Biologist Spun a Web of Questionable Data.” Science 367 (6478): 613–14. https://doi.org/10.1126/science.367.6478.613.\n\n\nReinhart & Rogoff - Growth in a Time of Debt - EXERCISE! 2019. https://www.youtube.com/watch?v=ItGMz0ERvcw.\n\n\nThe Economist. 2011. “An Array of Errors,” September 10, 2011. https://www.economist.com/science-and-technology/2011/09/10/an-array-of-errors.\n\n\n“Video: Keith Baggerly, \"When Is Reproducibility an Ethical Issue? Genomics, Personalized Medicine, and Human Error\".” n.d. Accessed September 23, 2020. http://www.birs.ca/events/2013/5-day-workshops/13w5083/videos/watch/201308141121-Baggerly.html.\n\n\nViglione, Giuliana. 2020. “‘Avalanche’ of Spider-Paper Retractions Shakes Behavioural-Ecology Community.” Nature, February. https://doi.org/10.1038/d41586-020-00287-y.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1, 1): 1–9. https://doi.org/10.1038/sdata.2016.18."
  }
]