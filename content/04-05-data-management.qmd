# Project and data management

## Reading

- @LaskowskiWhatWhenYou
- @NobleQuickGuideOrganizing2009
- @WilkinsonFAIRGuidingPrinciples2016
- @JenningsApplyingCAREPrinciples2023

## Some data management disasters

- @ArrayErrors2011, @VideoKeithBaggerly
- @HerndonDoesHighPublic2014, but you can just read @BaileyReinhartRogoffErrorHow, @CassidyReinhartRogoffControversy, and/or watch @ReinhartRogoffGrowth2019
- @LaskowskiWhatWhenYou, @ViglioneAvalancheSpiderpaperRetractions2020, @PennisiProminentSpiderBiologist2020
- @HowExcelMay2020

## Dumpster organization

![ðŸ˜± Source: <https://pbs.twimg.com/media/DFca5SRXsAAx1NA>](images/04-reproducibility/dumpster.jpg)

- Dump all of your files into one place
- Use search tools to find what you want
- Just assume that things aren't getting corrupted
- The way many Gen Z students think about their files?  [@ChinStudentsWhoGrew2021]

## The first rule of data management</br></br> Do not edit your data {.center style="font-size: 200%; text-align: center;"}


## Project organization

![Noble's [-@NobleQuickGuideOrganizing2009] sample folder structure is designed for experimental biologists](images/04-reproducibility/noble.png)

- Keep your project self-contained
- Locate files quickly
- Play nicely with version control
- *Self-document key relationships between project files*
- *Work with your data without editing it*

## A model for computational social science projects

```
.
â”œâ”€â”€ R
â”œâ”€â”€ data
â”œâ”€â”€ paper
â”‚Â Â  â”œâ”€â”€ _quarto.yml
â”‚Â Â  â”œâ”€â”€ paper.qmd
â”œâ”€â”€ plots
â”œâ”€â”€ readme.md
â”œâ”€â”€ scripts
â””â”€â”€ talk
```

- <https://github.com/dhicks/project_template>

- Configured as a GitHub "template," making it easy to create new repositories for new projects

- Designated folders for data, plots/outputs, and utility functions

## File naming convention

Files in `scripts`, `data`, and `plots` should generally use a sequential naming convention:  

- Scripts in `scripts` should have filenames starting with `01_`:
	- `01_scrape.R`
	- `02_parse.R`
	- `03_eda.R`, and so on
	
- Data and plot files (`data` and `plots`) should use a parallel naming convention:  
	- `00_` indicates raw data (produced or gathered outside of the pipeline in `scripts`)
	- `01_` indicates plots and intermediate data files produced by script number `01`, and so on


## The model in action: A mid-sized text-mining project

Published paper: <https://doi.org/10.1162/qss_a_00150>\
GitHub repo: <https://github.com/dhicks/orus>\
23 directories, 274 files (plus 160k data files)

```
.
â”œâ”€â”€ ORU\ faculty
â”‚Â Â  â””â”€â”€ ORU\ Publications.fld
â”œâ”€â”€ QSS\ forms
â”œâ”€â”€ R
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ author_histories
â”‚Â Â  â”œâ”€â”€ authors_meta
â”‚Â Â  â”œâ”€â”€ docs
â”‚Â Â  â”œâ”€â”€ ldatuning_results
â”‚Â Â  â”œâ”€â”€ parsed_blocks
â”‚Â Â  â”œâ”€â”€ pubs
â”‚Â Â  â””â”€â”€ temp
â”œâ”€â”€ paper
â”‚Â Â  â”œâ”€â”€ img
â”‚Â Â  â””â”€â”€ scraps
â”œâ”€â”€ plots
â”œâ”€â”€ presentations
â””â”€â”€ scripts
    â”œâ”€â”€ 12_analysis_cache
    â”‚Â Â  â””â”€â”€ html
    â”œâ”€â”€ 12_analysis_files
    â”‚Â Â  â””â”€â”€ figure-html
    â””â”€â”€ scraps
```

## The analysis pipeline

```
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ 01_parse_faculty_list.R
â”‚Â Â  â”œâ”€â”€ 02_Scopus_search_results.R
â”‚Â Â  â”œâ”€â”€ 03_match.R
â”‚Â Â  â”œâ”€â”€ 03_matched.csv
â”‚Â Â  â”œâ”€â”€ 04_author_meta.R
â”‚Â Â  â”œâ”€â”€ 05_filtering.R
â”‚Â Â  â”œâ”€â”€ 06_author_histories.R
â”‚Â Â  â”œâ”€â”€ 07_complete_histories.R
â”‚Â Â  â”œâ”€â”€ 08_text_annotation.R
â”‚Â Â  â”œâ”€â”€ 09_build_vocab.R
â”‚Â Â  â”œâ”€â”€ 10_topic_modeling.R
â”‚Â Â  â”œâ”€â”€ 11_depts.R
â”‚Â Â  â”œâ”€â”€ 11_depts.html
â”‚Â Â  â”œâ”€â”€ 12_analysis\ copy.html
â”‚Â Â  â”œâ”€â”€ 12_analysis-matched.html
â”‚Â Â  â”œâ”€â”€ 12_analysis.R
â”‚Â Â  â”œâ”€â”€ 12_analysis.html
â”‚Â Â  â”œâ”€â”€ 12_analysis_cache
â”‚Â Â  â”‚Â Â  â””â”€â”€ html
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ __packages
â”‚Â Â  â”‚Â Â      ...
â”‚Â Â  â”‚Â Â      â””â”€â”€ topic_viz_41d0cb157a88d4ec41810a16e769f5d5.rdx
â”‚Â Â  â”œâ”€â”€ 12_analysis_files
â”‚Â Â  â”‚Â Â  â””â”€â”€ figure-html
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ author-dept\ distance-1.png
â”‚Â Â  â”‚Â Â      ...
â”‚Â Â  â”‚Â Â      â””â”€â”€ topic_viz-2.png
â”‚Â Â  â”œâ”€â”€ api_key.R
â”‚Â Â  â””â”€â”€ scraps
â”‚Â Â      â”œâ”€â”€ 02_parse_pubs_list.R
â”‚Â Â      â”œâ”€â”€ 03_coe_pubs.R
â”‚Â Â      â”œâ”€â”€ 03_match_auids.R
â”‚Â Â      â”œâ”€â”€ 07.R
â”‚Â Â      â”œâ”€â”€ 12_regressions.R
â”‚Â Â      â”œâ”€â”€ BML-CMSI\ deep\ dive.R
â”‚Â Â      â”œâ”€â”€ Hellinger_low_memory.R
â”‚Â Â      â”œâ”€â”€ dept_hell_net.R
â”‚Â Â      â”œâ”€â”€ divergence\ against\ lagged\ distributions.R
â”‚Â Â      â”œâ”€â”€ exploring\ topics.R
â”‚Â Â      â”œâ”€â”€ fractional_authorship.R
â”‚Â Â      â”œâ”€â”€ hellinger.R
â”‚Â Â      â”œâ”€â”€ model_scratch.R
â”‚Â Â      â”œâ”€â”€ multicore.R
â”‚Â Â      â”œâ”€â”€ net_viz.R
â”‚Â Â      â”œâ”€â”€ prcomp.R
â”‚Â Â      â”œâ”€â”€ propensity.R
â”‚Â Â      â”œâ”€â”€ rs_diversity.R
â”‚Â Â      â”œâ”€â”€ spacyr.R
â”‚Â Â      â”œâ”€â”€ topic\ counts\ rather\ than\ entropies.R
â”‚Â Â      â”œâ”€â”€ topic_cosine_sim.R
â”‚Â Â      â”œâ”€â”€ unit-level.R
â”‚Â Â      â”œâ”€â”€ weighted\ regression.R
â”‚Â Â      â”œâ”€â”€ word-topic_distance.R
â”‚Â Â      â”œâ”€â”€ xx_construct_samples.R
â”‚Â Â      â””â”€â”€ xx_oru_complete_histories.R
```


## Intermediate data files

```
data
â”œâ”€â”€ *ORUs\ -\ DSL\ -\ Google\ Drive.webloc
â”œâ”€â”€ 00_UCD_2016.csv
â”œâ”€â”€ 00_UCD_2017.csv
â”œâ”€â”€ 00_UCD_2018.csv
â”œâ”€â”€ 00_faculty_list.html
â”œâ”€â”€ 00_manual_matches.csv
â”œâ”€â”€ 00_publications_list.html
â”œâ”€â”€ 01_departments.csv
â”œâ”€â”€ 01_departments_canonical.csv
â”œâ”€â”€ 01_faculty.Rds
â”œâ”€â”€ 02_pubs.Rds
â”œâ”€â”€ 03_codepartmentals.Rds
â”œâ”€â”€ 03_dropout.Rds
â”œâ”€â”€ 03_matched.Rds
â”œâ”€â”€ 03_unmatched.Rds
â”œâ”€â”€ 04_author_meta.Rds
â”œâ”€â”€ 04_dropouts.Rds
â”œâ”€â”€ 04_genderize
â”œâ”€â”€ 04_namsor.Rds
â”œâ”€â”€ 05_author_meta.Rds
â”œâ”€â”€ 05_dept_dummies.Rds
â”œâ”€â”€ 05_dropouts.Rds
â”œâ”€â”€ 05_layout.Rds
â”œâ”€â”€ 05_matched.Rds
â”œâ”€â”€ 06_author_histories.Rds
â”œâ”€â”€ 07_coauth_count.Rds
â”œâ”€â”€ 07_parsed_histories.Rds
â”œâ”€â”€ 08_phrases.Rds
â”œâ”€â”€ 09_H.Rds
â”œâ”€â”€ 09_atm.csv
â”œâ”€â”€ 09_vocab.tex
â”œâ”€â”€ 10_atm.csv
â”œâ”€â”€ 10_atm_pc.Rds
â”œâ”€â”€ 10_aytm.csv
â”œâ”€â”€ 10_aytm_comp.csv
â”œâ”€â”€ 10_aytm_did.csv
â”œâ”€â”€ 10_model_stats.Rds
â”œâ”€â”€ 10_models.Rds
â”œâ”€â”€ 11_au_dept_xwalk.Rds
â”œâ”€â”€ 11_departments.csv
â”œâ”€â”€ 11_departments_canonical.csv
â”œâ”€â”€ 11_dept_dummies.Rds
â”œâ”€â”€ 11_dept_gamma.Rds
â”œâ”€â”€ 11_dept_term_matrix.Rds
â”œâ”€â”€ 11_oru_gamma.Rds
â”œâ”€â”€ 11_oru_term_matrix.Rds
â”œâ”€â”€ 11_test_train.Rds
â”œâ”€â”€ 12_layout.Rds
â”œâ”€â”€ author_histories [7665 entries exceeds filelimit, not opening dir]
â”œâ”€â”€ authors_meta [6020 entries exceeds filelimit, not opening dir]
â”œâ”€â”€ docs [145144 entries exceeds filelimit, not opening dir]
â”œâ”€â”€ ldatuning_results
â”‚Â Â  â”œâ”€â”€ tuningResult_comp.Rds
â”‚Â Â  â”œâ”€â”€ tuningResult_comp.docx
â”‚Â Â  â”œâ”€â”€ tuningResult_comp.pdf
â”‚Â Â  â”œâ”€â”€ tuningResult_did.Rds
â”‚Â Â  â””â”€â”€ tuningResult_did.pdf
â”œâ”€â”€ ldatuning_results-20190415T164055Z-001.zip
â”œâ”€â”€ parsed_blocks [430 entries exceeds filelimit, not opening dir]
â”œâ”€â”€ pubs [282 entries exceeds filelimit, not opening dir]
â””â”€â”€ temp
```

## A reminder on paths

- Windows and Unix-based systems write paths differently
- Use `file.path()` or (even better) the `here` package to construct paths

## Exercise: Organize your EDA


```
.
â”œâ”€â”€ R
â”œâ”€â”€ data
â”œâ”€â”€ paper
â”œâ”€â”€ plots
â”œâ”€â”€ readme.md
â”œâ”€â”€ scripts
â””â”€â”€ talk
```


## Data management plans ##

- Much like a research plan, data management plans provide an overview of the steps you'll take to gather, publish, and maintain your data
    - Since 2011, NSF has required a 2-page data management plan for most types of proposals
    
### Common elements

- Who is responsible for data management
- Who else will have access to which data
- How data will be collected
- Data formatting standards
- Whether and how data will be archived and made available for reuse


## Data management plan examples and resources

- [UCM Library](https://library.ucmerced.edu/research/researchers/research-data-management/data-management-plans)
- [UCSD NSF examples](https://library.ucsd.edu/lpw-staging/research-and-collections/data-curation/data-management/dmp-samples.html)
    - [SBE example 1](https://library.ucsd.edu/lpw-staging/research-and-collections/data-curation/data-management/dmpsample/DMP-Example-Ayelet-Gneezy.pdf)
    - [SBE example 2](https://library.ucsd.edu/lpw-staging/research-and-collections/data-curation/data-management/dmpsample/DMP-Example-Wixted.pdf)
- [NSF policy summary](https://www.nsf.gov/bfa/dias/policy/dmp.jsp)
    - [SBE-specific guidance](https://www.nsf.gov/sbe/DMP/SBE_DataMgmtPlanPolicy_RevisedApril2018.pdf)
    


## FAIR principles for published data ##

- *Findable*
    - F1. (meta)data are assigned a globally unique and persistent identifier
    - F2. data are described with rich metadata (defined by R1 below)
    - F3. metadata clearly and explicitly include the identifier of the data it describes 
    - F4. (meta)data are registered or indexed in a searchable resource
- *Accessible*
    - A1. (meta)data are retrievable by their identifier using a standardized communications protocol 
        - A1.1 the protocol is open, free, and universally implementable
        - A1.2 the protocol allows for an authentication and authorization procedure, where necessary 
    - A2. metadata are accessible, even when the data are no longer available
- *Interoperable*
    - I1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation. 
    - I2. (meta)data use vocabularies that follow FAIR principles
    - I3. (meta)data include qualified references to other (meta)data
- *Reusable*
    - R1. meta(data) are richly described with a plurality of accurate and relevant attributes 
        - R1.1. (meta)data are released with a clear and accessible data usage license
        - R1.2. (meta)data are associated with detailed provenance
        - R1.3. (meta)data meet domain-relevant community standards

## CARE principles 

> Applications of FAIR Principles have the potential to neglect the rights of Indigenous Peoples and their protocols for cultural, spiritual and ecological information. [@JenningsApplyingCAREPrinciples2023]

- *Collective benefit*
    - C1. For inclusive development and innovation
    - C2. For improved governance and citizen engagement
    - C3. For equitable outcomes
- *Authority to control*
    - A1. Recognizing rights and interests
    - A2. Data for governance [self-governance and self-determination]
    - A3. Governance of data
- *Responsibility*
    - R1. For positive relationships
    - R2. For expanding capability and capacity
    - R3. For Indigenous languages and worldviews
- *Ethics*
    - E1. For minimizing harm and maximizing benefit
    - E2. For justice
    - E3. For future use

[<https://www.gida-global.org/care>]{.tiny}


## References 


    