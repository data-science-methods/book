# Reproducibility and Software Engineering #

- Reading: @PostolovskiYourCodeReview2020; @WilsonGoodEnoughPractices2017; @BuckensSelfdocumentingMythHow2019
- Recommended video:  @McElreathScienceAmateurSoftware2020

- The reproducibility side of the replication crisis suggests that academic science has a reproducibility problem
- But software engineering doesn't


## What can we learn from software engineering? ##

- Things we've already talked about
    - develop in a clean workspace
        - restart and rerun often
    - version control

- Things we'll talk about in this section of the course
    - using a style guide and linter
    - code review
    - the life-changing magic of tidying up (your files)
    - documenting code and data

- Things we'll talk about later, time permitting
    - `make` and continuous integration for automating reproducibility
    - unit testing for validating code


## Code style ##

- In written language, conventions for punctuation, capitalization aid comprehension by indicating structure

- Style guides do the same thing for code
    - In-line spacing makes it easier to pick out distinguish functions, operators, and variables in a line
    - Returns distinguish arguments in a function call
    - Indentation corresponds to structure of complex expressions
    - Common conventions for naming, assignment reduce cognitive load

- Tidyverse style guide:  <https://style.tidyverse.org/>

- `linr`: <https://cran.r-project.org/web/packages/lintr/index.html>


## A code review checklist ##

@PostolovskiYourCodeReview2020

1. **Readability / understandability**
    - [File naming](https://style.tidyverse.org/files.html)
    - Follows a [style guide](https://style.tidyverse.org/)
        - Or at least uses consistent style
    - Whitespace, whitespace, whitespace
    - Meaningful names / [self-documenting code](https://dev.to/woubuc/self-documenting-is-a-myth-and-how-to-make-your-code-self-documenting-3h2n)
    
2. **Maintainability**
    - [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)
    - Avoid hard-coded configuration
    - Check strong assumptions about structure and content of inputs
    - Avoid deprecated or removed features

3. Security
    - How are passwords for websites, APIs, and sensitive data handled? 
    - Data validation
    - Code validation (tests and assertions)

4. Speed and Performance
    - Performance for users
        - `tictoc` for simple wall time checks
        - Profiling for more thorough optimization
    - Resource use
        - Does this really need to be run on the MERCED cluster? 
        - How much will this cost the lab/team if it's accidentally run 1,000 times?
        - Cache raw returns from web scraping and API queries

5. **Documentation**
    - Scripts: Section markers, signposting of what's happening and why
    - Functions: Assumptions about inputs, explanation of output
        - Consider using `roxygen2` + R's package infrastructure
        - <https://r-pkgs.org/man.html#man-functions>
    - Data: Provenance, date and time retrieved, codebook
    - Project: README, NEWS

6. Reinventing the Wheel
    - Is there a (well-maintained, checked) package for this? 
    - [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)

7. **Reliability**
    - What happens if packages aren't installed (don't use `require()`!)?
    - data can't be found?
    - data has missing values or errors? 
    - data has been altered or corrupted? 
    - a function returns missing or zero-length values? 
    - one script in the middle of the pipeline is altered? 
    
8. Scalability

9. Reusability
    - Can this bit of code be dropped into the lab's/team's next project? 

10. In-house/lab/team style

11. Test coverage and quality
    - Unpopular opinion: For "bench data science," assertions often make more sense than unit tests
    - `assertthat` for assertions; `testthat` for unit tests
    
12. **Fit for purpose**

13. Notice what's missing
    - Edge cases, unexpected inputs, error handling
    
14. Zoom out

