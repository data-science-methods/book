# An Example Code Review #

- We'll review the code for @ColomboIntellectuallyHumblePrejudiced2020
    - Data and code available at <https://osf.io/k5qmw/>

- I find a lot of problems
    - But don't read this as criticizing the authors
    - These types of problems are widespread in computational science
    - Because researchers aren't trained to look for, prevent, and correct these problems
    

## A short checklist ##

0. What happens if you just run the script? 
1. Readability
2. Maintainability
3. Documentation
4. Reliability

## Project organization ##

- No documentation
    - README, attribution
    - Data provenance, codebook
- Folder and file names
    - Folder names suggest an order
    - Which files are inputs and which are outputs? 
    - Date stamps in file names - was version control used? 
    - Inconsistent styles in file names
- Are the inputs for the `meta` analysis the outputs from the studies? 
    - What happens if something changes in the study analyses? 

## `study1` ##

0. Just run the script
    - Error on line 16
    - Trying to take a subset of columns from the data being read in
    - Using column indices
    - But the indices don't match the number of columns in the data
    - We can't recover from this without documentation for the data
        - What columns should be in the raw data, and what columns will be used for the analysis? 

1. Readability
    - Long lines, inconsistent whitespace
    - Repetitive chunks of analysis aren't documented
        - Could be put into function(s) with meaningful names

2. Maintainability
    - Lots of repetition: lines 25-41; analysis on 90-95; numerous places throughout
    - Doesn't seem like this analysis is being written out anywhere? 
        - In particular, how is the output here supposed to feed into the meta-analysis? 
    - Line 132: why are we deleting things? 
    - `cor.mtest` is defined multiple times
    - Line 265: surprise `library()` call

3. Documentation
    - Line 46: Looks like we're starting one section of the analysis
        - But it's easy to miss this section break, similar breaks eg line 134
        - Contrast very visible break on 199
    - What are these tests, plots, and why are we doing them? 
    - Contrast with the factor analysis starting on line 79

4. Reliability
    - No checks to validate data


## `study2` ##

0. Just run the script
    - Syntax error on line 282
        - Uncommenting 281 gives a very cryptic error
        - Commenting out these lines -> everything seems to run
    - But should we be worried about all of these convergence warnings? 

- Pretty much all the same concerns as with `study1`
    - Seems to use exactly the same analysis functions
        - These should be defined once, in a single R file called by the analysis scripts
        - Could even be a local package loaded using `usethis::load_all()` (<https://devtools.r-lib.org/reference/load_all.html>)
    
## `study3` ##

0. Just run the script
    - Line 47: Missing dependency

1. Readability
    - Long lines, bad pipe style

2. Maintainability
    - Again, issues with repetition, not writing things out

3. Documentation
    - Basically none

4. Reliability
    - `TRUE` and `FALSE` are reserved words, meaning they can't be used as variable names
        - `T` and `F` are variables defined by default to match these
        - Their value can be changed
        - So using `TRUE` and `FALSE` is preferred
    - Several repetitions of `export(tidy(m6k, conf.int  = T), "export.xlsx")`
        - Writing over `m6k` each time with a new regression model
        - Is `export()` appending the results to `export.xlsx`, or overwriting the file? 

## `study4` and `meta` ##

- Same issues as the other script files
- Confirm that the input for the meta-analysis isn't directly generated from the study scripts
    

<!--
- Better: <https://osf.io/he5za/>; @CollingRegisteredReplicationReport2020
    - Although: 
    - Binder stalled out my first attempt
    - The manuscript file can't be knit
    - Running the analysis requires finding and running a series of scripts
    - Some of the scripts have warnings and errors, and it's not clear if those are problems
-->

    
- *[todo]* lab/exercise: spend 90 minutes on EDA of the Silberzahn et al. red card data, then code review
