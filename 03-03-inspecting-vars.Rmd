# Inspecting Variables #

```{r child = 'knitr_config.Rmd'}
```

For this EDA, we'll work with data on police stops in Oakland, California, that have been pre-cleaned and released by the Stanford Open Policing Project.  Because this analysis focuses on categorical data and counts of observations, most of the elements in @HuebnerSystematicApproachInitial2016 don't really fit.  So we'll follow the checklist from @PengArtDataScience2016. 

We'll also be learning how to use three R packages:  `skimr` and `visdat` for getting an overview of the data in a dataframe, and `dplyr` for manipulating dataframes.  A good cheatsheet for `dplyr` is available here:  <https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf>. 

## Packages ##

```{r, echo=TRUE, message=FALSE}
library(tidyverse)   # for working with the data
library(lubridate)   # for working with datetime data
library(vroom)       # for quickly reading large CSV files
library(skimr)       # generate a text-based overview of the data
library(visdat)      # generate plots visualizing data types and missingness
library(tictoc)      # for timing how long R takes to do the thing

## Set for the HTML output
## In RStudio, this is overriden by the width of the console pane
options(width = 100)  # wider output for skimr
```

## Get the Data ##

We'll be using data on police stops in Oakland, California, collected and published by the Stanford Open Policing Project, <https://openpolicing.stanford.edu/>.  We'll start by making a `data` folder and downloading the data, if necessary.  The URL for the data file can be found on <https://openpolicing.stanford.edu/data/>.  It will also be useful to have the readme handy, <https://github.com/stanford-policylab/opp/blob/master/data_readme.md>.  

Because the next chunk of code might be intimidating to type out, and you need the url, I encourage you to copy and paste. 

```{r}
data_dir = 'data'
target_file = file.path(data_dir, 'oakland.zip')

if (!dir.exists(data_dir)) {
    dir.create(data_dir)
}
if (!file.exists(target_file)) {
    download.file('https://stacks.stanford.edu/file/druid:yg821jf8611/yg821jf8611_ca_oakland_2020_04_01.csv.zip', 
                  target_file)
}
```

## 1. Formulate your question ##

The Black Lives Matter protests over the last several years have made us aware of the racial aspects of policing.  So we're interested in 

1. Whether Black people in Oakland might be more likely to be stopped than White people
2. Whether Black people who are stopped might be more likely to have contraband

These rough questions will be enough to get us started.  

## 2. Read in your data ##

The dataset is a zipped `csv` or comma-separated value file.  CSVs are structured like Excel spreadsheets, but are stored in plain text rather than Excel's format.  

```{r, eval=FALSE}
## Not run
## The base R function `read.csv()` can't read a zipped file, and returns gibberish
read.csv(target_file)

## The `read_csv()` function, part of the tidyverse, takes 1.7 seconds on my laptop, but misinterprets several columns as logicals, resulting in parsing problems.
## (What goes wrong?  By default, `read_csv()` looks at the first 1,000 rows to guess what the type of each column should be.  As we'll see below, columns like `beat` and `subject_age` are missing on the first 5,000+ rows.  So `read_csv()` sees 1,000 missing values and guesses that these columns are `logical`, with values `TRUE` or `FALSE`.  We could fix this by changing the `guess_max` argument, but that would slow things down even more.)
tic()
read_csv(target_file)
toc()
```
```{r}
## The vroom package is 10x faster on my laptop and doesn't have the parsing problems
tic()
dataf = vroom(target_file)
toc()

# class(dataf$beat)
# class(dataf$subject_age)
```

## 3. Check the packaging ##

@PengArtDataScience2016 use some base R functions to look at dimensions of the dataframe and column (variable) types.  `skimr` is more powerful. 

```{r, render=knitr::normal_print}
## May take a couple of seconds
skim(dataf)
```

- 133k rows (observations); 28 columns (variables)
- 16 variables are handled as characters
    - `raw_row_number` has 1 unique value per row
        - So it's probably some kind of identifier
    - `subject_race` and `subject_sex` have just 5 and 2 unique values
        - These are probably categorical variables represented as characters
    - Similarly with `type`, `outcome`, and `search_basis`
        - Though these have lots of missing values (high `n_missing`, low `complete_rate`)
- 1 variable represents the date, and another is `difftime`
    - `?difftime` tells us that `difftime` is used to represent intervals or "time differences"
- 7 logical variables
    - A lot of these look like coded outcomes that we might be interested in, eg, `search_conducted` and `contraband_found`
    - `search_conducted` has no missing values, but `contraband_found` has a lot of missing values
    
- For our motivating questions
    - Good:  `subject_race` is 100% complete
    - Also good:  `search_conducted` is also 100% complete
    - Potentially worrisome:  `contraband_found` is only 31% complete

Let's use `vis_miss()`, from `visdat`, to visualize missing values and check out what's up with `contraband_found`. 

```{r}
## This raises a warning about large data
# vis_miss(dataf)
## So we'll use sample_n() to draw a subset
set.seed(2020-09-10)
dataf_smol = sample_n(dataf, 1000)

vis_miss(dataf_smol)

## Arguments in vis_miss() are useful for picking up patterns in missing values
## cluster = TRUE uses hierarchical clustering to order the rows
vis_miss(dataf_smol, cluster = TRUE)
```

- Several variables related to search outcomes are missing together
    - `contraband_found`, `contraband_drugs`, `contraband_weapons`, `search_basis`, `use_of_force_description`, `raw_subject_typeofsearch`, and `raw_subject_resultofsearch`
    - However, `search_conducted` is complete
    - **Critical question:  When a search has been conducted, do we know whether contraband was found?**
    - Semi-translated:  when `search_conducted == TRUE`, is `!is.na(contraband_found)`? 
    
## Filter, mutate, count, and pipes ##

If the answer to this **critical question** is **no** (for a lot of rows), then we probably can't use it to answer our motivating questions.  We'll use 4 tools from the tidyverse to answer this critical question.  

- `filter(dataf, condition1, condition2, ...)` 
    - Returns a subset of `dataf` where all of the `condition`s are true
    - We'll use this to construct a subset where `search_conducted == TRUE`
- `mutate(dataf, newvar = f(oldvar))` 
    - Adds the column `newvar`; it doesn't change `oldvar`
    - We'll use this to add a variable, `contraband_known`, indicating whether `contraband_found` is missing. 
- `count(dataf, group1, group2, ...)` 
    - Organizes `dataf` into groups based on the unique values of the `group` variables, then
    - Counts the number of rows in each group
    - We'll use this to count the number of rows in our subset where the value of `contraband_found` is known or missing
- The pipe `%>%` 
    - Lets us string together a series of function calls
    - The output of one function gets passed as the first argument to the next function
    - The tidyverse functions are set up to be pipe-friendly: they take a dataframe as the first argument, and output a dataframe
    - We'll use this to put the function calls together into a single analysis pipeline
        - More functional: We won't be changing `dataf`
        - Less cluttered: We don't need to create intermediate variables for each step

```{r}
dataf %>% 
    filter(search_conducted) %>% 
    mutate(contraband_known = !is.na(contraband_found)) %>% 
    count(search_conducted, contraband_known)
```

## 4. Look at the top and the bottom of your data ##

With 28 columns, the dataframe is too wide to print in a readable way.  We could use the `select()` function (from the tidyverse) to extract and print a few columns at a time.  

Instead we'll use the base R function `View()` in an interactive session.  This shows us an Excel-like spreadsheet presentation of a dataframe.  

`View()` can cause significant problems if you use it with a large dataframe on a slower machine.  So we'll use a pipe: first extract the `head()` or `tail()` of the dataset, and then `View()` it.  We'll also go ahead and view `dataf_smol`, the subset we created for `visdat` above.  

```{r, eval=FALSE}
dataf %>% 
    head() %>% 
    View()

dataf %>% 
    tail() %>% 
    View()

View(dataf_smol)
```

Some of my observations: 

- The ID variable `raw_row_number` can't be turned into a numeric value
- `location` is a mix of addresses and intersections ("Bond St @ 48TH AVE")
    - If we were going to generate a map using this column, geocoding might be tricky
    - Fortunately we also get latitude and longitude columns
- `use_of_force_description` doesn't seem to be a descriptive text field; instead it seems to be mostly missing or "handcuffed"

We can also use `skimr` to check data quality by looking at the minimum and maximum values.  Do these ranges make sense for what we expect the variable to be?  

```{r, render=knitr::normal_print}
skim(dataf)
```

- Date range is April 1, 2013 to December 31, 2017
    - If we break things down by year, we should expect 2013 to have fewer cases
    - For some purposes, we might need to exclude 2013 data:  `filter(dataf, date >= '2014-01-01')`
- Age range is from 10 years old (!) to 97 (!)
    - Median (`p50`) is 29; 50% of values are between 23 and 41
    - For some purposes, we might need to restrict the analysis to working-age adults: `filter(dataf, subject_age >= 18, subject_age < 65)`
    
## 5. Check your “n”s (and) 6. Validate with at least one external data source ##

- @PengArtDataScience2016 use an air quality example with a regular sampling rate, 
    - so they can calculate exactly how many observations they should have. 
- We can't do that here
    - So we'll combine steps 5 and 6 together
    
- A web search leads us to this City of Oakland page on police stop data:  <https://www.oaklandca.gov/resources/stop-data>
    - The page mentions a Stanford study that was released in June 2016
    - Recall we got our data from the Stanford Open Policing Project
    - Our data run through December 2017
    - So there's a good chance we're using a superset of the "Stanford study" data
    
    - The page links to this report: <https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf>
    - Page 8 has two summary tables that we can compare to our data
    
```{r, echo=FALSE, fig.cap="Screenshot of the two summary tables from the Oakland report.  Source: <https://cao-94612.s3.amazonaws.com/documents/OPD-Racial-Impact-Report-2016-2018-Final-16Apr19.pdf>, page 8"}
knitr::include_graphics(file.path('images', '03-eda', 'Oakland_report_tables.png'))
```
    
## From dates to years ##

- Our data has the particular date of each stop
    - We need to extract the year of each stop
        - `lubridate::year()` does exactly this
    - Filter to the years in our data that overlap with the tables
    - And then aggregate by year (and gender) using `count`
    
```{r}
dataf %>% 
    mutate(year = year(date)) %>% 
    filter(year %in% c(2016, 2017)) %>% 
    count(year)

dataf %>% 
    mutate(year = year(date)) %>% 
    filter(year %in% c(2016, 2017)) %>% 
    count(year, subject_sex)
```

- *For both years, we have fewer observations than the report table indicates*
    - Could our data have been pre-filtered? 
    - Let's check the documentation for our data:  <https://github.com/stanford-policylab/opp/blob/master/data_readme.md#oakland-ca>
    
    - "Data is deduplicated on raw columns contactdate, contacttime, streetname, subject_sdrace, subject_sex, and subject_age, reducing the number of records by ~5.2%"
        - The difference with the report is on this order of magnitude, 
        - But varies within groups by several percentage points
        - So deduplication *might* explain the difference
        - But in a more serious analysis we might want to check, eg, with the Stanford Open Policing folks
    
```{r}
## Men in 2016 in the report vs. our data: 8.2%
(24576 - 22563) / 24576

## Women in 2016 in the report vs. our data: 3.6%
(7965 - 7677) / 7965

## All of 2016 in the report vs. our data: 7.1%
(32569 - 30268) / 32569
```

## 7. Make a plot ##

Plotting is a whole additional discussion.  We'll talk about it next week.  

## 8. Try the easy solution first ##

We are interested in two rough questions:  

1. Whether Black people in Oakland might be more likely to be stopped than White people
2. Whether Black people who are stopped might be more likely to have contraband

- The easy solution is to count rows, mostly using the tools we've already developed. 
    - We'll use `mutate()` after `count()` to calculate shares, ie, what fraction of the total are in each group
    - We'll also introduce the function `arrange()`, to sort the dataframe from greatest to smallest share
    - And `group_by()` to calculate shares within racial groups

### Number of stops, by race ###

```{r}
dataf %>% 
    count(subject_race) %>% 
    mutate(share = n / sum(n)) %>% 
    arrange(desc(share))
```

- Police stop demographics
    - 60% of subjects stopped by police are Black
    - 19% are Hispanic
    - 12% are White
    - 6% are API
- Oakland demographics: <https://en.wikipedia.org/wiki/Oakland,_California#Race_and_ethnicity>
    - 23% of residents are Black
    - 27% are Hispanic
    - 29% are non-Hispanic White
    - 15% are Asian
- Comparing the two
    - *Blacks are severely overrepresented in police stops*
    - Hispanics and API folks are slightly underrepresented
    - Whites are significantly underrepresented

### Contraband search results, by race ###

As a preliminary question, what fraction of stops had a search?  Are there disparities by race there?  We can answer the first question using tools we already have.  

```{r}
## What fraction of stops had a search? 
## 31%
dataf %>% 
    count(search_conducted) %>% 
    mutate(share = n / sum(n))
```

Across all subjects, 31% of stops involved a search. 

For the other question, we need to calculate something like `share` *within* racial groups.  That is, among all stops *of Black folks*, what fraction involved a search?  The `group_by()` function divides the dataset into groups that other tidyverse functions respect.  Specifically, `count()` and `mutate()` will both analyze each group defined by `subject_race` separately.  

After using `group_by()`, it's good practice to always call `ungroup()`.  We don't want to accidentally work with the grouped version in another part of our analysis. 

```{r}
## What fraction of stops had a search, by race? 
## Note that, for each racial group, `share` adds up to 100%
## For all groups, most stops didn't involve a search
## For Black subjects, 38% of stops involved a search
## For White subjects, 15% of stops involved a search
dataf %>% 
    group_by(subject_race) %>% 
    count(search_conducted) %>% 
    mutate(share = n / sum(n)) %>% 
    ungroup()
```

- For all groups, most stops didn't involve a search
- For Black subjects, 38% of stops involved a search
- For White subjects, 15% of stops involved a search

- *So police were much more likely to search stopped Black subjects than White subjects*

Finally, let's consider our second main question:  contraband results from searches.  Note that we want to restrict ourselves to only stops where `search_conducted` is true.  


```{r}
## Contraband results from searches, by race
dataf %>% 
    filter(search_conducted) %>% 
    count(subject_race, contraband_found) %>% 
    group_by(subject_race) %>% 
    mutate(share = n / sum(n)) %>% 
    ungroup() %>% 
    filter(contraband_found)
```    

- For Black subjects who were searched, contraband was found 15% of the time
- For White subjects, 17% of the time

**This preliminary analysis indicates that Black subjects were more likely to be searched than White subjects; but, when they were searched, White subjects were more likely to have contraband.**

## 9. Follow up ##

- There are several directions we could take this analysis:  
    - Investigate outstanding questions about quality and reliability of the data
        - eg, follow up with Stanford Open Policing Project about the difference in row counts
        - Fits with *risk management* (spiral model): spend a little more time checking to make sure the data aren't corrupted

    - Break down our question into more fine-grained analyses
        - eg, the Oakland web site and report talk about policy changes; do we see changes by year in the data? 
        - Fits with *epicycle of analysis*: refine and specify research questions

    - Apply more sophisticated statistical analysis
        - eg, a regression model to control for age, gender, and other covariates
        - Fits with *phenomena development*: reducing data, eliminating noise, in order to identity local phenomena


## Discussion questions ##

Time permitting, we'll discuss these during class.  But more likely we'll talk about them via Slack. 

1. Suppose you've conducted this EDA because you're working with an activist organization that promotes defunding the police and prison abolition.  Should you share the preliminary findings above with your organization contacts?  

2. What influence should the following factors make to your answer?  
    - Funding: Whether you're being paid as a consultant vs. volunteering your expertise
    - Values: Your own views about policing and prisons
    - Relationships: Whether you are friends with members of the activist organization and/or police
    - Communications: The degree to which you can control whether and how the organization will publish your preliminary findings
    - Timeliness:  Whether these findings are relevant to a pending law or policy change
    
3. What other factors should be taken into account as you decide whether to share your findings?  Or not taken into account? 

4. How has this "raw data" been shaped by upstream decisions?  (Think of Leonelli's relational account of data.)


## Lab ##

The lab related to this material is available at <https://github.com/data-science-methods/lab-w04-eda>. 


