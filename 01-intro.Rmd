# Introduction

## Land Acknowledgement ##

UC Merced and the City of Merced are on the traditional territory of the Yokut people.  This land was stolen by Spanish, Mexican, and American settlers through acts of slavery and genocide.  In addition, UC Merced is strongly associated with Ahwahne, known as Yosemite Valley.  This valley was the traditional home of the Ahwahnechee people, who were the victims of some especially horrific, state-sponsored genocidal acts.  For more on the history of Ahwahne, see <https://tinyurl.com/y879jw8s>.  For more information on land acknowledgments, see <https://native-land.ca>.  


## About me ##

Dan Hicks is a philosopher turned data scientist turned philosopher.  

I use they/them pronouns and identify as nonbinary.  I grew up in Placerville, about two hours north of Merced in the Sierra Foothills.  One branch of my family came to California during the Gold Rush, so I identify heavily as a Californian and have some complicated feelings about the genocide.  I finished my PhD in philosophy of science at Notre Dame in 2012.  After that I worked in a series of research positions in academia and the federal government.  During 2015-2019 I was using data science methods at least half-time.  I joined the faculty at UC Merced in Fall 2019.  

- Email: <dhicks4@ucmerced.edu>
- Student hours: By appointment: <https://doodle.com/mm/danhicks/office-hours>
- Website: <https://dhicks.github.io/>


## What this course isn't, and is ##

Is Not: 

- a statistics course (in the way you think)
- a general introduction to software engineering
- a basic introduction to R? (Unless that's what you all need?)
- carefully planned out from beginning to end

Is: 

- an introduction to data science
- about data cleaning, data management, and exploratory data analysis
- habituation to some good software engineering practices that are especially valuable for data science work
- in the [pre-alpha stage of development](https://en.wikipedia.org/wiki/Software_release_life_cycle#Pre-alpha)

## Learning outcomes ##

By the end of the course, students will be able to 

1. apply concepts from software engineering and philosophy of science to methodological decisions in data science, including
	- functional vs. object-oriented programming paradigms [@ChambersObjectOrientedProgrammingFunctional2014]
	- waterfall vs. spiral model of development
	- ladder of models
	- reproducibility vs. robustness vs. replicability
	- value and hazards of open data
	- thick data [@WangWhyBigData2016]

3. use exploratory data analysis techniques and tools to identify potential data errors and potential phenomena for further analysis
	- checklist [@HuebnerSystematicApproachInitial2016]
	- `skimr`, `visdat`
	- 1D summaries: histograms, density, ecdf plots
	- 2D summaries: scatterplots, boxplot, violin plot, beeswarm, upset plots; pairs plot (<https://www.data-imaginist.com/2019/a-flurry-of-facets/>); correlation matrix

2. clean raw data and produce a reproducible codebook for both downstream analysis and public release of data according to FAIR standards [@WilkinsonFAIRGuidingPrinciples2016]
	- [create and use a cloud database to work with big data]

4. manage data, analysis, and outputs for reproducibility [assembling a project compendium; @GentlemanStatisticalAnalysesReproducible2004] using best practices of data handling, a clear directory structure [@WilsonGoodEnoughPractices2017], self-documenting code, version control, build automation, [and continuous integration] 

5. apply philosophical and data science concepts to integrated ethical-technical analysis to case studies in algorithmic injustice
    - understand the background on the COMPAS case:  @AngwinMachineBiasThere2016; @Corbett-DaviesComputerProgramUsed2016
	- reproduce several fairness statistics on COMPAS data
	- explain tradeoffs between false negatives and false positives in terms of inductive risk
	- explain @KleinbergInherentTradeOffsFair2016 impossibility result (not necessarily the proof) and show that it applies to the COMPAS datas
	- critically discuss profiling and thresholds [@PiersonLargescaleAnalysisRacial2020] as potential explanation for differences in "base rates" of recidivism
	- critically locate discussions of fairness and recidivism in a broader social context, and identify implications for working data scientists [@SelbstFairnessAbstractionSociotechnical2018; @HoffmannWhereFairnessFails2019; @HannaCriticalRaceMethodology2019]

## Workflow ##

- The first ~10 weeks of the course will focus on skill development, with a mix of livecoding lectures and labs.  
    - Labs will be set up to give you automated feedback.  
    - There will probably be asynchronous discussion using Slack for some of the labs. 
- On November 10 we'll be working on project with Hanna Gunn's Data Ethics course.  
- The last few weeks of the course will be devoted to term projects.  
    - Tentative idea for this:  go find some data; clean it, explore it; produce a codebook and reproducible exploratory data analysis


## Notes on some course design decisions ##

### R, not Python ###

- Python is a perfectly good language for general programming
    - And has some advantages over R in certain data science aspects, including working with strings and developing custom data structures

However, 

- R is a better language for doing data science [@ShotwellWhyUse2019]
    - Vectors, data frames, and common statistical models are installed and loaded by default
    - R is more functional [@WuPythonVsChoosing2019; @PaleologoWhatDifferenceMachine2018]
    
- R is also better for beginners
    - [CRAN](https://twitter.com/kccarrell/status/1030482372775620613)
    - RStudio
    
### Tidyverse R, not base R ###

- "Base R" refers to the central code for R and the packages included when you install it
    - Base R is universal and free of additional dependencies.  Code written entirely in base R will run on any other R installation (with the same version). 
    - However, because the R Core Development Team has always thought backwards-compatibility is important, base R has some idiosyncracies [@KeyesRbitraryStandards2016]
    
- `tidyverse` is a suite of packages developed by the manufacturers of RStudio
    - A very useful set of general tools can be loaded simply by calling `library(tidyverse)`
        - And some more specialized tools are also installed as part of the suite
    - Tidyverse is designed to mitigate a lot of the base R idiosyncracies
    - While making code more human-readable and easier to reason about
    - However, several of its tools use something called "non-standard evaluation" (NSE). 
        - NSE makes tidyverse code easier to write for beginners, because you don't have to think as much about where exactly your variables are located
        - At the same time, NSE creates problems for intermediate coders, because passing tidyverse-style "bare names" through function arguments causes cryptic errors

### Scripts, not notebooks ###

- Quite a few scientists like the way notebooks mix together code, visualizations, tables, and text [@PerkelWhyJupyterData2018]

However, 

- Notebooks do not play nicely with version control  [@AhemadVersionControlJupyter2018; @WhitmoreJupyterNotebookBest2015]
- Notebooks encourage bad software engineering habits and discourage good ones [@GrusDonNotebooksJoel; @HiltchJupyterNotebookCancer2019]


## Data Science WTF ##

- Standard definition
    - ![Data science, defined as the intersection of CS, stats, and "business knowledge"](images/Abisiga.jpg){ width=30% }
    - Source: <https://www.kdnuggets.com/2020/08/top-10-lists-data-science.html>
    - The intersection of computer science/software engineering, statistics, and "business knowledge"
    - But this focuses on the tools and techniques, not the epistemic and practical *goals* of data science

- To understand my definition, first let's talk about *data*
    - *Representational view*: data are "reliable representations of reality which are produced via the interaction between humans and the world"
        - "the production of data is equivalent to 'capturing' features of the world that can be used for systematic study"
        - "data as 'raw' products of research, which are as close as it gets to unmediated knowledge of reality"
        - data provide "an objective foundation for the acquisition of knowledge"
        - **data become data when they're produced according to a standardized measurement process**

    - *Relational view*: "data are objects that are treated as potential or actual evidence for scientific claims in ways that can, at least in principle, be scrutinised and accounted for"
        - "any object can be used as a datum, or stop being used as such, depending on the circumstances"
        - **data become data when they're used as evidence** [@LeonelliScientificResearchBig2020]
        - Note implications for ideas of "unmediated knowledge" and "objective foundation"
        
- Data science â‰  lab science (often)
    - data collection isn't standardized
        - found (or bought), not made
        - data as byproduct
        - ex: scraping from Twitter, Wikipedia, administrative records
    - remixing and recombining data
        - ex: pesticide use + Census data in @HicksCensusDemographicsChlorpyrifos2020
    - data first, questions later
    - **relational view is often a better fit**
    
- The goal of data science
    - to *produce data*
    - to transform objects such that they can be used as evidence
    
    
